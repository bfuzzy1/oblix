<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>oblix</title>
    <style>
    /* --- Base Styles --- */
    a { color: white; }
    body {
      background: #000000; color: #fff; font-family: monospace;
      margin: 0; padding: 3% 5%; /* Adjusted padding */
      display: flex; flex-direction: column; gap: 15px;
      overflow-x: hidden;
    }
    h3 { margin: 1rem 0; } /* Adjusted margin */
    p { margin: 0 0 1rem 0; color: #aaaaaac8; line-height: 1.4; } /* Adjusted margin & color */
    .grid {
      display: grid;
      grid-template-columns: minmax(400px, 1.5fr) minmax(300px, 2fr); /* Adjusted ratios */
      gap: 20px; /* Increased gap */
      opacity: 0; transform: translateY(20px);
      animation: fadeInUp 0.5s ease-out forwards;
    }
    .widget {
      background: #111; border: 1px solid #333; border-radius: 10px;
      padding: 20px; /* Increased padding */
      box-sizing: border-box; width: 100%;
      opacity: 0; transform: translateY(20px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.1s; /* Faster delay */
      margin-bottom: 20px;
    }
    .widget-title {
      font-size: 1.2em; /* Larger title */
      margin: 0 0 15px 0; /* Adjusted margin */
      border-bottom: 1px solid #444; /* Lighter border */
      padding-bottom: 10px;
      opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.2s;
    }
    .input-group {
      margin-bottom: 15px; /* Increased margin */
      opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.3s; /* Staggered delay */
    }
    .input-group label {
        display: block; margin-bottom: 5px; /* Increased margin */
        font-size: 0.9em; color: #bbb; /* Lighter label */
        cursor: help; /* Indicate tooltips exist */
    }
    /* Checkbox label specific style */
    .input-group label input[type="checkbox"] {
        margin-right: 8px;
        vertical-align: middle;
        cursor: pointer;
    }
     .input-group label span { /* For inline text next to checkbox */
        vertical-align: middle;
        cursor: help;
     }

    .settings-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(130px, 1fr)); /* Adjusted minmax */
      gap: 15px; /* Increased gap */
      margin-bottom: 15px;
      opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.4s;
    }
    input[type="text"], input[type="number"], select, textarea {
      outline: none; width: 100%; padding: 8px; /* Increased padding */
      background: #222; border: 1px solid #444; color: #fff;
      border-radius: 6px; /* Slightly less rounded */
      box-sizing: border-box; transition: background 0.3s, border 0.3s;
      font-family: monospace; font-size: 0.95em; /* Slightly larger font */
    }
    #loadDataBtn {
      background-color: #eee; color: black; font-weight: 600;
      font-size: 12px; padding: 2px 5px; border-radius: 3px; cursor: pointer;
      transition: background-color 0.2s, color 0.2s; border: 1px solid #888;
    }
    #loadDataBtn:hover { background-color: #ccc; }
    input:focus, select:focus, textarea:focus { background: #333; border: 1px solid #777; } /* Lighter focus */
    button {
      background: #eee; color: #000; border: none; padding: 8px 15px; /* Increased padding */
      border-radius: 6px; cursor: pointer; transition: all 0.15s ease;
      border: 1px solid #888; /* Lighter border */
      opacity: 0; height: auto; /* Auto height */
      transform: translateY(10px); animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.5s; font-family: monospace; font-weight: bold;
      margin-right: 10px; /* Add spacing between buttons */
      margin-bottom: 5px; /* Ensure buttons wrap nicely */
    }
    button:hover:not(:disabled) {
      border: 1px solid white; color: white; background: #222; /* Darker hover */
    }
    button:disabled {
      background: #444; color: #888; border-color: #444; cursor: not-allowed;
    }
    .progress-container {
      height: 150px; /* Slightly shorter */
      position: relative; border: 1px solid #333; border-radius: 8px;
      margin-bottom: 10px; opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards; animation-delay: 0.6s;
      overflow: hidden; background-color: #1a1a1a;
    }
    .loss-graph, .network-graph {
      position: absolute; top: 0; left: 0; width: 100%; height: 100%;
    }

    /* --- Network Viz Container Scrolling --- */
    #network-viz-container {
        overflow-x: auto; /* Allow horizontal scroll */
        overflow-y: hidden; /* Hide vertical scroll */
        /* Optional: Style scrollbar */
        scrollbar-width: thin; /* Firefox */
        scrollbar-color: #555 #222; /* Firefox: thumb track */
    }
    #network-viz-container::-webkit-scrollbar {
        height: 8px;
    }
    #network-viz-container::-webkit-scrollbar-track {
        background: #222;
        border-radius: 4px;
    }
    #network-viz-container::-webkit-scrollbar-thumb {
        background-color: #555;
        border-radius: 4px;
        border: 2px solid #222;
    }
    #network-viz-container::-webkit-scrollbar-thumb:hover {
        background-color: #777;
    }
    /* --- END Network Viz Scrolling --- */

    .loss-graph, .network-graph {
        display: block; /* Prevents extra space below canvas */
    }
    
    .flex-container {
      display: flex; flex-wrap: wrap; /* Allow wrapping */
      gap: 20px; opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards; animation-delay: 0.7s;
    }
    .prediction-section, .visualization-container {
       flex: 1 1 300px; /* Flex basis */
       background: #111; border: 1px solid #333; border-radius: 10px;
       padding: 20px; box-sizing: border-box;
       margin-bottom: 0; /* Remove margin if it's inside another widget */
    }
    /* Separate widget for Model Management Buttons */
    .model-management-widget .button-group {
        display: flex; flex-wrap: wrap; gap: 10px;
        /* animations handled by parent */
        opacity: 1; transform: none; animation: none;
    }

    .epoch-progress {
      height: 6px; background: #333; /* Darker background */
      border-radius: 8px; overflow: hidden; margin-top: 8px;
    }
    .epoch-bar { height: 100%; width: 0; background: #eee; transition: width 0.3s ease; }
    #stats { margin-top: 10px; font-size: 0.9em; min-height: 2.5em; color: #ccc; }
    #stats strong { color: #76ff03; } /* Brighter green */
    #stats .error { color: #ff5252; } /* Brighter red */
    #hiddenLayersConfig .input-group { /* Reduce animation delay within layer config */
        animation-delay: 0.1s;
    }
    #hiddenLayersConfig .layer-options-container {
        display: contents; /* Allow options to flow in parent grid */
    }
    #hiddenLayersConfig .settings-grid {
       border-top: 1px solid #444; padding-top: 15px; margin-top: 15px;
    }
    .layer-note { /* Style for notes within layer config */
      font-size: 0.85em; color: #888; margin: 5px 0 0 0;
      grid-column: 1 / -1; /* Span full width */
      line-height: 1.3;
    }

    @keyframes fadeInUp { to { opacity: 1; transform: translateY(0); } }
    @media (max-width: 900px) { /* Adjust breakpoint */
      .grid { grid-template-columns: 1fr; }
    }
     @media (max-width: 480px) { /* Smaller screens */
        body { padding: 3% 3%; }
        .widget { padding: 15px; }
        input, select, textarea, button { font-size: 0.9em; padding: 6px 10px; }
     }
    </style>
</head>
<body>
  <h3>Oblix</h3>
  <p>a neural playground for anyone...</p>
  <p>Load dummy data: <span id="loadDataBtn">click here</span></p>

  <div class="grid">
    <!-- Group 1: Data & Training Config -->
    <div class="widget">
      <div class="widget-title">Data & Model Configuration</div>
      <div class="input-group">
        <label for="trainingData">Training Set (CSV, last column=output):</label>
        <textarea id="trainingData" rows="4" placeholder="0.1, 0.9, 0.1
0.9, 0.1, 0.9
0.2, 0.8, 0.2"></textarea>
      </div>
      <div class="input-group">
        <label for="testData">Validation Set (Optional):</label>
        <textarea id="testData" rows="3" placeholder="0.5, 0.5, 0.5"></textarea>
      </div>

      <div class="widget-title" style="margin-top: 20px;">Training Parameters</div>
      <div class="settings-grid">
        <div class="input-group">
          <label for="epochs" title="Number of full passes through the training dataset.">Epochs:</label>
          <input type="number" id="epochs" value="50" min="1">
        </div>
        <div class="input-group">
            <label for="lossFunction" title="How the model's error is calculated. MSE for regression, Cross-Entropy for classification.">Loss Function:</label>
            <select id="lossFunction">
                <option value="mse" selected>MSE</option>
                <option value="crossentropy">Cross-Entropy</option>
            </select>
        </div>
         <div class="input-group">
          <label for="optimizer" title="Algorithm used to update model weights based on error. Adam is often a good default.">Optimizer:</label>
          <select id="optimizer">
              <option value="sgd">SGD</option>
              <option value="adam" selected>Adam</option>
              <option value="rmsprop">RMSprop</option>
              <option value="adamw">AdamW</option>
          </select>
         </div>
        <div class="input-group">
          <label for="learningRate" title="How much the model weights are adjusted each update. Too high can diverge, too low is slow.">Learning Rate:</label>
          <input type="number" id="learningRate" value="0.01" step="0.001" min="0">
        </div>
        <div class="input-group">
            <label for="lrScheduler" title="How the learning rate changes over epochs.">LR Schedule:</label>
            <select id="lrScheduler">
                <option value="none" selected>None (Constant)</option>
                <option value="step">Step Decay</option>
                <option value="exponential">Exponential Decay</option>
            </select>
        </div>
        <div id="lrStepParams" class="input-group settings-grid" style="display: none; grid-column: 1 / -1; border-top: 1px solid #444; margin-top: 5px; padding-top: 5px;">
             <div class="input-group">
                 <label for="lrStepDecayFactor" title="Multiply LR by this factor at each step. (e.g., 0.1)">Decay Factor:</label>
                 <input type="number" id="lrStepDecayFactor" value="0.1" step="0.01" min="0" max="1">
             </div>
             <div class="input-group">
                 <label for="lrStepDecaySize" title="Decrease LR every N epochs. (e.g., 10)">Step Size (Epochs):</label>
                 <input type="number" id="lrStepDecaySize" value="10" step="1" min="1">
             </div>
        </div>
         <div id="lrExpParams" class="input-group settings-grid" style="display: none; grid-column: 1 / -1; border-top: 1px solid #444; margin-top: 5px; padding-top: 5px;">
             <div class="input-group">
                 <label for="lrExpDecayRate" title="Multiply LR by this rate each epoch. (e.g., 0.95)">Decay Rate:</label>
                 <input type="number" id="lrExpDecayRate" value="0.95" step="0.001" min="0" max="1">
             </div>
        </div>
        <div class="input-group" id="decayRateGroup" style="display: none;">
           <label for="decayRate" title="Decay factor for RMSprop optimizer's moving average.">Decay Rate (ρ):</label>
           <input type="number" id="decayRate" value="0.9" step="0.01" min="0" max="1">
        </div>
        <div class="input-group">
          <label for="batchSize" title="Number of training samples processed before updating weights.">Batch Size:</label>
          <input type="number" id="batchSize" value="8" min="1">
        </div>
        <div class="input-group">
            <label for="l2Lambda" title="Strength of L2 regularization (weight decay). Helps prevent overfitting (0 to disable).">L2 Lambda:</label>
            <input type="number" id="l2Lambda" value="0" step="0.0001" min="0">
        </div>
        <div class="input-group">
            <label for="gradientClipValue" title="Max absolute value for gradients before update (0 to disable). Helps prevent exploding gradients.">Grad Clip Val:</label>
            <input type="number" id="gradientClipValue" value="0" step="0.1" min="0">
        </div>
        <div class="input-group">
            <label title="Adds information about the position of inputs, useful for sequence data.">
                <input type="checkbox" id="usePositionalEncoding"><span>Use Positional Encoding</span>
            </label>
        </div>
      </div>

      <div class="widget-title" style="margin-top: 20px;">Layer Architecture</div>

      <!-- {{ Add Architecture Template Dropdown }} -->
      <div class="input-group">
          <label for="architectureTemplateSelect">Architecture Template:</label>
          <select id="architectureTemplateSelect">
              <option value="custom" selected>Custom</option>
              <option value="mlp">Simple MLP</option>
              <option value="autoencoder">Basic Autoencoder</option>
              <option value="transformerEncoder">Transformer Encoder</option>
              <option value="residual-attention">Residual Attention</option>
              <!-- Add more templates here in the future -->
          </select>
      </div>
      <!-- {{ End Dropdown }} -->

       <div class="input-group">
         <label for="numHiddenLayers" title="Number of layers between input and output. More layers allow for more complex patterns but increase training time/risk of overfitting.">Number of Hidden Layers:</label>
         <input type="number" id="numHiddenLayers" value="2" min="0">
       </div>
      <!-- Dynamic Layer Configuration UI -->
      <div id="hiddenLayersConfig"></div>

    </div> <!-- End Widget 1 -->

        <!-- Group 2: Training Control, Progress & Visualization -->
        <div class="widget">
            <div class="widget model-management-widget">
                <div class="widget-title">Model Management</div>
                <div class="button-group">
                    <button id="trainButton">Train Model</button>
                    <button id="saveButton">Save Model</button>
                    <button id="loadButton">Load Model</button>
                    <button id="unloadButton">Unload Model</button>
                </div>
           </div>
    
            <div class="widget-title">Training Progress</div>
            <div id="progress">
                <div class="progress-container">
                    <canvas id="lossGraph" class="loss-graph"></canvas>
                </div>
                <p style="text-align: center;">Train Loss (White), Validation Loss (Blue)</p> <!-- Updated Color Name -->
                <div class="epoch-progress">
                    <div id="epochBar" class="epoch-bar"></div>
                </div>
                <div id="stats">Status: Ready</div>
            </div> <!-- End #progress -->
    
            <!-- Network Visualization Card (Moved out of flex-container, placed first) -->
            <div class="visualization-container widget" style="margin-top: 20px;">
                <div class="widget-title">Network Visualization</div>
                <div id="network-viz-container" class="progress-container">
                    <canvas id="networkGraph" class="network-graph"></canvas>
                </div>
                <p style="text-align: center;">Structure & Last Activations</p>
            </div>
    
            <!-- Manual Prediction Card (Moved out of flex-container, placed second) -->
            <div class="prediction-section widget" style="margin-top: 20px;">
                <div class="widget-title">Manual Prediction</div>
                <p>Predict output for new input</p>
                <div class="input-group">
                    <label for="predictionInput">Input (CSV):</label>
                    <input type="text" id="predictionInput" placeholder="0.4, 0.2, 0.6">
                </div>
                <button id="predictButton">Predict</button>
                <div id="predictionResult" style="margin-top: 10px; font-weight: bold;">Result: -</div>
            </div>
    
            <!-- Removed the flex-container div that wrapped prediction and visualization -->
    
        </div> <!-- End Widget 2 -->
      </div> <!-- End Grid -->

  <script>
    // --- Namespace for Activation Functions ---
    const oblixActivations = {
        apply: function(x, activation) {
        const alpha = 0.01; const softplus = (v) => Math.log(1 + Math.exp(v));
        switch (activation) {
          case 'tanh': return Math.tanh(x); case 'sigmoid': return 1 / (1 + Math.exp(-x)); case 'relu': return Math.max(0, x); case 'leakyrelu': return x > 0 ? x : alpha * x; case 'gelu': return 0.5 * x * (1 + Math.tanh(Math.sqrt(2 / Math.PI) * (x + 0.044715 * x**3))); case 'selu': const sa = 1.67326, ss=1.0507; return x > 0 ? ss * x : ss * sa * (Math.exp(x) - 1);
          case 'swish': return x / (1 + Math.exp(-x)); case 'mish': return x * Math.tanh(softplus(x));
          case 'softmax': case 'none': default: return x;
        }
        },
        derivative: function(x, activation) {
          const alpha = 0.01; let val; const sigmoid = (v) => 1 / (1 + Math.exp(-v)); const softplus = (v) => Math.log(1 + Math.exp(v)); const dtanh_dx = (v) => 1 - Math.tanh(v)**2;
          switch (activation) {
              case 'tanh': val = Math.tanh(x); return 1 - val * val; case 'sigmoid': val = sigmoid(x); return val * (1 - val); case 'relu': return x > 0 ? 1 : 0; case 'leakyrelu': return x > 0 ? 1 : alpha; case 'gelu': const k=Math.sqrt(2 / Math.PI), inner=k*(x+0.044715*x**3), tanh_inner=Math.tanh(inner), d_inner_dx=k*(1+0.134145*x**2), sech_sq_inner=1-tanh_inner**2; return 0.5*(1+tanh_inner)+0.5*x*sech_sq_inner*d_inner_dx; case 'selu': const sa=1.67326, ss=1.0507; return x > 0 ? ss : ss * sa * Math.exp(x);
              case 'swish': const sig_x = sigmoid(x); return sig_x + x * sig_x * (1 - sig_x);
              case 'mish': const sp_x = softplus(x); const tanh_sp_x = Math.tanh(sp_x); const dsp_dx = sigmoid(x); const dtanh_dsp = dtanh_dx(sp_x); return tanh_sp_x + x * dtanh_dsp * dsp_dx;
              case 'softmax': case 'none': default: return 1;
          }
      }
    };
    // --- End Namespace ---

    // --- Namespace for Layer Operations ---
    const oblixLayerOps = {
        attentionForward: function(context, input, numHeads = 2) {
            const inputDim=input.length; if(inputDim===0) return []; if(numHeads<=0||!Number.isInteger(numHeads)||inputDim%numHeads!==0){ console.error(`Attn Error: Dim ${inputDim} not divisible by ${numHeads}`); return input; } const headSize=inputDim/numHeads; const output=new Array(inputDim).fill(0); const allAttentionWeights=[]; for(let h=0;h<numHeads;h++){ const start=h*headSize; const end=start+headSize; const q_head=input.slice(start,end); const k_head=q_head; const v_head=q_head; const scores=Array.from({length:headSize},(_,i)=>Array.from({length:headSize},(_,j)=>q_head[i]*k_head[j])); const scale=Math.sqrt(headSize)||1; const scaled=scores.map(r=>r.map(s=>s/scale)); const attn=scaled.map(r=>{ if(r.length===0) return []; const max=Math.max(...r,-Infinity); const exps=r.map(s=>Math.exp(s-max)); const sum=exps.reduce((a,b)=>a+b,1e-9); return exps.map(e=>e/sum); }); allAttentionWeights.push(attn); for(let i=0;i<headSize;i++){ output[start+i]=attn[i].reduce((sum,a,j)=>sum+a*v_head[j],0); } }
            // {{ Edit: Use context.forwardCache }}
            if(context.forwardCache) context.forwardCache.attentionIntermediates[context.forwardCache.activations.length-1] = { input, numHeads, headSize, attentionWeights: allAttentionWeights };
            return output;
        },
        attentionBackward: function(context, dOutput, cache) {
            if(!cache||!cache.input||!cache.attentionWeights){ const N=dOutput?.length||0; return {dInput:Array(N).fill(0)}; } // Changed attnWgts to attentionWeights to match forward pass cache
            const {input,numHeads,headSize,attentionWeights}=cache; const inputDim=input.length; if(dOutput.length!==inputDim){ return {dInput:Array(inputDim).fill(0)}; } const dInput=Array(inputDim).fill(0); const scale=Math.sqrt(headSize)||1; for(let h=0;h<numHeads;h++){ const start=h*headSize; const end=start+headSize; const q_h=input.slice(start,end); const v_h=q_h; const k_h=q_h; const dO_h=dOutput.slice(start,end); const alpha_h=attentionWeights[h]; if(!alpha_h||alpha_h.length!==headSize||alpha_h[0]?.length!==headSize){ console.error(`Attn Bkwd Err: Head ${h} weights invalid.`); continue; } const dQ_h=Array(headSize).fill(0); const dK_h=Array(headSize).fill(0); const dV_h=Array(headSize).fill(0); const dScores_h=Array.from({length:headSize},()=>Array(headSize).fill(0)); const dAlpha_h=Array.from({length:headSize},()=>Array(headSize).fill(0)); for(let j=0;j<headSize;j++){ for(let i=0;i<headSize;i++){ if(typeof dO_h[i]!=='number'||typeof v_h[j]!=='number'||typeof alpha_h[i][j]!=='number') continue; dV_h[j]+=alpha_h[i][j]*dO_h[i]; dAlpha_h[i][j]=dO_h[i]*v_h[j]; } } for(let i=0;i<headSize;i++){ let row_sum=0; for(let k=0;k<headSize;k++){ if(typeof dAlpha_h[i][k]!=='number'||typeof alpha_h[i][k]!=='number') continue; row_sum+=dAlpha_h[i][k]*alpha_h[i][k]; } for(let j=0;j<headSize;j++){ if(typeof alpha_h[i][j]!=='number'||typeof dAlpha_h[i][j]!=='number') continue; const dS_ij=alpha_h[i][j]*(dAlpha_h[i][j]-row_sum); dScores_h[i][j]=dS_ij/scale; } } for(let i=0;i<headSize;i++){ for(let j=0;j<headSize;j++){ if(typeof dScores_h[i][j]!=='number'||typeof k_h[j]!=='number'||typeof q_h[i]!=='number') continue; dQ_h[i]+=dScores_h[i][j]*k_h[j]; dK_h[j]+=dScores_h[i][j]*q_h[i]; } } for(let i=0;i<headSize;i++){ dInput[start+i]=(dQ_h[i]||0)+(dK_h[i]||0)+(dV_h[i]||0); } } return {dInput};
        },
        layerNormForward: function(context, input, gamma, beta) {
            // {{ Edit: Use context.epsilon }}
            const epsilon = context.epsilon;
            if(input.length===0) return {output:[],mean:0,variance:0,stddev:epsilon,normalizedInput:[]}; const mean=input.reduce((s,v)=>s+v,0)/input.length; const variance=input.reduce((s,v)=>s+Math.pow(v-mean,2),0)/input.length; const stddev=Math.sqrt(variance+epsilon); const normalized=input.map(v=>(v-mean)/stddev); if(gamma.length!==input.length||beta.length!==input.length) console.error(`LN size mismatch`); const output=normalized.map((v,i)=>(gamma[i]??1)*v+(beta[i]??0));
            // {{ Edit: Use context.forwardCache }}
            const cacheData={output,mean,variance,stddev,normalizedInput:normalized,input,gamma}; if(context.forwardCache) context.forwardCache.layerNormIntermediates[context.forwardCache.activations.length-1]=cacheData;
            return cacheData; // Return the full cache data including output
        },
        layerNormBackward: function(context, dOutput, cache) {
             if(!cache||!cache.input||!cache.normalizedInput||!cache.gamma||!dOutput){ const N_in=cache?.input?.length||dOutput?.length||0; return {dInput:Array(N_in).fill(0),dGamma:Array(N_in).fill(0),dBeta:Array(N_in).fill(0)}; } // Changed normInput to normalizedInput
             const {input,normalizedInput,mean,variance,stddev,gamma}=cache; const N=input.length; if(N===0) return {dInput:[],dGamma:[],dBeta:[]}; if(dOutput.length!==N||normalizedInput.length!==N||gamma.length!==N){ return {dInput:Array(N).fill(0),dGamma:Array(N).fill(0),dBeta:Array(N).fill(0)}; }
             // {{ Edit: Use context.epsilon }}
             const epsilon=context.epsilon;
             const dGamma=normalizedInput.map((n,i)=>dOutput[i]*n); const dBeta=dOutput.slice(); const dNorm=dOutput.map((d,i)=>d*(gamma[i]??1)); const dVar=dNorm.reduce((s,dn,i)=>s+dn*(input[i]-mean)*(-0.5)*Math.pow(variance+epsilon,-1.5),0); const invStd=1/stddev; const dMean1=dNorm.reduce((s,dn)=>s-dn*invStd,0); const dMean2=dVar*input.reduce((s,x)=>s-2*(x-mean),0)/N; const dMean=dMean1+dMean2; const dInput=dNorm.map((dn,i)=>{ const t1=dn*invStd; const t2=dVar*2*(input[i]-mean)/N; const t3=dMean/N; return t1+t2+t3; }); return {dInput,dGamma,dBeta};
        },
        dropoutForward: function(context, input, rate) {
            // {{ Edit: Use context properties }}
            const idx = (context.forwardCache?.activations?.length || 1) - 1;
            if(!context.isTraining||rate===0){ context.masks[idx]=null; return input; }
            if(rate<0||rate>=1){ console.warn(`Dropout rate ${rate} invalid`); context.masks[idx]=null; return input; }
            const scale=1/(1-rate); const mask=input.map(()=>(Math.random()>rate?scale:0)); context.masks[idx]=mask;
            return input.map((v,i)=>v*mask[i]);
        },
        dropoutBackward: function(context, dOutput, layerIndex) {
            // {{ Edit: Use context.masks }}
            const mask=context.masks[layerIndex];
            if(!mask){ return dOutput; }
            if(!dOutput||dOutput.length!==mask.length){ return dOutput; }
            return dOutput.map((g,i)=>g*mask[i]);
        },
        softmaxForward: function(context, input) {
            if(input.length===0) return []; const maxVal=Math.max(...input); const exps=input.map(x=>Math.exp(x-maxVal)); const sumExps=exps.reduce((a,b)=>a+b,1e-9); const output=exps.map(e=>e/sumExps);
            // {{ Edit: Use context.forwardCache }}
            if(context.forwardCache) context.forwardCache.softmaxOutputs[context.forwardCache.activations.length-1]=output;
            return output;
        },
        softmaxBackward: function(context, dOutput, layerIndex) {
            // Typically Softmax backward is combined with Cross-Entropy loss,
            // so this direct backward pass might not be strictly needed if using CE loss.
            // Returning dOutput is a simplification often used when Softmax isn't the *final* layer
            // or when loss isn't CE. For CE, the gradient d(Loss)/d(SoftmaxInput) simplifies nicely.
            console.warn("Softmax backward pass used directly.");
            return dOutput;
        }
    };
    // --- End Layer Namespace ---

    // --- Namespace for Optimizers ---
    const oblixOptimizers = {
        initializeState: function(context, optimizer) {
            const numLayers = context.layers.length;
            if (context.debug) console.log(`Init optimizer state: ${optimizer}, ${numLayers} layers.`);
            context.t = 0;
            context.m_dw = Array(numLayers).fill(null); context.v_dw = Array(numLayers).fill(null);
            context.m_db = Array(numLayers).fill(null); context.v_db = Array(numLayers).fill(null);
            context.m_dgamma = Array(numLayers).fill(null); context.v_dgamma = Array(numLayers).fill(null);
            context.m_dbeta = Array(numLayers).fill(null); context.v_dbeta = Array(numLayers).fill(null);
            context.s_dw = Array(numLayers).fill(null); context.s_db = Array(numLayers).fill(null);
            context.s_dgamma = Array(numLayers).fill(null); context.s_dbeta = Array(numLayers).fill(null);

            for (let i = 0; i < numLayers; i++) {
                const cfg = context.layers[i]; if (!cfg) continue;
                const w = context.weights[i]; const reqW = cfg.type === 'dense' && Array.isArray(w) && w.length > 0 && Array.isArray(w[0]);
                const b = context.biases[i]; const reqB = cfg.type === 'dense' && cfg.useBias && Array.isArray(b) && b.length > 0;
                const g = context.gammas[i]; const beta = context.betas[i]; const reqLN = cfg.type === 'layernorm' && Array.isArray(g) && Array.isArray(beta) && g.length === beta.length && g.length > 0;

                if (optimizer === 'adam' || optimizer === 'rmsprop' || optimizer === 'adamw') {
                    if (reqW) { try { const z = () => w.map(r => r.map(() => 0)); if (optimizer === 'adam' || optimizer === 'adamw') { context.m_dw[i] = z(); context.v_dw[i] = z(); } if (optimizer === 'rmsprop') { context.s_dw[i] = z(); } } catch (e) { console.error(`InitOpt L${i} W err: ${e.message}`); context.m_dw[i] = null; context.v_dw[i] = null; context.s_dw[i] = null; } }
                    if (reqB) { try { const z = () => b.map(() => 0); if (optimizer === 'adam' || optimizer === 'adamw') { context.m_db[i] = z(); context.v_db[i] = z(); } if (optimizer === 'rmsprop') { context.s_db[i] = z(); } } catch (e) { console.error(`InitOpt L${i} B err: ${e.message}`); context.m_db[i] = null; context.v_db[i] = null; context.s_db[i] = null; } }
                    if (reqLN) { try { const z = () => g.map(() => 0); if (optimizer === 'adam' || optimizer === 'adamw') { context.m_dgamma[i] = z(); context.v_dgamma[i] = z(); context.m_dbeta[i] = z(); context.v_dbeta[i] = z(); } if (optimizer === 'rmsprop') { context.s_dgamma[i] = z(); context.s_dbeta[i] = z(); } } catch (e) { console.error(`InitOpt L${i} LN err: ${e.message}`); context.m_dgamma[i] = null; context.v_dgamma[i] = null; context.m_dbeta[i] = null; context.v_dbeta[i] = null; context.s_dgamma[i] = null; context.s_dbeta[i] = null; } }
                }
            }
            if (context.debug) console.log(`Optimizer state init finished.`);
        },

        updateParameters: function(context, gradsW, gradsB, gradsGamma, gradsBeta, options) {
            const {
                learningRate, // Current epoch learning rate
                initialLearningRate, // Base LR before schedule adjustments
                optimizer,
                batchSize, // Use actual batch length
                l2Lambda,
                gradientClipValue,
                decayRate // For RMSProp, passed from context
        } = options;

            const batchMult = 1.0 / batchSize;
            context.t++; // Increment global timestep

            for (let i = 0; i < context.layers.length; i++) {
                const cfg = context.layers[i];
                const isDense = cfg.type === 'dense';
                const isLN = cfg.type === 'layernorm';

                // Optimizer-specific LR calculations
                let stepLR = learningRate; // LR for SGD/RMSprop step
                // Adam uses a base corrected LR, scaled by the scheduler's effect
                let adamCorrectedBaseLR = initialLearningRate * Math.sqrt(1 - context.beta2 ** context.t) / (1 - context.beta1 ** context.t);
                let adamStepLR = adamCorrectedBaseLR * (learningRate / initialLearningRate); // Apply schedule scaling

              // --- Weight Updates (Dense Layers) ---
                if (isDense && Array.isArray(context.weights[i]) && Array.isArray(gradsW[i])) {
                    const w = context.weights[i]; const gW = gradsW[i];
                for (let j = 0; j < cfg.outputSize; j++) {
                  for (let k = 0; k < cfg.inputSize; k++) {
                    if (typeof w[j]?.[k] !== 'number' || typeof gW[j]?.[k] !== 'number') continue;
                    const grad = gW[j][k] * batchMult;
                    let param = w[j][k];
                            let m = context.m_dw?.[i]?.[j]?.[k];
                            let v = context.v_dw?.[i]?.[j]?.[k];
                            let s = context.s_dw?.[i]?.[j]?.[k];
                    let update = 0;

                    const clippedGrad = gradientClipValue > 0 ? Math.max(-gradientClipValue, Math.min(gradientClipValue, grad)) : grad;

                    if (optimizer === 'adam' || optimizer === 'adamw') {
                                m = (typeof m === 'number' && isFinite(m)) ? m : 0; v = (typeof v === 'number' && isFinite(v)) ? v : 0;
                                m = context.beta1 * m + (1 - context.beta1) * clippedGrad;
                                v = context.beta2 * v + (1 - context.beta2) * clippedGrad ** 2;
                                const m_hat = m / (1 - context.beta1 ** context.t);
                                const v_hat = v / (1 - context.beta2 ** context.t);
                                update = adamStepLR * m_hat / (Math.sqrt(v_hat) + context.epsilon);
                                if (optimizer === 'adam' && l2Lambda > 0) { update += adamStepLR * l2Lambda * param; }
                                if (context.m_dw?.[i]?.[j]) context.m_dw[i][j][k] = m; if (context.v_dw?.[i]?.[j]) context.v_dw[i][j][k] = v;
                    } else if (optimizer === 'rmsprop') {
                      s = (typeof s === 'number' && isFinite(s)) ? s : 0;
                                s = decayRate * s + (1 - decayRate) * clippedGrad ** 2; // Use context's decayRate
                                update = stepLR * clippedGrad / (Math.sqrt(s) + context.epsilon);
                                if (l2Lambda > 0) { update += stepLR * l2Lambda * param; }
                                if (context.s_dw?.[i]?.[j]) context.s_dw[i][j][k] = s;
                    } else { // SGD
                      update = stepLR * clippedGrad;
                                if (l2Lambda > 0) { update += stepLR * l2Lambda * param; }
                            }
                            param -= update;
                            if (optimizer === 'adamw' && l2Lambda > 0) { param -= adamStepLR * l2Lambda * param; } // Decoupled decay
                            context.weights[i][j][k] = param; // Update weight in context
                        }
                }
              }

              // --- Bias Updates (Dense Layers) ---
                if (isDense && cfg.useBias && Array.isArray(context.biases[i]) && Array.isArray(gradsB[i])) {
                    const b = context.biases[i]; const gB = gradsB[i];
                for (let j = 0; j < cfg.outputSize; j++) {
                    if (typeof b[j] !== 'number' || typeof gB[j] !== 'number') continue;
                    const grad = gB[j] * batchMult;
                    let param = b[j];
                        let m = context.m_db?.[i]?.[j]; let v = context.v_db?.[i]?.[j]; let s = context.s_db?.[i]?.[j];
                    let update = 0;
                    const clippedGrad = gradientClipValue > 0 ? Math.max(-gradientClipValue, Math.min(gradientClipValue, grad)) : grad;

                    if (optimizer === 'adam' || optimizer === 'adamw') {
                            m = (typeof m === 'number' && isFinite(m)) ? m : 0; v = (typeof v === 'number' && isFinite(v)) ? v : 0;
                            m = context.beta1 * m + (1 - context.beta1) * clippedGrad; v = context.beta2 * v + (1 - context.beta2) * clippedGrad ** 2;
                            const m_hat = m / (1 - context.beta1 ** context.t); const v_hat = v / (1 - context.beta2 ** context.t);
                            update = adamStepLR * m_hat / (Math.sqrt(v_hat) + context.epsilon);
                            if (context.m_db?.[i]) context.m_db[i][j] = m; if (context.v_db?.[i]) context.v_db[i][j] = v;
                    } else if (optimizer === 'rmsprop') {
                      s = (typeof s === 'number' && isFinite(s)) ? s : 0;
                            s = decayRate * s + (1 - decayRate) * clippedGrad ** 2;
                            update = stepLR * clippedGrad / (Math.sqrt(s) + context.epsilon);
                            if (context.s_db?.[i]) context.s_db[i][j] = s;
                    } else { // SGD
                      update = stepLR * clippedGrad;
                    }
                    param -= update;
                        context.biases[i][j] = param; // Update bias in context
                }
              }

              // --- LayerNorm Parameter Updates (Gamma & Beta) ---
                if (isLN && Array.isArray(context.gammas[i]) && Array.isArray(context.betas[i]) && Array.isArray(gradsGamma[i]) && Array.isArray(gradsBeta[i])) {
                    const g = context.gammas[i]; const betaParam = context.betas[i];
                    const gGamma = gradsGamma[i]; const gBeta = gradsBeta[i];
                for (let j = 0; j < cfg.outputSize; j++) {
                  // Update Gamma
                  if (typeof g[j] === 'number' && typeof gGamma[j] === 'number') {
                            const gradGamma = gGamma[j] * batchMult; let paramGamma = g[j];
                            let mGamma = context.m_dgamma?.[i]?.[j]; let vGamma = context.v_dgamma?.[i]?.[j]; let sGamma = context.s_dgamma?.[i]?.[j];
                    let updateGamma = 0;
                    const clippedGradGamma = gradientClipValue > 0 ? Math.max(-gradientClipValue, Math.min(gradientClipValue, gradGamma)) : gradGamma;

                    if (optimizer === 'adam' || optimizer === 'adamw') {
                                mGamma = (typeof mGamma === 'number' && isFinite(mGamma)) ? mGamma : 0; vGamma = (typeof vGamma === 'number' && isFinite(vGamma)) ? vGamma : 0;
                                mGamma = context.beta1 * mGamma + (1 - context.beta1) * clippedGradGamma; vGamma = context.beta2 * vGamma + (1 - context.beta2) * clippedGradGamma ** 2;
                                const m_hat = mGamma / (1 - context.beta1 ** context.t); const v_hat = vGamma / (1 - context.beta2 ** context.t);
                                updateGamma = adamStepLR * m_hat / (Math.sqrt(v_hat) + context.epsilon);
                                if (context.m_dgamma?.[i]) context.m_dgamma[i][j] = mGamma; if (context.v_dgamma?.[i]) context.v_dgamma[i][j] = vGamma;
                    } else if (optimizer === 'rmsprop') {
                       sGamma = (typeof sGamma === 'number' && isFinite(sGamma)) ? sGamma : 0;
                                sGamma = decayRate * sGamma + (1 - decayRate) * clippedGradGamma ** 2;
                                updateGamma = stepLR * clippedGradGamma / (Math.sqrt(sGamma) + context.epsilon);
                                if (context.s_dgamma?.[i]) context.s_dgamma[i][j] = sGamma;
                    } else { // SGD
                       updateGamma = stepLR * clippedGradGamma;
                    }
                    paramGamma -= updateGamma;
                            context.gammas[i][j] = paramGamma; // Update gamma in context
                  }

                  // Update Beta
                  if (typeof betaParam[j] === 'number' && typeof gBeta[j] === 'number') {
                            const gradBeta = gBeta[j] * batchMult; let paramBeta = betaParam[j];
                            let mBeta = context.m_dbeta?.[i]?.[j]; let vBeta = context.v_dbeta?.[i]?.[j]; let sBeta = context.s_dbeta?.[i]?.[j];
                    let updateBeta = 0;
                    const clippedGradBeta = gradientClipValue > 0 ? Math.max(-gradientClipValue, Math.min(gradientClipValue, gradBeta)) : gradBeta;

                    if (optimizer === 'adam' || optimizer === 'adamw') {
                                mBeta = (typeof mBeta === 'number' && isFinite(mBeta)) ? mBeta : 0; vBeta = (typeof vBeta === 'number' && isFinite(vBeta)) ? vBeta : 0;
                                mBeta = context.beta1 * mBeta + (1 - context.beta1) * clippedGradBeta; vBeta = context.beta2 * vBeta + (1 - context.beta2) * clippedGradBeta ** 2;
                                const m_hat = mBeta / (1 - context.beta1 ** context.t); const v_hat = vBeta / (1 - context.beta2 ** context.t);
                                updateBeta = adamStepLR * m_hat / (Math.sqrt(v_hat) + context.epsilon);
                                if (context.m_dbeta?.[i]) context.m_dbeta[i][j] = mBeta; if (context.v_dbeta?.[i]) context.v_dbeta[i][j] = vBeta;
                    } else if (optimizer === 'rmsprop') {
                       sBeta = (typeof sBeta === 'number' && isFinite(sBeta)) ? sBeta : 0;
                                sBeta = decayRate * sBeta + (1 - decayRate) * clippedGradBeta ** 2;
                                updateBeta = stepLR * clippedGradBeta / (Math.sqrt(sBeta) + context.epsilon);
                                if (context.s_dbeta?.[i]) context.s_dbeta[i][j] = sBeta;
                    } else { // SGD
                       updateBeta = stepLR * clippedGradBeta;
                    }
                    paramBeta -= updateBeta;
                            context.betas[i][j] = paramBeta; // Update beta in context
                  }
                }
              }
            } // End param update layer loop
        }
    };
    // --- End Optimizer Namespace ---

    // --- Namespace for Utility Functions ---
    const oblixUtils = {
        positionalEncoding: function(input, maxLen = -1) {
            const dModel=input.length; if(dModel===0) return []; if(maxLen<0) maxLen=dModel; const pe=new Array(dModel).fill(0); for(let i=0;i<dModel;i++){ const divTermBase=Math.pow(10000,Math.floor(i/2)*2/dModel); if(divTermBase===0) continue; const angle=i/divTermBase; pe[i]=(i%2===0)?Math.sin(angle):Math.cos(angle); } return input.map((val,i)=>val+pe[i]);
        },

        /**
         * Calculates accuracy for classification tasks.
         * Assumes predictions are probability distributions (e.g., from Softmax)
         * and targets are either class indices or one-hot encoded vectors.
         * @param {number[][]} predictions - Array of prediction vectors.
         * @param {(number[]|number[][])} targets - Array of target class indices or one-hot vectors.
         * @returns {number} Accuracy value (0.0 to 1.0).
         */
        calculateAccuracy: function(predictions, targets) {
            if (!predictions || !targets || predictions.length === 0 || predictions.length !== targets.length) {
                console.warn("Accuracy calculation: Invalid input arrays.");
                return 0.0;
            }

            let correct = 0;
            for (let i = 0; i < predictions.length; i++) {
                const predVec = predictions[i];
                const targetInfo = targets[i];

                if (!predVec || !targetInfo) continue; // Skip invalid entries

                // Find the index of the highest probability in the prediction
                const predictedIndex = predVec.indexOf(Math.max(...predVec));

                let targetIndex = -1;
                // Check if target is a class index (single number)
                if (typeof targetInfo === 'number' && Number.isInteger(targetInfo)) {
                    targetIndex = targetInfo;
                }
                // Check if target is a one-hot vector
                else if (Array.isArray(targetInfo) && targetInfo.length === predVec.length) {
                    targetIndex = targetInfo.indexOf(1);
                    // Handle cases where it's not strictly one-hot (e.g., [0.9, 0.1]) - find max
                    if (targetIndex === -1) {
                       targetIndex = targetInfo.indexOf(Math.max(...targetInfo));
                    }
                } else {
                     console.warn(`Accuracy calc: Invalid target format at index ${i}`, targetInfo);
                     continue;
                }


                if (predictedIndex === targetIndex && targetIndex !== -1) {
                    correct++;
                }
            }
            return predictions.length > 0 ? correct / predictions.length : 0.0;
        },

        /**
         * Calculates the R-squared (Coefficient of Determination) for regression tasks.
         * @param {number[]} predictions - Array of predicted values.
         * @param {number[]} targets - Array of true target values.
         * @returns {number} R-squared value. Can be negative for poor models.
         */
        calculateRSquared: function(predictions, targets) {
            if (!predictions || !targets || predictions.length === 0 || predictions.length !== targets.length) {
                console.warn("R-squared calculation: Invalid input arrays.");
                return NaN; // Indicate invalid input
            }
             if (predictions.length < 2) {
                 console.warn("R-squared calculation: Need at least 2 data points.");
                 return NaN; // R-squared is ill-defined for n < 2
             }

            const targetMean = targets.reduce((sum, val) => sum + val, 0) / targets.length;

            let ssTot = 0; // Total sum of squares
            let ssRes = 0; // Residual sum of squares

            for (let i = 0; i < targets.length; i++) {
                const targetVal = targets[i];
                const predVal = predictions[i];
                if (typeof targetVal !== 'number' || typeof predVal !== 'number' || !isFinite(targetVal) || !isFinite(predVal)) {
                     console.warn(`R-squared calc: Non-finite number at index ${i}`);
                     return NaN; // Invalid data point
                }
                ssTot += (targetVal - targetMean) ** 2;
                ssRes += (targetVal - predVal) ** 2;
            }

            if (ssTot === 0) {
                // If all targets are the same, R-squared is undefined or sometimes treated as 1 if predictions are also perfect, 0 otherwise.
                // Returning NaN or 0 depends on convention. Let's return NaN for undefined case.
                return (ssRes === 0) ? 1.0 : NaN; // Perfect prediction if ssRes is also 0, otherwise undefined
            }

            return 1 - (ssRes / ssTot);
        }

        // Add other utils here later if needed
    };
    // --- End Utility Namespace ---


    class oblix {
      constructor(debug = true) {
        this.layers = []; this.weights = []; this.biases = []; this.gammas = []; this.betas = []; this.masks = [];
        this.details = {}; this.debug = debug; this.usePositionalEncoding = false; this.isTraining = false;
        this.beta1 = 0.9; this.beta2 = 0.999; this.epsilon = 1e-8; this.t = 0;
        this.m_dw = []; this.v_dw = []; this.m_db = []; this.v_db = [];
        this.m_dgamma = []; this.v_dgamma = []; this.m_dbeta = []; this.v_dbeta = [];
        this.decayRate = 0.9;
        this.s_dw = []; this.s_db = []; this.s_dgamma = []; this.s_dbeta = [];
        this.lastActivations = null; this.forwardCache = null;
      }

      reset() {
        if (this.debug) console.log("Resetting oblix instance...");
        this.layers = []; this.weights = []; this.biases = []; this.gammas = []; this.betas = []; this.masks = [];
        this.details = {};
        this.isTraining = false;
        this.t = 0;
        this.m_dw = []; this.v_dw = []; this.m_db = []; this.v_db = []; this.m_dgamma = []; this.v_dgamma = []; this.m_dbeta = []; this.v_dbeta = [];
        this.s_dw = []; this.s_db = []; this.s_dgamma = []; this.s_dbeta = [];
        this.lastActivations = null; this.forwardCache = null;
      }

      layer(config) {
        const { type = 'dense', inputSize, outputSize, activation = 'tanh', numHeads = 2, useBias = true, rate = 0.5 } = config;
        if (typeof inputSize !== 'number' || inputSize <= 0) throw new Error(`Layer ${this.layers.length}: Invalid inputSize: ${inputSize}.`);
        if (this.layers.length > 0) { const prevLayer = this.layers[this.layers.length - 1]; if (inputSize !== prevLayer.outputSize) throw new Error(`Layer ${this.layers.length} (${type}): Input size ${inputSize} doesn't match previous layer's output size ${prevLayer.outputSize}.`); }
        let actualOutputSize = outputSize;
        switch (type) { case 'dense': if (typeof outputSize !== 'number' || outputSize <= 0) throw new Error(`Dense Layer ${this.layers.length}: Invalid outputSize: ${outputSize}.`); break; case 'layernorm': case 'attention': case 'dropout': case 'softmax': actualOutputSize = inputSize; if (outputSize !== undefined && outputSize !== inputSize) console.warn(`${type} layer ${this.layers.length}: Output size ignored.`); break; default: throw new Error(`Unknown layer type: ${type}`); }
        if (type === 'attention' && inputSize % numHeads !== 0) throw new Error(`Attention layer ${this.layers.length}: Input size ${inputSize} not divisible by numHeads ${numHeads}.`);
        if (type === 'dropout' && (rate < 0 || rate >= 1)) throw new Error(`Dropout layer ${this.layers.length}: Rate ${rate} must be >= 0 and < 1.`);
        const layerConfig = { type, inputSize, outputSize: actualOutputSize, activation, numHeads, useBias, rate }; this.layers.push(layerConfig);
        this.weights.push(null); this.biases.push(null); this.gammas.push(null); this.betas.push(null); this.masks.push(null);
        this.m_dw.push(null); this.v_dw.push(null); this.m_db.push(null); this.v_db.push(null); this.m_dgamma.push(null); this.v_dgamma.push(null); this.m_dbeta.push(null); this.v_dbeta.push(null); this.s_dw.push(null); this.s_db.push(null); this.s_dgamma.push(null); this.s_dbeta.push(null);
        if (type === 'dense') { const limit = Math.sqrt(6 / (inputSize + actualOutputSize)); const weights = Array.from({ length: actualOutputSize }, () => Array.from({ length: inputSize }, () => (Math.random() * 2 - 1) * limit)); this.weights[this.layers.length - 1] = weights; if (useBias) this.biases[this.layers.length - 1] = Array(actualOutputSize).fill(0.01); }
        else if (type === 'layernorm') { this.gammas[this.layers.length - 1] = Array(actualOutputSize).fill(1.0); this.betas[this.layers.length - 1] = Array(actualOutputSize).fill(0.0); }
      }

      // --- positionalEncoding REMOVED (Moved to namespace) ---

      initializeOptimizerState(optimizer) {
        oblixOptimizers.initializeState(this, optimizer);
      }

      getTotalParameters() { let total=0; if(!this.layers||this.layers.length===0)return 0; this.layers.forEach((l,i)=>{if(l.type==='dense'){total+=this.weights[i]?this.weights[i].flat().length:0;total+=(l.useBias&&this.biases[i])?this.biases[i].length:0;}else if(l.type==='layernorm'){total+=this.gammas[i]?this.gammas[i].length:0;total+=this.betas[i]?this.betas[i].length:0;}}); return total; }

      getCurrentLearningRate(epoch, initialLR, options) {
          const { lrSchedule, lrStepDecayFactor, lrStepDecaySize, lrExpDecayRate } = options;
          let currentLR = initialLR;

          if (lrSchedule === 'step') {
              // Ensure step size is at least 1
              const stepSize = Math.max(1, Math.floor(lrStepDecaySize));
              // Calculate how many decay steps have occurred
              const decaySteps = Math.floor(epoch / stepSize);
              currentLR = initialLR * Math.pow(lrStepDecayFactor, decaySteps);
          } else if (lrSchedule === 'exponential') {
              // Decay happens every epoch
              currentLR = initialLR * Math.pow(lrExpDecayRate, epoch);
          }
          // 'none' schedule uses initialLR

          return currentLR;
      }

      async train(trainSet, options = {}) {
        this.isTraining = true; const start = Date.now();
        let {
            epochs = 100, learningRate = 0.01, batchSize = 16, printEveryEpochs = 10,
            earlyStopThreshold = 1e-7, testSet = null, callback = null, optimizer = 'adam',
            lossFunction = 'mse', l2Lambda = 0, decayRate = 0.9,
            usePositionalEncoding = this.usePositionalEncoding, gradientClipValue = 0,
            lrSchedule = 'none', lrStepDecayFactor = 0.1, lrStepDecaySize = 10, lrExpDecayRate = 0.95
        } = options;
        // Store the initial learning rate separately
        const initialLearningRate = learningRate;

        if (!trainSet || trainSet.length === 0) throw new Error("Training set empty."); if (this.layers.length === 0) throw new Error("No layers."); const effectiveBatchSize = Math.max(1, Math.min(batchSize, trainSet.length)); this.usePositionalEncoding = usePositionalEncoding; this.decayRate = decayRate;
        let needsOptimizerInit = this.m_dw?.length !== this.layers.length || this.v_dw?.length !== this.layers.length || this.s_dw?.length !== this.layers.length || this.m_db?.length !== this.layers.length || this.v_db?.length !== this.layers.length || this.s_db?.length !== this.layers.length; for (let i=0; i < this.layers.length && !needsOptimizerInit; i++) { if (this.layers[i].type === 'dense') { if (this.weights[i] && this.m_dw[i] === null) needsOptimizerInit = true; if (this.biases[i] && this.m_db[i] === null) needsOptimizerInit = true; } else if (this.layers[i].type === 'layernorm') { if (this.gammas[i] && this.m_dgamma[i] === null) needsOptimizerInit = true; if (this.betas[i] && this.m_dbeta[i] === null) needsOptimizerInit = true; } } if (needsOptimizerInit) { if (this.debug) console.log("Optimizer state needs init."); this.initializeOptimizerState(optimizer); }
        let lastTrainLoss = Infinity; let lastTestLoss = null;
        // {{ Add: Variable for the validation metric }}
        let lastValidationMetric = null;
        let validationMetricName = ''; // e.g., "Acc" or "R²"

        for (let epoch = 0; epoch < epochs; epoch++) {
          // ++ Calculate Learning Rate for the Current Epoch ++
          const currentEpochLearningRate = this.getCurrentLearningRate(epoch, initialLearningRate, options);
          // -- END LR Calculation --

          let totalEpochTrainError = 0; for (let i = trainSet.length - 1; i > 0; i--) { const j = Math.floor(Math.random() * (i + 1)); [trainSet[i], trainSet[j]] = [trainSet[j], trainSet[i]]; }
          for (let b = 0; b < trainSet.length; b += effectiveBatchSize) {
            const batch = trainSet.slice(b, b + effectiveBatchSize); if (batch.length === 0) continue;
            const gradsW = this.weights.map(L => L ? L.map(r => r.map(() => 0)) : null); const gradsB = this.biases.map(L => L ? L.map(() => 0) : null); const gradsGamma = this.gammas.map(L => L ? L.map(() => 0) : null); const gradsBeta = this.betas.map(L => L ? L.map(() => 0) : null); let batchLossSum = 0;
            for (const data of batch) {
                let currentInput = data.input; if (!Array.isArray(currentInput) || !Array.isArray(data.output)) { console.warn("Skip invalid data"); continue; }
                // {{ Edit: Use namespaced utility }}
                if (this.usePositionalEncoding) {
                    currentInput = oblixUtils.positionalEncoding(currentInput);
                }
                this.forwardCache = { activations: [currentInput], rawValues: [], layerNormIntermediates: [], attentionIntermediates: [], softmaxOutputs: [] }; let layerInput = currentInput;
                for (let i = 0; i < this.layers.length; i++) { const cfg=this.layers[i]; let out; this.forwardCache.rawValues[i]=null; this.forwardCache.layerNormIntermediates[i]=null; this.forwardCache.attentionIntermediates[i]=null; this.forwardCache.softmaxOutputs[i]=null; try { if(layerInput.length!==cfg.inputSize) throw new Error(`L${i}(${cfg.type}): Sz mismatch ${layerInput.length}!=${cfg.inputSize}`); switch(cfg.type){ case 'dense': const w=this.weights[i], bias=this.biases[i]; const raw=Array(cfg.outputSize).fill(0); for(let j=0;j<cfg.outputSize;++j){ let sum=bias?bias[j]:0; for(let k=0;k<cfg.inputSize;++k){sum+=layerInput[k]*w[j][k];} raw[j]=sum;} this.forwardCache.rawValues[i]=raw; out=raw.map(s=>oblixActivations.apply(s,cfg.activation)); break; case 'layernorm': out=oblixLayerOps.layerNormForward(this, layerInput,this.gammas[i],this.betas[i]).output; break; case 'attention': out=oblixLayerOps.attentionForward(this, layerInput,cfg.numHeads); break; case 'dropout': out=oblixLayerOps.dropoutForward(this, layerInput,cfg.rate); break; case 'softmax': out=oblixLayerOps.softmaxForward(this, layerInput); break; default: throw new Error(`Fwd Pass: Unknown type ${cfg.type}`); } this.forwardCache.activations.push(out); layerInput=out; } catch(e){ console.error(`Fwd L${i}(${cfg.type}) Err:`,e); this.isTraining=false; throw e; }}
                const finalOut=layerInput; const targetOut=data.output; if(finalOut.length!==targetOut.length) throw new Error(`Output/Target len mismatch`); let dLastErr; const eps_ce=1e-9;
                if(lossFunction==='crossentropy'){ let loss=0; const lastLyr=this.layers[this.layers.length-1]; const wasSoftmax=lastLyr.type==='softmax'||(lastLyr.type==='dense'&&lastLyr.activation==='softmax'); const wasSigmoid=lastLyr.type==='dense'&&lastLyr.activation==='sigmoid'; if(wasSoftmax){ const oneHot=Array(finalOut.length).fill(0); if(targetOut.length===1&&Number.isInteger(targetOut[0])&&targetOut[0]>=0&&targetOut[0]<finalOut.length){oneHot[targetOut[0]]=1;loss=-Math.log(finalOut[targetOut[0]]+eps_ce);}else if(targetOut.length===finalOut.length){targetOut.forEach((v,idx)=>oneHot[idx]=v);loss=-targetOut.reduce((s,t,i)=>s+t*Math.log(finalOut[i]+eps_ce),0);}else{throw new Error("CE target unclear");} dLastErr=finalOut.map((p,i)=>p-oneHot[i]);}else if(wasSigmoid){if(finalOut.length!==1||targetOut.length!==1)throw new Error("BCE needs single out/target");const p=finalOut[0],t=targetOut[0];loss=-(t*Math.log(p+eps_ce)+(1-t)*Math.log(1-p+eps_ce));dLastErr=[p-t];}else{console.warn("CE w/o final softmax/sigmoid");dLastErr=finalOut.map((p,i)=>p-targetOut[i]);loss=-targetOut.reduce((s,t,i)=>s+t*Math.log(finalOut[i]+eps_ce),0);} if(!isNaN(loss))batchLossSum+=loss; }else{dLastErr=finalOut.map((o,i)=>o-targetOut[i]);let loss=0.5*dLastErr.reduce((s,e)=>s+e*e,0); if(!isNaN(loss))batchLossSum+=loss;}
                let dAct = dLastErr;
                for(let i=this.layers.length-1; i>=0; i--){ const cfg=this.layers[i]; const act_prev=this.forwardCache.activations[i]; let dIn; if(!Array.isArray(dAct)||dAct.length!==cfg.outputSize){console.warn(`Bkwd L${i}(${cfg.type}): Invalid dAct. Zeros.`);dIn=Array(cfg.inputSize).fill(0);dAct=dIn;continue;} try{ switch(cfg.type){ case 'dense': const w=this.weights[i], bias=this.biases[i], raw=this.forwardCache.rawValues[i], act=cfg.activation, inSz=cfg.inputSize, outSz=cfg.outputSize; if(!Array.isArray(raw))throw new Error(`L${i} Dense: Missing raw`); const delta=raw.map((r,j)=>{const dA=(typeof dAct[j]==='number'&&isFinite(dAct[j]))?dAct[j]:0; const deriv=oblixActivations.derivative(r,act); if(typeof deriv!=='number'||!isFinite(deriv)){console.warn(`L${i} Dense, j=${j}: Deriv NaN/Inf.`); return 0;} return dA*deriv;}); dIn=Array(inSz).fill(0); for(let k=0;k<inSz;k++){for(let j=0;j<outSz;j++){if(w?.[j]?.[k]!==undefined){dIn[k]+=(delta[j]||0)*w[j][k];}}} for(let j=0;j<outSz;j++){if(gradsW?.[i]?.[j]){for(let k=0;k<inSz;k++){if(typeof act_prev?.[k]==='number'){gradsW[i][j][k]+=(delta[j]||0)*act_prev[k];}}} if(bias&&gradsB?.[i]){gradsB[i][j]+=(delta[j]||0);}} break; case 'layernorm': const lnC=(this.forwardCache.layerNormIntermediates||[]).find(c=>c&&c.input.length===cfg.inputSize); if(!lnC)throw new Error(`L${i} LN: Missing cache`); const lnG=oblixLayerOps.layerNormBackward(this, dAct,lnC); dIn=lnG.dInput; if(gradsGamma?.[i]&&gradsBeta?.[i]){for(let j=0;j<lnG.dGamma.length;j++){gradsGamma[i][j]+=lnG.dGamma[j]||0;gradsBeta[i][j]+=lnG.dBeta[j]||0;}} break; case 'attention': const attC=(this.forwardCache.attentionIntermediates||[]).find(c=>c&&c.input.length===cfg.inputSize); if(!attC)throw new Error(`L${i} Attn: Missing cache`); const attG=oblixLayerOps.attentionBackward(this, dAct,attC); dIn=attG.dInput; break; case 'dropout': dIn=oblixLayerOps.dropoutBackward(this, dAct,i); break; case 'softmax': dIn=oblixLayerOps.softmaxBackward(this, dAct,i); break; default: throw new Error(`Bkwd Pass: Unknown type ${cfg.type}`); } dAct=dIn;}catch(e){console.error(`Bkwd L${i}(${cfg.type}) Err:`,e); this.isTraining=false; throw e;} } // End bkwd layer loop
            } // End batch sample loop

            // --- Parameter Update ---
            // {{ Edit: Delegate parameter updates to namespace }}
            const updateOptions = {
                learningRate: currentEpochLearningRate,
                initialLearningRate: initialLearningRate,
                optimizer: optimizer,
                batchSize: batch.length, // Use actual batch length
                l2Lambda: l2Lambda,
                gradientClipValue: gradientClipValue,
                decayRate: this.decayRate // Pass context's decayRate
            };
            oblixOptimizers.updateParameters(this, gradsW, gradsB, gradsGamma, gradsBeta, updateOptions);

            totalEpochTrainError += batchLossSum;
          } // End batch loop

          lastTrainLoss = totalEpochTrainError / trainSet.length;

          // --- Validation Step ---
          if (testSet && testSet.length > 0) {
              let testError = 0;
              const allValPredictions = [];
              const allValTargets = [];

              for (const data of testSet) {
                  const prediction = this.predict(data.input);
                  if (prediction && data.output && prediction.length === data.output.length) {
                      const target = data.output;
                      allValPredictions.push(prediction);
                      allValTargets.push(target);

                      // --- Calculate Loss (Explicitly include CE loss) ---
                      let sampleLoss = 0;
                      const eps_ce = 1e-9;
                      if (lossFunction === 'crossentropy') {
                          // {{ Add: Explicit Cross-Entropy sample loss calculation }}
                          const lastLayer = this.layers[this.layers.length-1];
                          const wasSoftmax = lastLayer.type === 'softmax' || (lastLayer.type === 'dense' && lastLayer.activation === 'softmax');
                          const wasSigmoid = lastLayer.type === 'dense' && lastLayer.activation === 'sigmoid';

                          if (wasSoftmax) {
                              // Handle one-hot or index targets
                              if (target.length === 1 && Number.isInteger(target[0]) && target[0] >= 0 && target[0] < prediction.length) {
                                  // Target is a class index
                                  sampleLoss = -Math.log(prediction[target[0]] + eps_ce);
                              } else if (target.length === prediction.length) {
                                   // Target is likely a probability distribution/one-hot vector
                                  sampleLoss = -target.reduce((sum, t, i) => sum + t * Math.log(prediction[i] + eps_ce), 0);
                              } else {
                                  console.warn("CE Val Loss: Target/Prediction shape mismatch for Softmax.");
                                  sampleLoss = NaN; // Cannot calculate
                              }
                          } else if (wasSigmoid && prediction.length === 1 && target.length === 1) {
                              // Binary Cross-Entropy
                              const p = prediction[0]; const t = target[0];
                              sampleLoss = - (t * Math.log(p + eps_ce) + (1 - t) * Math.log(1 - p + eps_ce));
                          } else {
                              // Fallback if CE is selected but final layer isn't appropriate
                              console.warn("CE Val Loss: Final layer activation not Softmax/Sigmoid. Using MSE for loss metric.");
                              sampleLoss = 0.5 * prediction.reduce((sum, p, i) => sum + (p - target[i])**2, 0);
                          }
                      } else { // Default to MSE
                          sampleLoss = 0.5 * prediction.reduce((sum, p, i) => sum + (p - target[i])**2, 0);
                      }
                      // {{ End Add }}

                      if (!isNaN(sampleLoss) && isFinite(sampleLoss)) {
                          testError += sampleLoss;
                      }
                  }
              }
              lastTestLoss = testError / testSet.length; // Average loss over validation set

              // --- Calculate Accuracy or R-squared ---
              if (lossFunction === 'crossentropy') {
                  validationMetricName = 'Acc';
                  // Targets might be [[0], [1], [0]] or [0, 1, 0]
                  // Accuracy function handles both index and one-hot vector targets
                  lastValidationMetric = oblixUtils.calculateAccuracy(allValPredictions, allValTargets);
              } else { // Default to R-squared for MSE / other regression tasks
                  validationMetricName = 'R²';
                  // R-squared expects flat arrays of numbers
                  // Flatten predictions/targets if they are multi-dimensional (e.g., [[0.1], [0.9]])
                  const flatPreds = allValPredictions.flat();
                  const flatTargets = allValTargets.flat();
                  lastValidationMetric = oblixUtils.calculateRSquared(flatPreds, flatTargets);
              }

          } else {
              lastTestLoss = null;
              lastValidationMetric = null; // No validation set, no metric
              validationMetricName = '';
          }
          // --- End Validation Step ---

          if ((epoch + 1) % printEveryEpochs === 0 && this.debug) {
              const lrStr = lrSchedule !== 'none' ? `, LR: ${currentEpochLearningRate.toExponential(2)}` : '';
              let logMsg = `Epoch ${epoch + 1}/${epochs}, Train Loss: ${lastTrainLoss.toFixed(6)}`;
              if (lastTestLoss !== null) {
                   logMsg += `, Val Loss: ${lastTestLoss.toFixed(6)}`;
              }
              // {{ Add: Log metric if calculated }}
              if (lastValidationMetric !== null && !isNaN(lastValidationMetric)) {
                  logMsg += `, Val ${validationMetricName}: ${lastValidationMetric.toFixed(4)}`;
              }
               logMsg += lrStr;
               console.log(logMsg);
          }

          // {{ Edit: Pass metric name and value to callback }}
          if (callback) {
              await callback(epoch + 1, lastTrainLoss, lastTestLoss, validationMetricName, lastValidationMetric);
          }

          await new Promise(resolve => setTimeout(resolve, 0)); // Yield to browser
          if (lastTrainLoss < earlyStopThreshold) {
              if (this.debug) console.log(`Early stopping @ Epoch ${epoch + 1}.`);
              epochs = epoch + 1; // Adjust total epochs for summary
              break;
          }
        } // End epoch loop

        const end = Date.now(); this.isTraining = false;
        const totalParams = this.getTotalParameters();

        // {{ Edit: Add validation metric to summary }}
        const trainingSummary = {
            trainLoss: lastTrainLoss,
            testLoss: lastTestLoss,
            validationMetric: { name: validationMetricName, value: lastValidationMetric },
            parameters: totalParams,
            training: {
                time: end - start, epochs: epochs,
                learningRate: initialLearningRate, // Report initial LR
                batchSize: effectiveBatchSize, optimizer: optimizer, lossFunction: lossFunction,
                l2Lambda: l2Lambda, decayRate: this.decayRate,
                usePositionalEncoding: this.usePositionalEncoding, gradientClipValue: gradientClipValue,
                lrSchedule: lrSchedule,
                lrStepDecayFactor: lrSchedule === 'step' ? lrStepDecayFactor : undefined,
                lrStepDecaySize: lrSchedule === 'step' ? lrStepDecaySize : undefined,
                lrExpDecayRate: lrSchedule === 'exponential' ? lrExpDecayRate : undefined
            },
            layers: this.layers.map(l => ({ type: l.type, inputSize: l.inputSize, outputSize: l.outputSize, activation: l.activation, numHeads: l.numHeads, useBias: l.useBias, rate: l.rate }))
        };
        this.details = trainingSummary;
        if (this.debug) console.log("Training finished.", trainingSummary);
        return trainingSummary;
      }

      predict(input) { const wasTraining = this.isTraining; this.isTraining = false; if (!this.layers || this.layers.length === 0) { console.error("Predict: Not initialized."); return null; } if (!Array.isArray(input)) { console.error("Predict: Invalid input."); return null; } let currentInput = [...input];
        // {{ Edit: Use namespaced utility }}
        if (this.usePositionalEncoding) {
            currentInput = oblixUtils.positionalEncoding(currentInput);
        }
        this.lastActivations = [currentInput]; try { for (let i = 0; i < this.layers.length; i++) { const cfg=this.layers[i]; const inputLayer=this.lastActivations[this.lastActivations.length - 1]; if (inputLayer.length !== cfg.inputSize) throw new Error(`L${i}(${cfg.type}): Sz mismatch ${inputLayer.length}!=${cfg.inputSize}`); let output; switch(cfg.type) { case 'dense': const w=this.weights[i], b=this.biases[i]; if (!w) throw new Error(`L${i} Dense: W not init.`); output = Array(cfg.outputSize).fill(0); for(let j=0; j<cfg.outputSize; ++j) { let sum = b ? b[j] : 0; for(let k=0; k<cfg.inputSize; ++k) { sum += inputLayer[k] * w[j][k]; } output[j] = oblixActivations.apply(sum, cfg.activation); } break; case 'layernorm': if (!this.gammas[i] || !this.betas[i]) throw new Error(`L${i} LN: G/B not init.`); const { output: lnOut } = oblixLayerOps.layerNormForward(this, inputLayer, this.gammas[i], this.betas[i]); output = lnOut; break; case 'attention': output = oblixLayerOps.attentionForward(this, inputLayer, cfg.numHeads); break; case 'dropout': output = oblixLayerOps.dropoutForward(this, inputLayer, cfg.rate); break; case 'softmax': output = oblixLayerOps.softmaxForward(this, inputLayer); break; default: throw new Error(`Predict: Unknown type ${cfg.type}`); } this.lastActivations.push(output); } this.isTraining = wasTraining; return this.lastActivations[this.lastActivations.length - 1]; } catch (error) { console.error("Prediction Error:", error); this.lastActivations = null; this.isTraining = wasTraining; return null; } }
      save(name = 'model') { if (!this.layers || this.layers.length === 0) { console.warn("Save: Empty model."); } const numLayers = this.layers.length; const ensureLen = (arr, dv = null) => (!Array.isArray(arr) || arr.length !== numLayers) ? Array(numLayers).fill(dv) : arr; const data = { weights: this.weights, biases: this.biases, gammas: this.gammas, betas: this.betas, layers: this.layers, details: this.details, usePositionalEncoding: this.usePositionalEncoding, optimizerState: { t: this.t, m_dw: ensureLen(this.m_dw), v_dw: ensureLen(this.v_dw), m_db: ensureLen(this.m_db), v_db: ensureLen(this.v_db), m_dgamma: ensureLen(this.m_dgamma), v_dgamma: ensureLen(this.v_dgamma), m_dbeta: ensureLen(this.m_dbeta), v_dbeta: ensureLen(this.v_dbeta), s_dw: ensureLen(this.s_dw), s_db: ensureLen(this.s_db), s_dgamma: ensureLen(this.s_dgamma), s_dbeta: ensureLen(this.s_dbeta) } }; try { const jsonStr = JSON.stringify(data); const blob = new Blob([jsonStr], { type: 'application/json' }); const url = URL.createObjectURL(blob); const a = document.createElement('a'); a.href = url; a.download = `${name}.json`; document.body.appendChild(a); a.click(); document.body.removeChild(a); URL.revokeObjectURL(url); if (this.debug) console.log(`Model saved: ${name}.json`); } catch (e) { console.error("Save failed.", e); } }
       load(callback) { const input = document.createElement('input'); input.type = 'file'; input.accept = '.json'; input.style.display = 'none'; const handleListener = (event) => { const file = event.target.files[0]; if (!file) { cleanup(); return; } const reader = new FileReader(); reader.onload = (e) => { const text = e.target.result; try { const data = JSON.parse(text); if (!data.layers || !Array.isArray(data.layers)) throw new Error("Invalid model: 'layers' missing."); this.layers = data.layers; this.details = data.details || {}; this.usePositionalEncoding = data.usePositionalEncoding || false; const numLayers = this.layers.length; const loadAndCheck = (name, src, len, dv = null) => { let p = src?.[name] || []; if (!Array.isArray(p)) p = []; if (p.length !== len) { const res = Array(len).fill(dv); for (let i = 0; i < Math.min(len, p.length); i++) res[i] = p[i]; return res; } if (name === 'weights') p = p.map(w => Array.isArray(w) ? w : null); if (['m_dw', 'v_dw', 's_dw'].includes(name)) p = p.map((s, i) => (this.weights[i] && !Array.isArray(s)) ? null : s); if (['m_db', 'v_db', 's_db', 'm_dgamma', 'v_dgamma', 's_dgamma', 'm_dbeta', 'v_dbeta', 's_dbeta'].includes(name)) p = p.map((s, i) => { const needs = (this.layers[i]?.type === 'dense' && this.layers[i]?.useBias && this.biases[i]) || (this.layers[i]?.type === 'layernorm' && this.gammas[i]); return (needs && !Array.isArray(s)) ? null : s; }); return p; }; this.weights = loadAndCheck('weights', data, numLayers); this.biases = loadAndCheck('biases', data, numLayers); this.gammas = loadAndCheck('gammas', data, numLayers); this.betas = loadAndCheck('betas', data, numLayers); this.masks = Array(numLayers).fill(null); const optState = data.optimizerState || {}; this.t = optState.t || 0; this.m_dw = loadAndCheck('m_dw', optState, numLayers); this.v_dw = loadAndCheck('v_dw', optState, numLayers); this.m_db = loadAndCheck('m_db', optState, numLayers); this.v_db = loadAndCheck('v_db', optState, numLayers); this.m_dgamma = loadAndCheck('m_dgamma', optState, numLayers); this.v_dgamma = loadAndCheck('v_dgamma', optState, numLayers); this.m_dbeta = loadAndCheck('m_dbeta', optState, numLayers); this.v_dbeta = loadAndCheck('v_dbeta', optState, numLayers); this.s_dw = loadAndCheck('s_dw', optState, numLayers); this.s_db = loadAndCheck('s_db', optState, numLayers); this.s_dgamma = loadAndCheck('s_dgamma', optState, numLayers); this.s_dbeta = loadAndCheck('s_dbeta', optState, numLayers); this.lastActivations = null; this.forwardCache = null; this.isTraining = false; if (callback) callback(); if (this.debug) console.log('Model loaded.'); } catch (err) { console.error('Load failed:', err); alert(`Error loading model: ${err.message}`); if (callback) callback(err); } finally { cleanup(); } }; reader.onerror = (err) => { console.error('File read error:', err); alert('Error reading file.'); cleanup(); if (callback) callback(err); }; reader.readAsText(file); }; const cleanup = () => { input.removeEventListener('change', handleListener); document.body.removeChild(input); }; input.addEventListener('change', handleListener); document.body.appendChild(input); input.click(); }

    } // End oblix class


    // --- UI Interaction Logic ---
    document.addEventListener('DOMContentLoaded', () => {
        const nn = new oblix(true);
        let lossHistory = [];
        const lossCanvas = document.getElementById('lossGraph'); const networkCanvas = document.getElementById('networkGraph'); const lossCtx = lossCanvas?.getContext('2d'); const networkCtx = networkCanvas?.getContext('2d');
        const statsEl = document.getElementById('stats'); const trainButton = document.getElementById('trainButton'); const predictButton = document.getElementById('predictButton'); const saveButton = document.getElementById('saveButton'); const loadButton = document.getElementById('loadButton'); const unloadButton = document.getElementById('unloadButton'); const epochBar = document.getElementById('epochBar'); const predictionResultEl = document.getElementById('predictionResult'); const numHiddenLayersInput = document.getElementById('numHiddenLayers'); const hiddenLayersConfigContainer = document.getElementById('hiddenLayersConfig'); const optimizerSelect = document.getElementById('optimizer'); const usePositionalEncodingCheckbox = document.getElementById('usePositionalEncoding'); const lossFunctionSelect = document.getElementById('lossFunction'); const l2LambdaInput = document.getElementById('l2Lambda'); const decayRateGroup = document.getElementById('decayRateGroup'); const decayRateInput = document.getElementById('decayRate'); const gradientClipValueInput = document.getElementById('gradientClipValue'); const trainingDataTextarea = document.getElementById('trainingData'); const testDataTextarea = document.getElementById('testData'); const epochsInput = document.getElementById('epochs'); const learningRateInput = document.getElementById('learningRate'); const batchSizeInput = document.getElementById('batchSize');
        const lrSchedulerSelect = document.getElementById('lrScheduler');
        const lrStepParamsDiv = document.getElementById('lrStepParams');
        const lrExpParamsDiv = document.getElementById('lrExpParams');
        // {{ Add reference to template select }}
        const architectureTemplateSelect = document.getElementById('architectureTemplateSelect');

        // {{ Define Architecture Templates }}
        const architectureTemplates = {
            'mlp': {
                numHidden: 2,
                layers: [
                    { type: 'dense', size: 16, activation: 'relu', useBias: true },
                    { type: 'dense', size: 8, activation: 'relu', useBias: true }
                ]
            },
            'autoencoder': {
                numHidden: 3,
                layers: [
                    { type: 'dense', size: 16, activation: 'relu', useBias: true }, // Encoder
                    { type: 'dense', size: 4, activation: 'relu', useBias: true },  // Bottleneck
                    { type: 'dense', size: 16, activation: 'relu', useBias: true }  // Decoder
                ],
                // Special hint for final layer config during training
                finalActivationHint: 'sigmoid' // Suggest sigmoid if inputs are 0-1, or 'none' otherwise
            },
            'transformerEncoder': {
                numHidden: 8,
                layers: [
                    // First: Multi-head attention with layer norm
                    { type: 'layernorm', size: null },  // Pre-norm architecture
                    { type: 'attention', numHeads: 3 },
                    { type: 'dense', size: 32, activation: 'relu', useBias: true }, // FFN part 1
                    { type: 'dropout', rate: 0.1 },  // Regularization
                    // Second attention block
                    { type: 'layernorm', size: null },
                    { type: 'attention', numHeads: 4 },
                    { type: 'dense', size: 32, activation: 'relu', useBias: true },
                    { type: 'dropout', rate: 0.1 }
                ],
                // Hint for final layer based on task
                finalActivationHint: 'none'  // Default to linear output
            },
            'residual-attention': {
                numHidden: 5,
                layers: [
                    { type: 'layernorm' },
                    { type: 'attention', numHeads: 3 },
                    { type: 'dense', size: 32, activation: 'relu', useBias: true },
                    { type: 'dropout', rate: 0.1 },
                    { type: 'layernorm' }
                ],
                finalActivationHint: 'none'
            }
            // Add more templates here
        };
        // {{ End Template Definitions }}

        function generateRandomData(numSamples, numInputs, numOutputs = 1, noiseLevel = 0.05) { if (numInputs <= 0 || numOutputs <= 0) return ""; const data = []; for (let i = 0; i < numSamples; i++) { const input = []; for (let j = 0; j < numInputs; j++) input.push(Math.random()); const output = []; for (let j = 0; j < numOutputs; j++) { const base = Math.sin(input[0] * Math.PI * 2) * 0.4 + 0.5; const noise = (Math.random() - 0.5) * 2 * noiseLevel; let final = Math.max(0.01, Math.min(0.99, base + noise)); output.push(final); } data.push([...input, ...output].map(v => v.toFixed(3)).join(', ')); } return data.join('\n'); }
        document.getElementById("loadDataBtn").onclick = () => { const numTrain=100, numTest=25, numIn=3, numOut=1; trainingDataTextarea.value = generateRandomData(numTrain, numIn, numOut); testDataTextarea.value = generateRandomData(numTest, numIn, numOut); statsEl.innerHTML = `Generated ${numTrain}/${numTest} random samples.`; try { const sample = parseCSV(trainingDataTextarea.value)[0]; if(sample && nn.layers && nn.layers.length > 0) nn.predict(sample.input); } catch (e) {} drawNetwork(); }
        function parseCSV(csvString) { if (!csvString || typeof csvString !== 'string') return []; return csvString.trim().split('\n').map(r=>r.trim()).filter(r=>r.length>0).map((r,idx)=>{ const v=r.split(',').map(v=>parseFloat(v.trim())); if(v.some(isNaN)){console.warn(`R ${idx+1} NaN`);return null;} if(v.length<2){console.warn(`R ${idx+1} <2 vals`);return null;} const i=v.slice(0,-1), o=v.slice(-1); if(i.length===0){console.warn(`R ${idx+1} no input`);return null;} return {input:i,output:o}; }).filter(i=>i!==null); }
        function drawLossGraph() { if (!lossCtx || !lossCanvas) return; lossCtx.clearRect(0, 0, lossCanvas.width, lossCanvas.height); if (lossHistory.length < 2) return; const trainL=lossHistory.map(h=>h.train).filter(l=>l!==null&&isFinite(l)); const testL=lossHistory.map(h=>h.test).filter(l=>l!==null&&isFinite(l)); let maxL=0.1; if(trainL.length>0)maxL=Math.max(maxL,...trainL); if(testL.length>0)maxL=Math.max(maxL,...testL); maxL=Math.max(maxL,0.1); const W=lossCanvas.width,H=lossCanvas.height,nPts=lossHistory.length,pH=H*0.9,yOff=H*0.05; const plot=(ctx,pts,c)=>{ ctx.strokeStyle=c; ctx.lineWidth=1.5; ctx.beginPath(); let first=true; pts.forEach((p,i)=>{if(p!==null&&isFinite(p)){const x=(i/Math.max(1,nPts-1))*W; const y=H-(p/maxL)*pH-yOff; if(first){ctx.moveTo(x,y);first=false;}else{ctx.lineTo(x,y);}}else{first=true;}}); ctx.stroke(); }; const trainC=getComputedStyle(document.body).getPropertyValue('--text')?.trim()||'#fff'; plot(lossCtx,lossHistory.map(h=>h.train),trainC); plot(lossCtx,lossHistory.map(h=>h.test),'#87CEEB'); }

         function createLayerConfigUI(numLayers) {
            hiddenLayersConfigContainer.innerHTML = '';
            const activationTypes = ['tanh', 'sigmoid', 'relu', 'leakyrelu', 'gelu', 'selu', 'swish', 'mish', 'softmax', 'none'];
            const layerTypes = ['dense', 'layernorm', 'attention', 'dropout', 'softmax'];
            if (numLayers === 0) { hiddenLayersConfigContainer.innerHTML = '<p class="layer-note">No hidden layers. Direct input-to-output connection (final layer added automatically).</p>'; return; }
            for (let i = 0; i < numLayers; i++) { const layerGroup = document.createElement('div'); layerGroup.className = 'input-group settings-grid'; const typeDiv = document.createElement('div'); typeDiv.className = 'input-group'; const typeLabel = document.createElement('label'); typeLabel.textContent = `Layer ${i+1} Type:`; typeLabel.htmlFor = `layerType_${i}`; typeLabel.title = "Choose the operation for this layer..."; const typeSelect = document.createElement('select'); typeSelect.id = `layerType_${i}`; typeSelect.dataset.layerIndex = i; typeSelect.dataset.configType = 'type'; layerTypes.forEach(t => { const o = document.createElement('option'); o.value = t; o.textContent = t; if (t === 'dense') o.selected = true; typeSelect.appendChild(o); }); typeDiv.appendChild(typeLabel); typeDiv.appendChild(typeSelect); layerGroup.appendChild(typeDiv); const optionsDiv = document.createElement('div'); optionsDiv.className = 'layer-options-container'; optionsDiv.dataset.layerIndex = i; layerGroup.appendChild(optionsDiv);
            // {{ Add listener to layer type select for manual changes }}
            typeSelect.addEventListener('change', () => architectureTemplateSelect.value = 'custom');

            const updateOptionsUI = (idx, selType) => { const optsDiv = hiddenLayersConfigContainer.querySelector(`.layer-options-container[data-layer-index='${idx}']`); if (!optsDiv) return; optsDiv.innerHTML = ''; const createIn = (l,id,t,v,mn,st,cfg,nt=null,tt=null)=>{const dv=document.createElement('div'); dv.className='input-group'; const lb=document.createElement('label'); lb.textContent=l; lb.htmlFor=id; if(tt)lb.title=tt; const ip=document.createElement('input'); ip.type=t; ip.id=id; ip.value=v; if(mn!==null)ip.min=mn; if(st!==null)ip.step=st; ip.dataset.layerIndex=idx; ip.dataset.configType=cfg; dv.appendChild(lb); dv.appendChild(ip); if(nt){const p=document.createElement('p');p.className='layer-note';p.textContent=nt;dv.appendChild(p);} return dv;}; const createSel = (l,id,opts,selV,cfg,tt=null)=>{const dv=document.createElement('div'); dv.className='input-group'; const lb=document.createElement('label'); lb.textContent=l; lb.htmlFor=id; if(tt)lb.title=tt; const sel=document.createElement('select'); sel.id=id; sel.dataset.layerIndex=idx; sel.dataset.configType=cfg; opts.forEach(o=>{const op=document.createElement('option'); op.value=o; op.textContent=o; if(o===selV)op.selected=true; sel.appendChild(op);}); dv.appendChild(lb); dv.appendChild(sel); return dv;}; const createChk=(l,id,chkd,cfg,tt=null)=>{const dv=document.createElement('div'); dv.className='input-group'; const lb=document.createElement('label'); const ip=document.createElement('input'); ip.type='checkbox'; ip.id=id; ip.checked=chkd; ip.dataset.layerIndex=idx; ip.dataset.configType=cfg; const sp=document.createElement('span'); sp.textContent=l; if(tt)sp.title=tt; lb.appendChild(ip); lb.appendChild(sp); dv.appendChild(lb); return dv;}; const createNt=(txt)=>{const p=document.createElement('p'); p.className='layer-note'; p.textContent=txt; const dv=document.createElement('div'); dv.style.gridColumn='1/-1'; dv.appendChild(p); return dv;};
            if(selType==='dense'){optsDiv.appendChild(createIn('Nodes:',`layerNodes_${idx}`, 'number', 10, 1, 1, 'size', null,"Num neurons.")); optsDiv.appendChild(createSel('Activation:',`layerAct_${idx}`, activationTypes, 'tanh', 'activation', "Neuron output function.")); optsDiv.appendChild(createChk('Use Bias:',`layerBias_${idx}`, true, 'useBias', "Add learnable bias term?"));} else if(selType==='attention'){optsDiv.appendChild(createIn('Num Heads:',`layerHeads_${idx}`, 'number', 2, 1, 1, 'numHeads', null,"Parallel attention mechanisms. Input size must be divisible by heads.")); optsDiv.appendChild(createNt('Input size must be divisible by Num Heads. Output size matches input.'));} else if(selType==='layernorm'){optsDiv.appendChild(createNt('Normalizes features across the feature dimension.'));} else if(selType==='dropout'){optsDiv.appendChild(createIn('Dropout Rate:',`layerRate_${idx}`, 'number', 0.5, 0, 0.01, 'rate', 'Fraction of neurons to zero out during training (0 to <1). Helps prevent overfitting.',"Higher value means more dropout."));} else if(selType==='softmax'){optsDiv.appendChild(createNt('Outputs probabilities summing to 1. For multi-class classification.'));}};
            typeSelect.addEventListener('change', (event) => updateOptionsUI(i, event.target.value)); hiddenLayersConfigContainer.appendChild(layerGroup); updateOptionsUI(i, typeSelect.value); }
        }
        numHiddenLayersInput.addEventListener('change', (event) => { const numLayers = Math.max(0, parseInt(event.target.value) || 0); event.target.value = numLayers; createLayerConfigUI(numLayers);
        // {{ Reset template dropdown on manual layer count change }}
        architectureTemplateSelect.value = 'custom';
        }); createLayerConfigUI(parseInt(numHiddenLayersInput.value));
        optimizerSelect.addEventListener('change', () => { decayRateGroup.style.display = optimizerSelect.value === 'rmsprop' ? 'block' : 'none'; }); decayRateGroup.style.display = optimizerSelect.value === 'rmsprop' ? 'block' : 'none';

        lrSchedulerSelect.addEventListener('change', () => {
            const selectedSchedule = lrSchedulerSelect.value;
            lrStepParamsDiv.style.display = selectedSchedule === 'step' ? 'grid' : 'none'; // Use grid for settings-grid
            lrExpParamsDiv.style.display = selectedSchedule === 'exponential' ? 'grid' : 'none'; // Use grid for settings-grid
        });
        lrSchedulerSelect.dispatchEvent(new Event('change'));

        // {{ Add Template Selector Logic }}
        architectureTemplateSelect.addEventListener('change', (event) => {
            const templateKey = event.target.value;
            if (templateKey === 'custom') {
                // Allow manual configuration, do nothing here
                return;
            }

            const template = architectureTemplates[templateKey];
            if (!template) {
                console.warn("Selected template not found:", templateKey);
                return;
            }

            statsEl.innerHTML = `Applying '${templateKey}' template...`;

            // 1. Set number of hidden layers
            numHiddenLayersInput.value = template.numHidden;

            // 2. Trigger change to regenerate UI (IMPORTANT)
            numHiddenLayersInput.dispatchEvent(new Event('change'));

            // 3. Use setTimeout to allow DOM update before populating
            setTimeout(() => {
                try {
                    template.layers.forEach((layerConfig, i) => {
                        const setV=(s,v)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&v!==undefined)e.value=v;};
                        const setC=(s,c)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&c!==undefined)e.checked=c;};
                        const triggerChange=(s)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e) e.dispatchEvent(new Event('change',{bubbles:true}));}; // Helper to trigger change

                        // Set layer type first and trigger change to update options UI
                        setV(`select[data-layer-index="${i}"][data-config-type="type"]`, layerConfig.type || 'dense');
                        triggerChange(`select[data-layer-index="${i}"][data-config-type="type"]`); // Crucial!

                        // Set other config values based on type
                        switch(layerConfig.type){
                            case 'dense':
                                setV(`input[data-layer-index="${i}"][data-config-type="size"]`, layerConfig.size);
                                setV(`select[data-layer-index="${i}"][data-config-type="activation"]`, layerConfig.activation);
                                setC(`input[data-layer-index="${i}"][data-config-type="useBias"]`, layerConfig.useBias);
                                break;
                            case 'attention':
                                setV(`input[data-layer-index="${i}"][data-config-type="numHeads"]`, layerConfig.numHeads);
                                break;
                            case 'dropout':
                                setV(`input[data-layer-index="${i}"][data-config-type="rate"]`, layerConfig.rate);
                                break;
                            // LayerNorm and Softmax have no specific config values in this simple setup
                        }
                    });
                    // {{ Edit 1: Apply blue color and bold to only the template name }}
                    statsEl.innerHTML = `Applied <span style="color: #87CEEB; font-weight: bold;">${templateKey}</span> template. Ready.`;
                 } catch (error) {
                     console.error("Error applying template UI:", error);
                     statsEl.innerHTML = `<span class="error">Error applying template: ${error.message}</span>`;
                     architectureTemplateSelect.value = 'custom'; // Revert on error
                 }
            }, 0); // Timeout needed to ensure createLayerConfigUI finishes DOM manipulation
        });
        // {{ End Template Selector Logic }}


        trainButton.addEventListener('click', async () => {
            statsEl.innerHTML = 'Starting training...'; trainButton.disabled = true; trainButton.textContent = 'Training...'; predictButton.disabled = true; saveButton.disabled = true; loadButton.disabled = true; unloadButton.disabled = true; epochBar.style.width = '0%'; lossHistory = []; drawLossGraph();
            try {
                const trainingData = parseCSV(trainingDataTextarea.value); const testData = parseCSV(testDataTextarea.value); if (trainingData.length === 0) throw new Error("Training data empty/invalid."); if (!trainingData[0]?.input || !trainingData[0]?.output) throw new Error("Cannot get input/output size from data.");
                nn.reset(); const numHidden = parseInt(numHiddenLayersInput.value); const layerCfgs = []; const numIns = trainingData[0].input.length; let curInSize = numIns;
                for (let i = 0; i < numHidden; i++) { const getV=(s)=>hiddenLayersConfigContainer.querySelector(s)?.value; const getC=(s)=>hiddenLayersConfigContainer.querySelector(s)?.checked; const lType=getV(`select[data-layer-index="${i}"][data-config-type="type"]`) || 'dense'; let cfg={type:lType, inputSize:curInSize}; switch(lType){ case 'dense': cfg.outputSize=parseInt(getV(`input[data-layer-index="${i}"][data-config-type="size"]`)||1); if(cfg.outputSize<=0) throw new Error(`L${i+1} Dense: Invalid nodes.`); cfg.activation=getV(`select[data-layer-index="${i}"][data-config-type="activation"]`) || 'tanh'; cfg.useBias=getC(`input[data-layer-index="${i}"][data-config-type="useBias"]`)??true; break; case 'attention': cfg.numHeads=parseInt(getV(`input[data-layer-index="${i}"][data-config-type="numHeads"]`)||1); if(cfg.numHeads<=0) throw new Error(`L${i+1} Attn: Invalid heads.`); if(curInSize%cfg.numHeads!==0) throw new Error(`L${i+1} Attn: Input ${curInSize} not divisible by ${cfg.numHeads} heads.`); cfg.outputSize=curInSize; break; case 'dropout': cfg.rate=parseFloat(getV(`input[data-layer-index="${i}"][data-config-type="rate"]`)||0); if(cfg.rate<0||cfg.rate>=1) throw new Error(`L${i+1} Dropout: Invalid rate.`); cfg.outputSize=curInSize; break; case 'layernorm': case 'softmax': cfg.outputSize=curInSize; break; default: throw new Error(`L${i+1}: Unknown type "${lType}".`); } if(!cfg.outputSize) throw new Error(`L${i+1}: No output size.`); layerCfgs.push(cfg); curInSize=cfg.outputSize; }
                // {{ Adjust final layer based on template and data }}
                const isAutoencoder = architectureTemplateSelect.value === 'autoencoder';
                const numOuts = isAutoencoder ? numIns : trainingData[0].output.length;
                if(numOuts <= 0) throw new Error("Zero output cols defined.");

                const selectedLoss = lossFunctionSelect.value; // Get selected loss function

                // {{ Edit: Refined final activation logic }}
                let finalAct = 'none'; // Default to linear activation ('none') for regression (MSE)

                if (selectedLoss === 'crossentropy') {
                    if (isAutoencoder) {
                         // Autoencoders typically reconstruct input, often using sigmoid/tanh depending on input range
                    finalAct = architectureTemplates['autoencoder']?.finalActivationHint || 'sigmoid';
                         console.log(`Autoencoder with Cross-Entropy? Using template hint or sigmoid: ${finalAct}`);
                         // Note: Using CE loss with autoencoders is less common than MSE unless inputs are treated as probabilities.
                    } else if (numOuts > 1) {
                         // Multi-class classification
                         finalAct = 'softmax';
                    } else if (numOuts === 1) {
                         // Binary classification
                         finalAct = 'sigmoid';
                    }
                } else if (selectedLoss === 'mse') {
                     if (isAutoencoder) {
                         // Autoencoders with MSE often use sigmoid/tanh if input is bounded, or none otherwise
                         finalAct = architectureTemplates['autoencoder']?.finalActivationHint || 'sigmoid'; // Keep sigmoid/tanh hint possibility
                         console.log(`Autoencoder with MSE, using template hint or sigmoid: ${finalAct}`);
                     } else {
                         // Standard regression - default is already 'none'
                         // If target values are known to be bounded (e.g., 0-1 or -1 to 1),
                         // tanh or sigmoid *might* be appropriate, but 'none' is safer general default.
                         finalAct = 'none';
                         console.log('MSE loss selected, using linear final activation: none');
                     }
                }
                // If a different loss function were added later, it would default to 'none' here.

                console.log(`Adding final dense: ${curInSize}->${numOuts} (Act:${finalAct})`);
                layerCfgs.push({type:'dense', inputSize:curInSize, outputSize:numOuts, activation:finalAct, useBias:true});
                // {{ End final layer adjustment }}

                layerCfgs.forEach((cfg,i)=>{try{nn.layer(cfg);}catch(e){throw new Error(`Cfg L${i+1}(${cfg.type}): ${e.message}`);}}); if(nn.layers.length===0)throw new Error("Zero layers configured."); if(nn.debug)console.log("Net structure:",nn.layers);
                const opts={
                    epochs:parseInt(epochsInput.value)||50,
                    learningRate:parseFloat(learningRateInput.value)||0.01, // Initial LR
                    batchSize:parseInt(batchSizeInput.value)||8,
                    testSet:testData.length>0?testData:null,
                    optimizer:optimizerSelect.value,
                    lossFunction:lossFunctionSelect.value,
                    l2Lambda:parseFloat(l2LambdaInput.value)||0,
                    decayRate:parseFloat(decayRateInput.value)||0.9,
                    gradientClipValue:parseFloat(gradientClipValueInput.value)||0,
                    usePositionalEncoding:usePositionalEncodingCheckbox.checked,
                    lrSchedule: lrSchedulerSelect.value,
                    lrStepDecayFactor: parseFloat(document.getElementById('lrStepDecayFactor').value) || 0.1,
                    lrStepDecaySize: parseInt(document.getElementById('lrStepDecaySize').value) || 10,
                    lrExpDecayRate: parseFloat(document.getElementById('lrExpDecayRate').value) || 0.95,
                    callback:async(ep,trL,tstL,metricName,metricVal)=>{
                        lossHistory.push({train:trL,test:tstL});
                        drawLossGraph();
                        epochBar.style.width=`${(ep/opts.epochs)*100}%`;

                        const currentLR = nn.getCurrentLearningRate(ep -1, opts.learningRate, opts); // ep is 1-based, need 0-based for calc
                        const lrString = opts.lrSchedule !== 'none' ? ` | LR: ${currentLR.toExponential(2)}` : '';

                        let statusText = `Ep:${ep}/${opts.epochs} | Loss:${trL.toFixed(6)}`;
                        if (tstL !== null) {
                             statusText += ` | Val Loss:${tstL.toFixed(6)}`;
                        }
                        // {{ Add: Display metric in status if available }}
                        if (metricName && metricVal !== null && !isNaN(metricVal)) {
                             statusText += ` | Val ${metricName}:${metricVal.toFixed(4)}`;
                        }
                         statusText += lrString;
                         statsEl.innerHTML = statusText;

                        await new Promise(requestAnimationFrame); // Yield for UI update
                    }
                };
                statsEl.innerHTML=`Training (${opts.optimizer}, ${opts.lossFunction}` + (opts.lrSchedule !== 'none' ? `, ${opts.lrSchedule} LR` : '') + `)...`;
                const summary=await nn.train(trainingData,opts);
                const totalParams=nn.getTotalParameters();
                statsEl.innerHTML=`<strong>Done!</strong> Loss:${summary.trainLoss.toFixed(6)}`+(summary.testLoss!==null?`, Val:${summary.testLoss.toFixed(6)}`:'')+` | Params:${totalParams.toLocaleString()}`;
                console.log("Final Summary:",summary);
                if(trainingData.length>0){try{nn.predict(trainingData[0].input);drawNetwork();}catch(e){}}
            } catch (error) { console.error('Train err:', error); statsEl.innerHTML = `<span class="error">Error: ${error.message}</span>`; }
            finally { trainButton.disabled=false; trainButton.textContent='Train Model'; predictButton.disabled=false; saveButton.disabled=false; loadButton.disabled=false; unloadButton.disabled=false; }
        });

        function drawNetwork() {
            if (!networkCtx || !networkCanvas) return;
            const networkContainer = networkCanvas.parentElement; // Should be #network-viz-container
            if (!networkContainer) return;
            const containerWidth = networkContainer.clientWidth; const containerHeight = networkContainer.clientHeight;
            networkCtx.clearRect(0, 0, networkCanvas.width, networkCanvas.height); // Clear previous drawing
            const hasModel = nn.lastActivations && nn.lastActivations.length > 0 && nn.layers && nn.layers.length > 0;

            if (!hasModel) {
                networkCtx.fillStyle = "#555"; networkCtx.font = "10px monospace"; networkCtx.textAlign = "center"; networkCtx.textBaseline = "middle";
                // Set canvas size to container size if no model to draw
                if (networkCanvas.width !== containerWidth) networkCanvas.width = containerWidth;
                if (networkCanvas.height !== containerHeight) networkCanvas.height = containerHeight;
                networkCtx.fillText("Train/Predict to visualize", containerWidth / 2, containerHeight / 2);
                return;
            }

            // Configs
            const pad=35, maxNds=20, nRBase=2, nRScale=3, cBOp=0.02, cMOp=0.85, cWScale=2, ellOff=10, lblOff=20, lblFnt="10px monospace", lblClr="#aaa";
            const nVizLyrs=nn.lastActivations.length;

            // Adjust spacing based on number of layers
            // More spacing for fewer layers, minimum spacing for many layers
            const baseSpacing = 150; // Base spacing between layers (increased from 70)
            const minLayerSpacing = Math.max(120, Math.min(baseSpacing, containerWidth / (nVizLyrs > 1 ? nVizLyrs : 1)));

            // Calculate required width with improved spacing
            const requiredWidth = (nVizLyrs <= 1)
                ? containerWidth
                : pad * 2 + (nVizLyrs - 1) * minLayerSpacing;

            // Ensure we have at least 20% more space than the minimal calculated width
            // This creates more visual breathing room
            const canvasDrawWidth = Math.max(containerWidth, requiredWidth * 1.2);

            // Set Canvas Attributes
            networkCanvas.width = canvasDrawWidth; networkCanvas.height = containerHeight;

            // Drawing dimensions
            const drawAreaWidth = canvasDrawWidth - pad * 2; const drawAreaHeight = containerHeight - pad * 2;
            const layerXs = Array.from({ length: nVizLyrs }, (_, i) => pad + (nVizLyrs === 1 ? drawAreaWidth / 2 : (drawAreaWidth * i) / (nVizLyrs - 1)));

            // 1. Calculate Node Positions
            const layerPos = [];
            nn.lastActivations.forEach((act, lIdx) => {
                if (!Array.isArray(act)) { layerPos.push([]); return; }
                const lNodes = []; const nNodes = act.length; const dNodes = Math.min(nNodes, maxNds); const lX = layerXs[lIdx];
                for (let j = 0; j < dNodes; j++) { const origIdx = nNodes <= maxNds ? j : Math.floor(j * nNodes / dNodes); const nodeVal = act[origIdx]; const nY = pad + (dNodes === 1 ? drawAreaHeight / 2 : (drawAreaHeight * j) / (dNodes - 1)); lNodes.push({ x: lX, y: nY, value: (typeof nodeVal === 'number' && isFinite(nodeVal) ? nodeVal : 0) }); } // Ensure value is finite number
                if (nNodes > maxNds) lNodes.push({ x: lX, y: pad + drawAreaHeight + ellOff, value: 0, isEllipsis: true, originalCount: nNodes });
                layerPos.push(lNodes);
            });

            // 2. Draw Connections
            networkCtx.lineWidth = 1;
            for (let i = 0; i < nVizLyrs - 1; i++) {
                const curNodes = layerPos[i].filter(n => !n.isEllipsis);
                const nextNodes = layerPos[i + 1].filter(n => !n.isEllipsis);
                const cfg = nn.layers[i];
                if (!cfg) continue;

                const isDenseW = cfg.type === 'dense' && Array.isArray(nn.weights?.[i]);
                const w = isDenseW ? nn.weights[i] : null;

                for (let j = 0; j < curNodes.length; j++) {
                    for (let k = 0; k < nextNodes.length; k++) {
                        let op = 0.1, col = '100,100,100', lw = 0.5;
                        let lineDash = []; // Default: solid line

                        if (isDenseW && w?.[k]?.[j] !== undefined) {
                            // --- Dense layer: Keep existing logic (weighted white/blue) ---
                            const weight = w[k][j];
                            if (typeof weight === 'number' && typeof curNodes[j].value === 'number') {
                                const wMag = Math.tanh(Math.abs(weight));
                                const aMag = Math.tanh(Math.abs(curNodes[j].value));
                                const combSig = (wMag * 0.8 + aMag * 0.2);
                                op = Math.min(Math.max(cBOp + combSig * (cMOp - cBOp), cBOp), cMOp); // op ~ 0.02 to 0.85
                                col = weight >= 0 ? '255,255,255' : '180,180,255'; // White/Blue
                                lw = Math.min(Math.max(0.5, op * cWScale), 2); // lw ~ 0.5 to 2
                                lineDash = []; // Explicitly solid
                            }
                             // --- End dense logic ---
                        } else if (cfg.type === 'attention') {
                            // --- Attention: Bright Cyan ---
                            op = 0.5; lw = 1.2; col = '0,200,200'; lineDash = []; // Solid line
                        } else if (cfg.type === 'layernorm') {
                            // --- LayerNorm: Yellow, dashed ---
                            op = 0.4;
                            lw = 0.8;
                            col = '255,255,0'; // Yellow
                            lineDash = [2, 2]; // Simple dash
                        } else if (cfg.type === 'dropout') {
                             // --- Dropout: Orange, dashed, relatively faint ---
                            op = 0.2; // Still low opacity
                            lw = 0.6; // Still thin
                            col = '255,165,0'; // Orange
                            lineDash = [3, 3]; // Dashed line
                        } else if (cfg.type === 'softmax') {
                             // --- Softmax: Magenta/Purple ---
                             op = 0.4; lw = 1.0; col = '200,0,200'; lineDash = []; // Solid line
                        } else {
                             // Fallback
                             op = 0.1; lw = 0.5; col = '100,100,100'; lineDash = []; // Solid line
                        }

                        networkCtx.strokeStyle = `rgba(${col},${op})`;
                        networkCtx.lineWidth = lw;
                        networkCtx.setLineDash(lineDash || []); // Use specified dash or default solid
                        networkCtx.beginPath();
                        networkCtx.moveTo(curNodes[j].x, curNodes[j].y);
                        networkCtx.lineTo(nextNodes[k].x, nextNodes[k].y);
                        networkCtx.stroke();
                        networkCtx.setLineDash([]); // Reset to solid lines for next iteration
                    }
                }
            }

            // 3. Draw Nodes and Labels
            networkCtx.textAlign = "center";

            // Draw the "Layers" label centered over all hidden layers
            if (layerXs.length > 2) {
                // Calculate center position between first hidden layer and last hidden layer
                const firstHiddenLayerX = layerXs[1]; // Index 1 is first hidden layer
                const lastHiddenLayerX = layerXs[layerXs.length - 2]; // Second-to-last is last hidden layer
                const centerX = (firstHiddenLayerX + lastHiddenLayerX) / 2;

                // Draw "Layers" label at the calculated center position
                networkCtx.fillStyle = lblClr;
                networkCtx.font = lblFnt;
                networkCtx.textBaseline = "bottom";
                networkCtx.fillText("Layers", centerX, pad - lblOff / 2);
            }

            layerPos.forEach((lNodes, lIdx) => {
                // Draw Label
                networkCtx.fillStyle = lblClr; networkCtx.font = lblFnt; networkCtx.textBaseline = "bottom";
                if (lIdx === 0) {
                    networkCtx.fillText("Input", layerXs[lIdx], pad - lblOff / 2);
                } else if (lIdx === layerPos.length - 1) {
                    networkCtx.fillText("Output", layerXs[lIdx], pad - lblOff / 2);
                }
                // Draw Nodes
                lNodes.forEach(n => {
                    if (n.isEllipsis) { networkCtx.fillStyle = "#777"; networkCtx.font = "10px monospace"; networkCtx.textBaseline = "top"; networkCtx.fillText(`(${n.originalCount} nodes)`, n.x, n.y); }
                    else { const actStr = Math.tanh(Math.abs(n.value)); const r = nRBase + actStr * nRScale; const op = 0.3 + actStr * 0.7; const col = n.value >= 0 ? '255,255,255' : '200,200,255'; networkCtx.fillStyle = `rgba(${col},${op})`; networkCtx.strokeStyle = 'rgba(255,255,255,0.6)'; networkCtx.lineWidth = 1; networkCtx.beginPath(); networkCtx.arc(n.x, n.y, r, 0, Math.PI * 2); networkCtx.fill(); networkCtx.stroke(); }
                });
            });
        }
        function resizeCanvases() {
             const lossContainer = lossCanvas?.parentElement;
             // Target the specific container for the network graph
             const networkContainer = document.getElementById('network-viz-container');

             // Resize and redraw loss graph as before
             if (lossContainer?.clientWidth > 0 && lossCanvas) {
                 lossCanvas.width = lossContainer.clientWidth;
                 lossCanvas.height = lossContainer.clientHeight;
                 drawLossGraph();
             }
             // For network graph, just call drawNetwork. It will read the container size
             // and set its own canvas width/height appropriately before drawing.
             if (networkContainer?.clientWidth > 0 && networkCanvas) {
                 drawNetwork();
             }
        }
        window.addEventListener('resize', resizeCanvases); setTimeout(resizeCanvases, 150);

        saveButton.addEventListener('click', () => { if (!nn.layers || nn.layers.length === 0) { statsEl.innerHTML = '<span class="error">No model to save.</span>'; return; } nn.save('oblix_model'); statsEl.innerHTML = 'Model saved.'; });
        loadButton.addEventListener('click', () => { statsEl.innerHTML = 'Loading...'; nn.load((error) => { if (error) { statsEl.innerHTML = `<span class="error">Load failed: ${error.message}</span>`; return; } const params = nn.getTotalParameters(); statsEl.innerHTML = `<strong>Model loaded!</strong> Params: ${params.toLocaleString()}`; try { const d = nn.details?.training; const l = nn.layers || []; usePositionalEncodingCheckbox.checked = nn.usePositionalEncoding || false; if (d) { epochsInput.value=d.epochs||50; learningRateInput.value=d.learningRate||0.01; batchSizeInput.value=d.batchSize||8; optimizerSelect.value=d.optimizer||'adam'; lossFunctionSelect.value=d.lossFunction||'mse'; l2LambdaInput.value=d.l2Lambda||0; decayRateInput.value=d.decayRate||0.9; gradientClipValueInput.value=d.gradientClipValue||0; optimizerSelect.dispatchEvent(new Event('change')); } const numHid = Math.max(0, l.length - 1); numHiddenLayersInput.value = numHid; createLayerConfigUI(numHid); l.slice(0, numHid).forEach((layer, i) => { const setV=(s,v)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&v!==undefined)e.value=v;}; const setC=(s,c)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&c!==undefined)e.checked=c;}; setV(`select[data-layer-index="${i}"][data-config-type="type"]`, layer.type||'dense'); const ts=hiddenLayersConfigContainer.querySelector(`select[data-layer-index="${i}"][data-config-type="type"]`); if(ts)ts.dispatchEvent(new Event('change',{bubbles:true})); switch(layer.type){ case 'dense': setV(`input[data-layer-index="${i}"][data-config-type="size"]`, layer.outputSize); setV(`select[data-layer-index="${i}"][data-config-type="activation"]`, layer.activation); setC(`input[data-layer-index="${i}"][data-config-type="useBias"]`, layer.useBias); break; case 'attention': setV(`input[data-layer-index="${i}"][data-config-type="numHeads"]`, layer.numHeads); break; case 'dropout': setV(`input[data-layer-index="${i}"][data-config-type="rate"]`, layer.rate); break; } });
                // {{ Set template dropdown to Custom after load }}
                architectureTemplateSelect.value = 'custom';
                lossHistory=[]; drawLossGraph(); predictionResultEl.innerHTML='Result: -'; try { const sample = parseCSV(trainingDataTextarea.value)[0]; if(sample) nn.predict(sample.input); } catch(e) {} drawNetwork(); } catch (uiError) { console.error("UI update err after load:", uiError); statsEl.innerHTML += ` <span class="error">(UI update failed)</span>`; } }); });
        predictButton.addEventListener('click', () => { predictionResultEl.innerHTML=`Predicting...`; try { const inputStr = document.getElementById('predictionInput').value; if(!inputStr)throw new Error("Input empty."); const input=inputStr.split(',').map(s=>parseFloat(s.trim())); if(input.some(isNaN))throw new Error("Invalid input."); if(!nn.layers||nn.layers.length===0)throw new Error("Network not init."); const expectSz=nn.layers[0]?.inputSize; if(expectSz===undefined)throw new Error("Cannot get input size."); if(input.length!==expectSz)throw new Error(`Input size mismatch: Exp ${expectSz}, got ${input.length}.`); const pred=nn.predict(input); if(pred===null)throw new Error("Prediction failed."); const predStr=pred.map(p=>p.toFixed(5)).join(', '); predictionResultEl.innerHTML=`Result: [${predStr}]`; drawNetwork(); } catch (error) { console.error("Predict error:", error); predictionResultEl.innerHTML = `<span class="error">Error: ${error.message}</span>`; } });
        unloadButton.addEventListener('click', () => { console.log("Unload clicked."); try { nn.reset(); lossHistory = []; drawLossGraph(); drawNetwork(); epochBar.style.width = '0%'; statsEl.innerHTML = 'Status: Model unloaded.'; predictionResultEl.innerHTML = 'Result: -'; const defaultLayers = 2; numHiddenLayersInput.value = defaultLayers; createLayerConfigUI(defaultLayers);
            // {{ Set template dropdown to Custom after unload/reset }}
            architectureTemplateSelect.value = 'custom';
             console.log("Model & UI reset."); } catch (error) { console.error("Unload error:", error); statsEl.innerHTML = `<span class="error">Unload error: ${error.message}</span>`; } });

    }); // End DOMContentLoaded
  </script>
</body>
</html>
