<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>oblix</title>
    <style>
    /* --- Base Styles --- */
    a { color: white; }
    body {
      background: #000000; color: #fff; font-family: monospace;
      margin: 0; padding: 3% 5%; /* Adjusted padding */
      display: flex; flex-direction: column; gap: 15px;
      overflow-x: hidden;
    }
    h3 { margin: 1rem 0; } /* Adjusted margin */
    p { margin: 0 0 1rem 0; color: #aaaaaac8; line-height: 1.4; } /* Adjusted margin & color */
    .grid {
      display: grid;
      grid-template-columns: minmax(400px, 1.5fr) minmax(300px, 2fr); /* Adjusted ratios */
      gap: 20px; /* Increased gap */
      opacity: 0; transform: translateY(20px);
      animation: fadeInUp 0.5s ease-out forwards;
    }
    .widget {
      background: #111; border: 1px solid #333; border-radius: 10px;
      padding: 20px; /* Increased padding */
      box-sizing: border-box; width: 100%;
      opacity: 0; transform: translateY(20px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.1s; /* Faster delay */
      margin-bottom: 20px;
    }
    .widget-title {
      font-size: 1.2em; /* Larger title */
      margin: 0 0 15px 0; /* Adjusted margin */
      border-bottom: 1px solid #444; /* Lighter border */
      padding-bottom: 10px;
      opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.2s;
    }
    .input-group {
      margin-bottom: 15px; /* Increased margin */
      opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.3s; /* Staggered delay */
    }
    .input-group label {
        display: block; margin-bottom: 5px; /* Increased margin */
        font-size: 0.9em; color: #bbb; /* Lighter label */
        cursor: help; /* Indicate tooltips exist */
    }
    /* Checkbox label specific style */
    .input-group label input[type="checkbox"] {
        margin-right: 8px;
        vertical-align: middle;
        cursor: pointer;
    }
     .input-group label span { /* For inline text next to checkbox */
        vertical-align: middle;
        cursor: help;
     }

    .settings-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(130px, 1fr)); /* Adjusted minmax */
      gap: 15px; /* Increased gap */
      margin-bottom: 15px;
      opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.4s;
    }
    input[type="text"], input[type="number"], select, textarea {
      outline: none; width: 100%; padding: 8px; /* Increased padding */
      background: #222; border: 1px solid #444; color: #fff;
      border-radius: 6px; /* Slightly less rounded */
      box-sizing: border-box; transition: background 0.3s, border 0.3s;
      font-family: monospace; font-size: 0.95em; /* Slightly larger font */
    }
    #loadDataBtn {
      background-color: #eee; color: black; font-weight: 600;
      font-size: 12px; padding: 2px 5px; border-radius: 3px; cursor: pointer;
      transition: background-color 0.2s, color 0.2s; border: 1px solid #888;
    }
    #loadDataBtn:hover { background-color: #ccc; }
    input:focus, select:focus, textarea:focus { background: #333; border: 1px solid #777; } /* Lighter focus */
    button {
      background: #eee; color: #000; border: none; padding: 8px 15px; /* Increased padding */
      border-radius: 6px; cursor: pointer; transition: all 0.15s ease;
      border: 1px solid #888; /* Lighter border */
      opacity: 0; height: auto; /* Auto height */
      transform: translateY(10px); animation: fadeInUp 0.5s ease-out forwards;
      animation-delay: 0.5s; font-family: monospace; font-weight: bold;
      margin-right: 10px; /* Add spacing between buttons */
      margin-bottom: 5px; /* Ensure buttons wrap nicely */
    }
    button:hover:not(:disabled) {
      border: 1px solid white; color: white; background: #222; /* Darker hover */
    }
    button:disabled {
      background: #444; color: #888; border-color: #444; cursor: not-allowed;
    }
    .progress-container {
      height: 150px; /* Slightly shorter */
      position: relative; border: 1px solid #333; border-radius: 8px;
      margin-bottom: 10px; opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards; animation-delay: 0.6s;
      overflow: hidden; background-color: #1a1a1a;
    }
    .loss-graph, .network-graph {
      position: absolute; top: 0; left: 0; width: 100%; height: 100%;
    }

    /* --- Network Viz Container Scrolling --- */
    #network-viz-container {
        overflow-x: auto; /* Allow horizontal scroll */
        overflow-y: hidden; /* Hide vertical scroll */
        /* Optional: Style scrollbar */
        scrollbar-width: thin; /* Firefox */
        scrollbar-color: #555 #222; /* Firefox: thumb track */
    }
    #network-viz-container::-webkit-scrollbar {
        height: 8px;
    }
    #network-viz-container::-webkit-scrollbar-track {
        background: #222;
        border-radius: 4px;
    }
    #network-viz-container::-webkit-scrollbar-thumb {
        background-color: #555;
        border-radius: 4px;
        border: 2px solid #222;
    }
    #network-viz-container::-webkit-scrollbar-thumb:hover {
        background-color: #777;
    }
    /* --- END Network Viz Scrolling --- */

    .loss-graph, .network-graph {
        display: block; /* Prevents extra space below canvas */
    }
    
    .flex-container {
      display: flex; flex-wrap: wrap; /* Allow wrapping */
      gap: 20px; opacity: 0; transform: translateY(10px);
      animation: fadeInUp 0.5s ease-out forwards; animation-delay: 0.7s;
    }
    .prediction-section, .visualization-container {
       flex: 1 1 300px; /* Flex basis */
       background: #111; border: 1px solid #333; border-radius: 10px;
       padding: 20px; box-sizing: border-box;
       margin-bottom: 0; /* Remove margin if it's inside another widget */
    }
    /* Separate widget for Model Management Buttons */
    .model-management-widget .button-group {
        display: flex; flex-wrap: wrap; gap: 10px;
        /* animations handled by parent */
        opacity: 1; transform: none; animation: none;
    }

    .epoch-progress {
      height: 6px; background: #333; /* Darker background */
      border-radius: 8px; overflow: hidden; margin-top: 8px;
    }
    .epoch-bar { height: 100%; width: 0; background: #eee; transition: width 0.3s ease; }
    #stats { margin-top: 10px; font-size: 0.9em; min-height: 2.5em; color: #ccc; }
    #stats strong { color: #76ff03; } /* Brighter green */
    #stats .error { color: #ff5252; } /* Brighter red */
    #hiddenLayersConfig .input-group { /* Reduce animation delay within layer config */
        animation-delay: 0.1s;
    }
    #hiddenLayersConfig .layer-options-container {
        display: contents; /* Allow options to flow in parent grid */
    }
    #hiddenLayersConfig .settings-grid {
       border-top: 1px solid #444; padding-top: 15px; margin-top: 15px;
    }
    .layer-note { /* Style for notes within layer config */
      font-size: 0.85em; color: #888; margin: 5px 0 0 0;
      grid-column: 1 / -1; /* Span full width */
      line-height: 1.3;
    }

    @keyframes fadeInUp { to { opacity: 1; transform: translateY(0); } }
    @media (max-width: 900px) { /* Adjust breakpoint */
      .grid { grid-template-columns: 1fr; }
    }
     @media (max-width: 480px) { /* Smaller screens */
        body { padding: 3% 3%; }
        .widget { padding: 15px; }
        input, select, textarea, button { font-size: 0.9em; padding: 6px 10px; }
     }
    </style>
</head>
<body>
  <h3>Oblix</h3>
  <p>a neural playground for anyone...</p>
  <p>Load dummy data:

  <!-- Data Generation Settings Panel -->
  <div class="widget" style="margin-bottom: 20px;">
    <div class="widget-title">Data Generation</div>
    <div class="settings-grid">
      <div class="input-group">
        <label for="numTrainSamples" title="Number of training samples to generate">Training Samples:</label>
        <input type="number" id="numTrainSamples" value="100" min="1" step="1">
      </div>
      <div class="input-group">
        <label for="numTestSamples" title="Number of test/validation samples to generate">Test Samples:</label>
        <input type="number" id="numTestSamples" value="25" min="1" step="1">
      </div>
      <div class="input-group">
        <label for="numInputDims" title="Number of input features per sample">Input Dimensions:</label>
        <input type="number" id="numInputDims" value="3" min="1" step="1">
      </div>
      <div class="input-group">
        <label for="numOutputDims" title="Number of output values per sample">Output Dimensions:</label>
        <input type="number" id="numOutputDims" value="1" min="1" step="1">
      </div>
      <div class="input-group">
        <label for="noiseLevel" title="Amount of random noise to add (0-1)">Noise Level:</label>
        <input type="number" id="noiseLevel" value="0.05" min="0" max="1" step="0.01">
      </div>
    </div>
    <div class="button-group" style="margin-top: 10px;">
      <button id="generateDataBtn">Generate Data</button>
    </div>
  </div>

  <div class="grid">
    <!-- Group 1: Data & Training Config -->
    <div class="widget">
      <div class="widget-title">Data & Model Configuration</div>
      <div class="input-group">
        <label for="trainingData">Training Set (CSV, last column=output):</label>
        <textarea id="trainingData" rows="4" placeholder="0.1, 0.9, 0.1
0.9, 0.1, 0.9
0.2, 0.8, 0.2"></textarea>
      </div>
      <div class="input-group">
        <label for="testData">Validation Set (Optional):</label>
        <textarea id="testData" rows="3" placeholder="0.5, 0.5, 0.5"></textarea>
      </div>

      <div class="widget-title" style="margin-top: 20px;">Training Parameters</div>
      <div class="settings-grid">
        <div class="input-group">
          <label for="epochs" title="Number of full passes through the training dataset.">Epochs:</label>
          <input type="number" id="epochs" value="50" min="1">
        </div>
        <div class="input-group">
            <label for="lossFunction" title="How the model's error is calculated. MSE for regression, Cross-Entropy for classification.">Loss Function:</label>
            <select id="lossFunction">
                <option value="mse" selected>MSE</option>
                <option value="crossentropy">Cross-Entropy</option>
            </select>
        </div>
         <div class="input-group">
          <label for="optimizer" title="Algorithm used to update model weights based on error. Adam is often a good default.">Optimizer:</label>
          <select id="optimizer">
              <option value="sgd">SGD</option>
              <option value="adam" selected>Adam</option>
              <option value="rmsprop">RMSprop</option>
              <option value="adamw">AdamW</option>
          </select>
         </div>
        <!-- {{ Add Weight Init Dropdown }} -->
        <div class="config-item">
            <label for="weightInit" title="Method used to initialize dense layer weights. Glorot is often good for tanh/sigmoid, He for ReLU.">Weight Init:</label>
            <select id="weightInit">
                <option value="glorot" selected>Glorot (Xavier)</option>
                <option value="he">He</option>
                <!-- Add more options here later if needed -->
            </select>
        </div>
        <!-- {{ End Weight Init Dropdown }} -->
        <div class="input-group">
          <label for="learningRate" title="How much the model weights are adjusted each update. Too high can diverge, too low is slow.">Learning Rate:</label>
          <input type="number" id="learningRate" value="0.01" step="0.001" min="0">
        </div>
        <div class="input-group">
            <label for="lrScheduler" title="How the learning rate changes over epochs.">LR Schedule:</label>
            <select id="lrScheduler">
                <option value="none" selected>None (Constant)</option>
                <option value="step">Step Decay</option>
                <option value="exponential">Exponential Decay</option>
            </select>
        </div>
        <div id="lrStepParams" class="input-group settings-grid" style="display: none; grid-column: 1 / -1; border-top: 1px solid #444; margin-top: 5px; padding-top: 5px;">
             <div class="input-group">
                 <label for="lrStepDecayFactor" title="Multiply LR by this factor at each step. (e.g., 0.1)">Decay Factor:</label>
                 <input type="number" id="lrStepDecayFactor" value="0.1" step="0.01" min="0" max="1">
             </div>
             <div class="input-group">
                 <label for="lrStepDecaySize" title="Decrease LR every N epochs. (e.g., 10)">Step Size (Epochs):</label>
                 <input type="number" id="lrStepDecaySize" value="10" step="1" min="1">
             </div>
        </div>
         <div id="lrExpParams" class="input-group settings-grid" style="display: none; grid-column: 1 / -1; border-top: 1px solid #444; margin-top: 5px; padding-top: 5px;">
             <div class="input-group">
                 <label for="lrExpDecayRate" title="Multiply LR by this rate each epoch. (e.g., 0.95)">Decay Rate:</label>
                 <input type="number" id="lrExpDecayRate" value="0.95" step="0.001" min="0" max="1">
             </div>
        </div>
        <div class="input-group" id="decayRateGroup" style="display: none;">
           <label for="decayRate" title="Decay factor for RMSprop optimizer's moving average.">Decay Rate (ρ):</label>
           <input type="number" id="decayRate" value="0.9" step="0.01" min="0" max="1">
        </div>
        <div class="input-group">
          <label for="batchSize" title="Number of training samples processed before updating weights.">Batch Size:</label>
          <input type="number" id="batchSize" value="8" min="1">
        </div>
        <div class="input-group">
            <label for="l2Lambda" title="Strength of L2 regularization (weight decay). Helps prevent overfitting (0 to disable).">L2 Lambda:</label>
            <input type="number" id="l2Lambda" value="0" step="0.0001" min="0">
        </div>
        <div class="input-group">
            <label for="gradientClipValue" title="Max absolute value for gradients before update (0 to disable). Helps prevent exploding gradients.">Grad Clip Val:</label>
            <input type="number" id="gradientClipValue" value="0" step="0.1" min="0">
        </div>
        <div class="input-group">
            <label title="Adds information about the position of inputs, useful for sequence data.">
                <input type="checkbox" id="usePositionalEncoding"><span>Use Positional Encoding</span>
            </label>
        </div>
      </div>

      <div class="widget-title" style="margin-top: 20px;">Layer Architecture</div>

      <!-- {{ Add Architecture Template Dropdown }} -->
      <div class="input-group">
          <label for="architectureTemplateSelect">Architecture Template:</label>
          <select id="architectureTemplateSelect">
              <option value="custom" selected>Custom</option>
              <option value="mlp">Simple MLP</option>
              <option value="autoencoder">Basic Autoencoder</option>
              <option value="transformerEncoder">Transformer Encoder</option>
              <option value="residualAttention">Residual Attention</option>
              <!-- Add more templates here in the future -->
          </select>
      </div>
      <!-- {{ End Dropdown }} -->

       <div class="input-group">
         <label for="numHiddenLayers" title="Number of layers between input and output. More layers allow for more complex patterns but increase training time/risk of overfitting.">Number of Hidden Layers:</label>
         <input type="number" id="numHiddenLayers" value="2" min="0">
       </div>
      <!-- Dynamic Layer Configuration UI -->
      <div id="hiddenLayersConfig"></div>

    </div> <!-- End Widget 1 -->

        <!-- Group 2: Training Control, Progress & Visualization -->
        <div class="widget">
            <div class="widget model-management-widget">
                <div class="widget-title">Model Management</div>
                <div class="button-group">
                    <button id="trainButton">Train Model</button>
                    <button id="saveButton">Save Model</button>
                    <button id="loadButton">Load Model</button>
                    <button id="unloadButton">Unload Model</button>
                </div>
           </div>
    
            <div class="widget-title">Training Progress</div>
            <div id="progress">
                <div class="progress-container">
                    <canvas id="lossGraph" class="loss-graph"></canvas>
                </div>
                <p style="text-align: center;">Train Loss (White), Validation Loss (Blue)</p> <!-- Updated Color Name -->
                <div class="epoch-progress">
                    <div id="epochBar" class="epoch-bar"></div>
                </div>
                <div id="stats">Status: Ready</div>
            </div> <!-- End #progress -->
    
            <!-- Network Visualization Card (Moved out of flex-container, placed first) -->
            <div class="visualization-container widget" style="margin-top: 20px;">
                <div class="widget-title">Network Visualization</div>
                <div id="network-viz-container" class="progress-container">
                    <canvas id="networkGraph" class="network-graph"></canvas>
                </div>
                <p style="text-align: center;">Structure & Last Activations</p>
            </div>
    
            <!-- Manual Prediction Card (Moved out of flex-container, placed second) -->
            <div class="prediction-section widget" style="margin-top: 20px;">
                <div class="widget-title">Manual Prediction</div>
                <p>Predict output for new input</p>
                <div class="input-group">
                    <label for="predictionInput">Input (CSV):</label>
                    <input type="text" id="predictionInput" placeholder="0.4, 0.2, 0.6">
                </div>
                <button id="predictButton">Predict</button>
                <div id="predictionResult" style="margin-top: 10px; font-weight: bold;">Result: -</div>
            </div>
    
            <!-- Removed the flex-container div that wrapped prediction and visualization -->
    
        </div> <!-- End Widget 2 -->
      </div> <!-- End Grid -->

  <script>
    // --- Namespace for Activation Functions ---
    const oblixActivations = {
        apply: function(x, activation) {
        const alpha = 0.01; const softplus = (v) => Math.log(1 + Math.exp(v));
        switch (activation) {
          case 'tanh': return Math.tanh(x); case 'sigmoid': return 1 / (1 + Math.exp(-x)); case 'relu': return Math.max(0, x); case 'leakyrelu': return x > 0 ? x : alpha * x; case 'gelu': return 0.5 * x * (1 + Math.tanh(Math.sqrt(2 / Math.PI) * (x + 0.044715 * x**3))); case 'selu': const sa = 1.67326, ss=1.0507; return x > 0 ? ss * x : ss * sa * (Math.exp(x) - 1);
          case 'swish': return x / (1 + Math.exp(-x)); case 'mish': return x * Math.tanh(softplus(x));
          case 'softmax': case 'none': default: return x;
        }
        },
        derivative: function(x, activation) {
          const alpha = 0.01; let val; const sigmoid = (v) => 1 / (1 + Math.exp(-v)); const softplus = (v) => Math.log(1 + Math.exp(v)); const dtanh_dx = (v) => 1 - Math.tanh(v)**2;
          switch (activation) {
              case 'tanh': val = Math.tanh(x); return 1 - val * val; case 'sigmoid': val = sigmoid(x); return val * (1 - val); case 'relu': return x > 0 ? 1 : 0; case 'leakyrelu': return x > 0 ? 1 : alpha; case 'gelu': const k=Math.sqrt(2 / Math.PI), inner=k*(x+0.044715*x**3), tanh_inner=Math.tanh(inner), d_inner_dx=k*(1+0.134145*x**2), sech_sq_inner=1-tanh_inner**2; return 0.5*(1+tanh_inner)+0.5*x*sech_sq_inner*d_inner_dx; case 'selu': const sa=1.67326, ss=1.0507; return x > 0 ? ss : ss * sa * Math.exp(x);
              case 'swish': const sig_x = sigmoid(x); return sig_x + x * sig_x * (1 - sig_x);
              case 'mish': const sp_x = softplus(x); const tanh_sp_x = Math.tanh(sp_x); const dsp_dx = sigmoid(x); const dtanh_dsp = dtanh_dx(sp_x); return tanh_sp_x + x * dtanh_dsp * dsp_dx;
              case 'softmax': case 'none': default: return 1;
          }
      }
    };
    // --- End Namespace ---

    // --- Namespace for Layer Operations ---
    const oblixLayerOps = {
        // {{ Phase 2: Update attentionForward to use Float32Array (assuming input is Float32Array) }}
        attentionForward: function(context, input, numHeads = 2) {
            // Input validation
            if (!(input instanceof Float32Array)) { console.warn("Phase 2 AttnFwd: Input not Float32Array.", input); input = new Float32Array(input); } // Coerce if needed, with warning
            const inputDim = input.length;
            if (inputDim === 0) return new Float32Array(0);
            if (numHeads <= 0 || !Number.isInteger(numHeads) || inputDim % numHeads !== 0) { console.error(`Attn Error: Dim ${inputDim} not divisible by ${numHeads}`); return input; } // Return original (potentially coerced) input on error

            const headSize = inputDim / numHeads;
            // Create Float32Array for output
            const output = new Float32Array(inputDim).fill(0);
            const allAttentionWeights = []; // Keep standard array for potentially jagged weights structure

             if (context.debug) console.log(`Phase 2 AttnFwd: Input type=${input.constructor.name}, len=${input.length}, heads=${numHeads}, headSize=${headSize}`);

            for (let h = 0; h < numHeads; h++) {
                const start = h * headSize;
                const end = start + headSize;
                // Slice Float32Array - creates a *copy*
                const q_head = input.slice(start, end);
                const k_head = q_head; // Self-attention
                const v_head = q_head; // Self-attention

                // Calculate scores (using standard arrays temporarily for simplicity, could optimize later)
                const scores = Array.from({ length: headSize }, (_, i) =>
                    Array.from({ length: headSize }, (_, j) => q_head[i] * k_head[j])
                );

                const scale = Math.sqrt(headSize) || 1;
                const scaled = scores.map(r => r.map(s => s / scale));

                // Calculate attention weights (softmax per row)
                const attn = scaled.map(r => {
                    if (r.length === 0) return [];
                    const max = Math.max(...r, -Infinity);
                    const exps = r.map(s => Math.exp(s - max));
                    const sum = exps.reduce((a, b) => a + b, 1e-9); // Add epsilon for stability
                    return exps.map(e => e / sum);
                });
                allAttentionWeights.push(attn); // Store weights for potential backward pass/analysis

                // Apply attention weights to value heads
                for (let i = 0; i < headSize; i++) {
                    let weighted_sum = 0;
                    for (let j = 0; j < headSize; j++) {
                        weighted_sum += attn[i][j] * v_head[j];
                    }
                    output[start + i] = weighted_sum;
                }
            }

            if (context.debug) console.log(`Phase 2 AttnFwd: Output type=${output.constructor.name}, len=${output.length}, first val=${output[0]?.toFixed(4)}`);

            // Cache intermediates (input is Float32Array, weights standard array)
            if (context.forwardCache) context.forwardCache.attentionIntermediates[context.forwardCache.activations.length - 1] = { input, numHeads, headSize, attentionWeights: allAttentionWeights };
            return output; // Return Float32Array
        },
        // {{ End Phase 2 Update }}

        // {{ Phase 3: Update attentionBackward (Partial Float32Array Integration - Note: complex, needs validation) }}
        attentionBackward: function(context, dOutput, cache) {
            // Input Validation
            if (!(dOutput instanceof Float32Array)) { console.warn("Phase 3 AttnBkwd: dOutput not Float32Array.", dOutput); dOutput = new Float32Array(dOutput); }
            if (!cache || !(cache.input instanceof Float32Array) || !Array.isArray(cache.attentionWeights)) { const N = dOutput?.length || 0; return { dInput: new Float32Array(N).fill(0) }; }

            const { input, numHeads, headSize, attentionWeights } = cache;
            const inputDim = input.length;
            if (dOutput.length !== inputDim) { console.error(`Attn Bkwd Err: dOutput size ${dOutput.length} !== input size ${inputDim}`); return { dInput: new Float32Array(inputDim).fill(0) }; }

            const dInput = new Float32Array(inputDim).fill(0); // Initialize output gradient array
            const scale = Math.sqrt(headSize) || 1;

            if (context.debug) console.log(`Phase 3 AttnBkwd: Input type=${input.constructor.name}, len=${inputDim}, dOutput type=${dOutput.constructor.name}, heads=${numHeads}, headSize=${headSize}`);


            for (let h = 0; h < numHeads; h++) {
                const start = h * headSize;
                const end = start + headSize;

                // Slice creates copies
                const q_h = input.slice(start, end); // Float32Array
                const v_h = q_h; // Float32Array (self-attention)
                const k_h = q_h; // Float32Array (self-attention)
                const dO_h = dOutput.slice(start, end); // Float32Array

                const alpha_h = attentionWeights[h]; // Standard nested array
                if (!alpha_h || alpha_h.length !== headSize || alpha_h[0]?.length !== headSize) { console.error(`Attn Bkwd Err: Head ${h} weights invalid.`); continue; }

                // Intermediate gradients (using standard arrays for now due to complexity)
                const dQ_h = Array(headSize).fill(0);
                const dK_h = Array(headSize).fill(0);
                const dV_h = Array(headSize).fill(0);
                const dScores_h = Array.from({ length: headSize }, () => Array(headSize).fill(0));
                const dAlpha_h = Array.from({ length: headSize }, () => Array(headSize).fill(0));

                // Backprop through weighted sum (dAlpha, dV)
                for (let j = 0; j < headSize; j++) { // Index for V
                    for (let i = 0; i < headSize; i++) { // Index for dO
                         // Ensure numeric types before calculation
                        const dOut_i = dO_h[i]; // number from Float32Array
                        const val_j = v_h[j]; // number from Float32Array
                        const weight_ij = alpha_h[i]?.[j]; // number from standard array

                        if (typeof dOut_i !== 'number' || !isFinite(dOut_i) ||
                            typeof val_j !== 'number' || !isFinite(val_j) ||
                            typeof weight_ij !== 'number' || !isFinite(weight_ij)) {
                             // console.warn(`Attn Bkwd H${h}: Skipping non-finite value at i=${i}, j=${j}`); // Can be very noisy
                             continue;
                         }

                        dV_h[j] += weight_ij * dOut_i;
                        dAlpha_h[i][j] = dOut_i * val_j;
                    }
                }

                // Backprop through Softmax (dScores)
                for (let i = 0; i < headSize; i++) { // Row index
                    let row_sum = 0;
                    for (let k = 0; k < headSize; k++) { // Col index within the row for sum
                        const dAlpha_ik = dAlpha_h[i]?.[k];
                        const alpha_ik = alpha_h[i]?.[k];
                         if (typeof dAlpha_ik === 'number' && typeof alpha_ik === 'number' && isFinite(dAlpha_ik) && isFinite(alpha_ik)) {
                             row_sum += dAlpha_ik * alpha_ik;
                         }
                    }

                    for (let j = 0; j < headSize; j++) { // Col index for specific score grad
                        const alpha_ij = alpha_h[i]?.[j];
                        const dAlpha_ij = dAlpha_h[i]?.[j];
                         if (typeof alpha_ij === 'number' && typeof dAlpha_ij === 'number' && isFinite(alpha_ij) && isFinite(dAlpha_ij)) {
                            const dS_ij = alpha_ij * (dAlpha_ij - row_sum);
                            dScores_h[i][j] = dS_ij / scale; // Backprop through scaling
                         }
                    }
                }

                // Backprop through score calculation (dQ, dK)
                for (let i = 0; i < headSize; i++) { // q index
                    for (let j = 0; j < headSize; j++) { // k index
                        const dS_ij = dScores_h[i]?.[j];
                        const k_val_j = k_h[j]; // number from Float32Array
                        const q_val_i = q_h[i]; // number from Float32Array

                         if (typeof dS_ij === 'number' && isFinite(dS_ij) &&
                             typeof k_val_j === 'number' && isFinite(k_val_j) &&
                             typeof q_val_i === 'number' && isFinite(q_val_i)) {
                            dQ_h[i] += dS_ij * k_val_j;
                            dK_h[j] += dS_ij * q_val_i;
                         }
                    }
                }

                // Combine gradients for input (Q, K, V grads - they are the same input in self-attn)
                for (let i = 0; i < headSize; i++) {
                    // Ensure finite numbers before summing
                    const dQi = typeof dQ_h[i] === 'number' && isFinite(dQ_h[i]) ? dQ_h[i] : 0;
                    const dKi = typeof dK_h[i] === 'number' && isFinite(dK_h[i]) ? dK_h[i] : 0;
                    const dVi = typeof dV_h[i] === 'number' && isFinite(dV_h[i]) ? dV_h[i] : 0;
                    dInput[start + i] = dQi + dKi + dVi;
                }
            }
            if (context.debug) console.log(`Phase 3 AttnBkwd: Output dInput type=${dInput.constructor.name}, len=${dInput.length}, first val=${dInput[0]?.toFixed(4)}`);
            return { dInput }; // Return object with Float32Array dInput
        },
        // {{ End Phase 3 Update }}

        // {{ Phase 2: Update layerNormForward to use Float32Array }}
        layerNormForward: function(context, input, gamma, beta) {
             // Input validation
            if (!(input instanceof Float32Array)) { console.warn("Phase 2 LN_Fwd: Input not Float32Array.", input); input = new Float32Array(input); }
            if (!(gamma instanceof Float32Array)) { console.warn("Phase 2 LN_Fwd: Gamma not Float32Array.", gamma); gamma = new Float32Array(gamma); }
            if (!(beta instanceof Float32Array)) { console.warn("Phase 2 LN_Fwd: Beta not Float32Array.", beta); beta = new Float32Array(beta); }

            const epsilon = context.epsilon;
            const N = input.length;
             if (N === 0) return { output: new Float32Array(0), mean: 0, variance: 0, stddev: epsilon, normalizedInput: new Float32Array(0) };

            // Calculate mean
            let mean = 0;
            for(let i = 0; i < N; i++) mean += input[i];
            mean /= N;

            // Calculate variance
            let variance = 0;
            for(let i = 0; i < N; i++) variance += Math.pow(input[i] - mean, 2);
            variance /= N;

            const stddev = Math.sqrt(variance + epsilon);
            const invStddev = 1 / stddev;

            // Normalize and scale/shift - create Float32Arrays for output and normalized cache
            const normalizedInput = new Float32Array(N);
            const output = new Float32Array(N);

             if (gamma.length !== N || beta.length !== N) console.error(`LN size mismatch: Input ${N}, Gamma ${gamma.length}, Beta ${beta.length}`);

             if (context.debug) console.log(`Phase 2 LN_Fwd: Input type=${input.constructor.name}, len=${N}, Mean=${mean.toFixed(4)}, Var=${variance.toFixed(4)}, StdDev=${stddev.toFixed(4)}`);

            for (let i = 0; i < N; i++) {
                normalizedInput[i] = (input[i] - mean) * invStddev;
                // Handle potential length mismatch gracefully by checking index validity
                const g = gamma[i] ?? 1.0;
                const b = beta[i] ?? 0.0;
                output[i] = g * normalizedInput[i] + b;
            }

             if (context.debug) console.log(`Phase 2 LN_Fwd: Output type=${output.constructor.name}, len=${output.length}, first val=${output[0]?.toFixed(4)}`);


            // Cache intermediates (all should be Float32Array or numbers)
            const cacheData = { output, mean, variance, stddev, normalizedInput, input, gamma };
            if (context.forwardCache) context.forwardCache.layerNormIntermediates[context.forwardCache.activations.length - 1] = cacheData;
            return cacheData; // Return the full cache data including the Float32Array output
        },
        // {{ End Phase 2 Update }}

        // {{ Phase 3: Update layerNormBackward for Float32Array }}
        layerNormBackward: function(context, dOutput, cache) {
            // Input Validation
            if (!(dOutput instanceof Float32Array)) { console.warn("Phase 3 LNBkwd: dOutput not Float32Array.", dOutput); dOutput = new Float32Array(dOutput); }
            if (!cache || !(cache.input instanceof Float32Array) || !(cache.normalizedInput instanceof Float32Array) || !(cache.gamma instanceof Float32Array)) { const N = dOutput?.length || cache?.input?.length || 0; return { dInput: new Float32Array(N).fill(0), dGamma: new Float32Array(N).fill(0), dBeta: new Float32Array(N).fill(0) }; }

            const { input, normalizedInput, mean, variance, stddev, gamma } = cache;
            const N = input.length;

            if (N === 0) return { dInput: new Float32Array(0), dGamma: new Float32Array(0), dBeta: new Float32Array(0) };
            if (dOutput.length !== N || normalizedInput.length !== N || gamma.length !== N) { console.error(`LN Bkwd Err: Size mismatch N=${N}, dOut=${dOutput.length}, normIn=${normalizedInput.length}, gamma=${gamma.length}`); return { dInput: new Float32Array(N).fill(0), dGamma: new Float32Array(N).fill(0), dBeta: new Float32Array(N).fill(0) }; }

            const epsilon = context.epsilon;
            const invStddev = 1 / stddev;

            // Calculate gradients w.r.t. gamma, beta, and normalized output
            const dGamma = new Float32Array(N);
            const dBeta = new Float32Array(dOutput); // Direct copy
            const dNorm = new Float32Array(N);

            if (context.debug) console.log(`Phase 3 LNBkwd: Input type=${input.constructor.name}, len=${N}, dOutput type=${dOutput.constructor.name}`);


            for(let i=0; i<N; i++) {
                dGamma[i] = dOutput[i] * normalizedInput[i];
                dNorm[i] = dOutput[i] * (gamma[i] ?? 1); // Handle potential gamma mismatch during loading? Default to 1.
            }

            // Calculate gradient w.r.t. variance
            let dVariance = 0;
            for(let i=0; i<N; i++) {
                 // Check for non-finite numbers
                 if (isFinite(dNorm[i]) && isFinite(input[i]) && isFinite(mean)) {
                      dVariance += dNorm[i] * (input[i] - mean);
                 }
            }
            dVariance *= (-0.5) * Math.pow(variance + epsilon, -1.5); // Apply the rest of the formula

            // Calculate gradient w.r.t. mean
            let dMean1 = 0;
            for(let i=0; i<N; i++) {
                 if (isFinite(dNorm[i])) { // Check non-finite
                     dMean1 -= dNorm[i] * invStddev;
                 }
            }

            let dMean2Term = 0;
            for(let i=0; i<N; i++) {
                 if (isFinite(input[i]) && isFinite(mean)) { // Check non-finite
                     dMean2Term += -2 * (input[i] - mean);
                 }
            }
            const dMean2 = dVariance * dMean2Term / N;
            const dMean = dMean1 + dMean2;

            // Calculate gradient w.r.t. input
            const dInput = new Float32Array(N);
            for(let i=0; i<N; i++) {
                 const term1 = isFinite(dNorm[i]) ? dNorm[i] * invStddev : 0; // Check non-finite
                 const term2 = (isFinite(dVariance) && isFinite(input[i]) && isFinite(mean)) ? dVariance * 2 * (input[i] - mean) / N : 0; // Check non-finite
                 const term3 = isFinite(dMean) ? dMean / N : 0; // Check non-finite
                dInput[i] = term1 + term2 + term3;
            }

            if (context.debug) console.log(`Phase 3 LNBkwd: Output dInput type=${dInput.constructor.name}, len=${dInput.length}, first val=${dInput[0]?.toFixed(4)}`);


            return { dInput, dGamma, dBeta }; // Return object with Float32Arrays
        },
        // {{ End Phase 3 Update }}

        // {{ Phase 2: Update dropoutForward to use Float32Array }}
        dropoutForward: function(context, input, rate) {
            // Input validation
            if (!(input instanceof Float32Array)) { console.warn("Phase 2 DropFwd: Input not Float32Array.", input); input = new Float32Array(input); }

            const idx = (context.forwardCache?.activations?.length || 1) - 1;
            if (!context.isTraining || rate === 0) {
                context.masks[idx] = null; // No mask needed if not training or rate is 0
                 if (context.debug) console.log(`Phase 2 DropFwd: Not training or rate=0. Output type=${input.constructor.name}, len=${input.length}`);
                return input; // Return input directly (it's already Float32Array or coerced)
            }
            if (rate < 0 || rate >= 1) {
                console.warn(`Dropout rate ${rate} invalid`);
                context.masks[idx] = null;
                return input; // Return input directly
            }

            const N = input.length;
            const scale = 1 / (1 - rate);
            // Create mask as standard array or Float32Array (standard array might be fine)
            const mask = new Float32Array(N); // Using Float32Array for mask too
            const output = new Float32Array(N);

             if (context.debug) console.log(`Phase 2 DropFwd: Input type=${input.constructor.name}, len=${N}, Rate=${rate}, Scale=${scale.toFixed(4)}`);

            for(let i=0; i<N; i++) {
                if (Math.random() > rate) {
                    mask[i] = scale;
                    output[i] = input[i] * scale;
                } else {
                    mask[i] = 0;
                    output[i] = 0;
                }
            }
            context.masks[idx] = mask; // Store Float32Array mask

            if (context.debug) console.log(`Phase 2 DropFwd: Output type=${output.constructor.name}, len=${output.length}, first val=${output[0]?.toFixed(4)}`);

            return output; // Return Float32Array
        },
         // {{ End Phase 2 Update }}

        // {{ Phase 3: Update dropoutBackward for Float32Array }}
        dropoutBackward: function(context, dOutput, layerIndex) {
            // Input validation
            if (!(dOutput instanceof Float32Array)) { console.warn("Phase 3 DropBkwd: dOutput not Float32Array.", dOutput); dOutput = new Float32Array(dOutput); }

            const mask = context.masks[layerIndex]; // Should be Float32Array or null

            if (!mask) {
                if (context.debug) console.log(`Phase 3 DropBkwd L${layerIndex}: No mask found (not training or rate=0). Passing through dOutput.`);
                return dOutput; // Pass through gradient if no mask (inference or rate=0)
            }

            if (!(mask instanceof Float32Array)) { // Safety check
                 console.error(`Phase 3 DropBkwd L${layerIndex}: Mask is not a Float32Array!`, mask);
                 return dOutput; // Return original gradient on error
            }

            if (dOutput.length !== mask.length) {
                console.error(`Phase 3 DropBkwd L${layerIndex}: dOutput size ${dOutput.length} !== mask size ${mask.length}`);
                // Return dOutput only if sizes mismatch, as we can't meaningfully apply mask
                return dOutput;
            }

            const dInput = new Float32Array(dOutput.length);
            for(let i=0; i<dInput.length; i++) {
                // Check for non-finite numbers before multiplying
                 const grad = dOutput[i];
                 const maskVal = mask[i];
                 if (isFinite(grad) && isFinite(maskVal)) {
                     dInput[i] = grad * maskVal;
                 } else {
                      dInput[i] = 0; // Assign 0 if non-finite value encountered
                      // console.warn(`Phase 3 DropBkwd L${layerIndex}: Non-finite value encountered at index ${i}. Grad=${grad}, Mask=${maskVal}`); // Optional warning
                 }
            }

            if (context.debug) console.log(`Phase 3 DropBkwd L${layerIndex}: Applied mask. dInput type=${dInput.constructor.name}, len=${dInput.length}, first val=${dInput[0]?.toFixed(4)}`);

            return dInput; // Return Float32Array gradient
        },
        // {{ End Phase 3 Update }}

        // {{ Phase 2: Update softmaxForward to use Float32Array }}
        softmaxForward: function(context, input) {
             // Input validation
            if (!(input instanceof Float32Array)) { console.warn("Phase 2 SoftFwd: Input not Float32Array.", input); input = new Float32Array(input); }

            const N = input.length;
            if (N === 0) return new Float32Array(0);

            // Find max value for numerical stability
            let maxVal = -Infinity;
            for(let i=0; i<N; i++) {
                if(input[i] > maxVal) maxVal = input[i];
            }

            // Calculate exponentials and sum
            const exps = new Float32Array(N);
            let sumExps = 0;
            for(let i=0; i<N; i++) {
                const expVal = Math.exp(input[i] - maxVal);
                exps[i] = expVal;
                sumExps += expVal;
            }

            sumExps += 1e-9; // Add epsilon for stability if sum is zero

            // Normalize to get probabilities
            const output = new Float32Array(N);
            for(let i=0; i<N; i++) {
                output[i] = exps[i] / sumExps;
            }

             if (context.debug) console.log(`Phase 2 SoftFwd: Input type=${input.constructor.name}, len=${N}, Output type=${output.constructor.name}, first val=${output[0]?.toFixed(4)}`);

            // Cache output
            if (context.forwardCache) context.forwardCache.softmaxOutputs[context.forwardCache.activations.length - 1] = output;
            return output; // Return Float32Array
        },
        // {{ End Phase 2 Update }}

        softmaxBackward: function(context, dOutput, layerIndex) {
             // Input validation
             if (!(dOutput instanceof Float32Array)) { console.warn("Phase 3 SoftBkwd: dOutput not Float32Array.", dOutput); dOutput = new Float32Array(dOutput); }
             // NOTE: Correct Softmax backward involves Jacobian, but often simplified when used with CrossEntropyLoss.
             // The dError passed is typically (output - target), which IS the gradient w.r.t. the pre-softmax activations (logits).
             // So, just passing the gradient through is the common practice in that specific scenario.
             if (context.debug) console.log(`Phase 3 SoftBkwd L${layerIndex}: Passing through gradient (assuming CE Loss). dInput type=${dOutput.constructor.name}, len=${dOutput.length}`);
             // console.warn("Softmax backward pass used directly. Assumes CrossEntropy loss for correctness.");
             return dOutput; // Return the input gradient (Float32Array)
        }
    };
    // --- End Layer Namespace ---

    // --- Namespace for Optimizers ---
    const oblixOptimizers = {
        initializeState: function(context, optimizer) {
            const numLayers = context.layers.length;
            if (context.debug) console.log(`Init optimizer state: ${optimizer}, ${numLayers} layers.`);
            context.t = 0;
            context.m_dw = Array(numLayers).fill(null); context.v_dw = Array(numLayers).fill(null);
            context.m_db = Array(numLayers).fill(null); context.v_db = Array(numLayers).fill(null);
            context.m_dgamma = Array(numLayers).fill(null); context.v_dgamma = Array(numLayers).fill(null);
            context.m_dbeta = Array(numLayers).fill(null); context.v_dbeta = Array(numLayers).fill(null);
            context.s_dw = Array(numLayers).fill(null); context.s_db = Array(numLayers).fill(null);
            context.s_dgamma = Array(numLayers).fill(null); context.s_dbeta = Array(numLayers).fill(null);

            for (let i = 0; i < numLayers; i++) {
                const cfg = context.layers[i]; if (!cfg) continue;
                const w = context.weights[i]; const reqW = cfg.type === 'dense' && Array.isArray(w) && w.length > 0 && Array.isArray(w[0]);
                const b = context.biases[i]; const reqB = cfg.type === 'dense' && cfg.useBias && Array.isArray(b) && b.length > 0;
                const g = context.gammas[i]; const beta = context.betas[i]; const reqLN = cfg.type === 'layernorm' && Array.isArray(g) && Array.isArray(beta) && g.length === beta.length && g.length > 0;

                if (optimizer === 'adam' || optimizer === 'rmsprop' || optimizer === 'adamw') {
                    if (reqW) { try { const z = () => w.map(r => r.map(() => 0)); if (optimizer === 'adam' || optimizer === 'adamw') { context.m_dw[i] = z(); context.v_dw[i] = z(); } if (optimizer === 'rmsprop') { context.s_dw[i] = z(); } } catch (e) { console.error(`InitOpt L${i} W err: ${e.message}`); context.m_dw[i] = null; context.v_dw[i] = null; context.s_dw[i] = null; } }
                    if (reqB) { try { const z = () => b.map(() => 0); if (optimizer === 'adam' || optimizer === 'adamw') { context.m_db[i] = z(); context.v_db[i] = z(); } if (optimizer === 'rmsprop') { context.s_db[i] = z(); } } catch (e) { console.error(`InitOpt L${i} B err: ${e.message}`); context.m_db[i] = null; context.v_db[i] = null; context.s_db[i] = null; } }
                    if (reqLN) { try { const z = () => g.map(() => 0); if (optimizer === 'adam' || optimizer === 'adamw') { context.m_dgamma[i] = z(); context.v_dgamma[i] = z(); context.m_dbeta[i] = z(); context.v_dbeta[i] = z(); } if (optimizer === 'rmsprop') { context.s_dgamma[i] = z(); context.s_dbeta[i] = z(); } } catch (e) { console.error(`InitOpt L${i} LN err: ${e.message}`); context.m_dgamma[i] = null; context.v_dgamma[i] = null; context.m_dbeta[i] = null; context.v_dbeta[i] = null; context.s_dgamma[i] = null; context.s_dbeta[i] = null; } }
                }
            }
            if (context.debug) console.log(`Optimizer state init finished.`);
        },

        // {{ Phase 3: Update updateParameters for Float32Array }}
        updateParameters: function(context, gradsW, gradsB, gradsGamma, gradsBeta, options) {
            const {
                learningRate, initialLearningRate, optimizer, batchSize,
                l2Lambda, gradientClipValue, decayRate // RMSprop ρ
        } = options;

            const batchMult = batchSize > 0 ? 1.0 / batchSize : 1.0; // Avoid division by zero
            context.t++; // Increment global timestep

            if (context.debug) console.log(`Phase 3 Optimizer Update: T=${context.t}, LR=${learningRate.toExponential(3)}, BatchSize=${batchSize}, Opt=${optimizer}, L2=${l2Lambda}`);


            for (let i = 0; i < context.layers.length; i++) {
                const cfg = context.layers[i];
                const isDense = cfg.type === 'dense';
                const isLN = cfg.type === 'layernorm';

                 // Optimizer-specific LR calculation (no change needed here)
                 let stepLR = learningRate;
                let adamCorrectedBaseLR = initialLearningRate * Math.sqrt(1 - context.beta2 ** context.t) / (1 - context.beta1 ** context.t);
                 let adamStepLR = adamCorrectedBaseLR * (learningRate / initialLearningRate);

                 const applyUpdate = (param, grad, mState, vState, sState, paramIdx, layerIdx, paramType) => {
                     if (typeof param !== 'number' || typeof grad !== 'number' || !isFinite(param) || !isFinite(grad)) {
                         // console.warn(`Opt L${layerIdx}-${paramType}[${paramIdx}]: Skipping update due to non-finite param/grad (P:${param}, G:${grad})`);
                          return param; // Return unchanged parameter
                     }

                     const effectiveGrad = grad * batchMult;
                     const clippedGrad = gradientClipValue > 0 ? Math.max(-gradientClipValue, Math.min(gradientClipValue, effectiveGrad)) : effectiveGrad;
                     if (!isFinite(clippedGrad)) {
                         // console.warn(`Opt L${layerIdx}-${paramType}[${paramIdx}]: Skipping update due to non-finite clipped grad (${clippedGrad}) from effGrad (${effectiveGrad})`);
                         return param; // Return unchanged parameter if clipping results in non-finite value
                     }


                     let update = 0;
                     let m = (typeof mState === 'number' && isFinite(mState)) ? mState : 0;
                     let v = (typeof vState === 'number' && isFinite(vState)) ? vState : 0;
                     let s = (typeof sState === 'number' && isFinite(sState)) ? sState : 0;

                    if (optimizer === 'adam' || optimizer === 'adamw') {
                                m = context.beta1 * m + (1 - context.beta1) * clippedGrad;
                                v = context.beta2 * v + (1 - context.beta2) * clippedGrad ** 2;
                                const m_hat = m / (1 - context.beta1 ** context.t);
                                const v_hat = v / (1 - context.beta2 ** context.t);
                         // Ensure v_hat doesn't lead to NaN/Infinity
                         const sqrt_v_hat = Math.sqrt(v_hat);
                         if (isFinite(m_hat) && isFinite(sqrt_v_hat) && (sqrt_v_hat + context.epsilon) !== 0) {
                             update = adamStepLR * m_hat / (sqrt_v_hat + context.epsilon);
                         } else {
                             // console.warn(`Opt L${layerIdx}-${paramType}[${paramIdx}]: Adam update NaN/Inf. m_hat=${m_hat}, v_hat=${v_hat}, sqrt_v_hat=${sqrt_v_hat}`);
                              update = 0; // Prevent NaN update
                         }

                                if (optimizer === 'adam' && l2Lambda > 0) { update += adamStepLR * l2Lambda * param; }
                         // Store updated m, v back into their Float32Arrays (done outside this helper)
                    } else if (optimizer === 'rmsprop') {
                         s = decayRate * s + (1 - decayRate) * clippedGrad ** 2;
                         const sqrt_s = Math.sqrt(s);
                          if (isFinite(clippedGrad) && isFinite(sqrt_s) && (sqrt_s + context.epsilon) !== 0) {
                             update = stepLR * clippedGrad / (sqrt_s + context.epsilon);
                         } else {
                             // console.warn(`Opt L${layerIdx}-${paramType}[${paramIdx}]: RMSprop update NaN/Inf. grad=${clippedGrad}, s=${s}, sqrt_s=${sqrt_s}`);
                              update = 0;
                         }
                                if (l2Lambda > 0) { update += stepLR * l2Lambda * param; }
                          // Store updated s back (done outside this helper)
                    } else { // SGD
                      update = stepLR * clippedGrad;
                                if (l2Lambda > 0) { update += stepLR * l2Lambda * param; }
                            }

                     if (!isFinite(update)) {
                          // console.warn(`Opt L${layerIdx}-${paramType}[${paramIdx}]: Update value non-finite (${update}). Skipping param change.`);
                          return { param, m, v, s }; // Return original param and potentially updated states
                     }

                     param -= update; // Apply main update

                     // Apply AdamW decoupled weight decay after main update
                     if (optimizer === 'adamw' && l2Lambda > 0 && paramType === 'W') { // Only apply decay to Weights
                         param -= adamStepLR * l2Lambda * param;
                     }

                     if (!isFinite(param)) {
                         // console.warn(`Opt L${layerIdx}-${paramType}[${paramIdx}]: Parameter became non-finite after update/decay (${param}). Resetting? (Currently keeping original value)`);
                         // Decide on recovery strategy: maybe reset to 0? Or keep original value? For now, keep original.
                         // return { param: originalParamValue, m, v, s }; // Need original value passed in
                         // Let's return NaN for now to make it obvious
                         return { param: NaN, m, v, s };
                     }


                     return { param, m, v, s }; // Return updated param and states
                 };


                // --- Weight Updates (Dense Layers) ---
                if (isDense && context.weights[i] instanceof Float32Array && gradsW[i] instanceof Float32Array) {
                    const w = context.weights[i]; const gW = gradsW[i]; // Float32Arrays
                    const mW = context.m_dw[i]; const vW = context.v_dw[i]; const sW = context.s_dw[i]; // Float32Arrays or null

                    for (let k = 0; k < w.length; k++) { // Iterate through flat array
                        const mVal = mW ? mW[k] : null;
                        const vVal = vW ? vW[k] : null;
                        const sVal = sW ? sW[k] : null;

                         const result = applyUpdate(w[k], gW[k], mVal, vVal, sVal, k, i, 'W');

                         if (isNaN(result.param)) { // Handle NaN outcome
                              console.warn(`Optimizer L${i}-W[${k}] resulted in NaN. Keeping original value ${w[k]}.`);
                              // Optionally reset optimizer states for this param?
                              // if (mW) mW[k] = 0; if (vW) vW[k] = 0; if (sW) sW[k] = 0;
                         } else {
                              w[k] = result.param;
                              if (mW) mW[k] = result.m;
                              if (vW) vW[k] = result.v;
                              if (sW) sW[k] = result.s;
                         }
                    }
                     if (context.debug && i === 0 && w.length > 0) console.log(`Phase 3 Opt L0 W: First few updated = ${w.slice(0, 3).map(v=>v.toFixed(4))}`);
              }

              // --- Bias Updates (Dense Layers) ---
                 if (isDense && cfg.useBias && context.biases[i] instanceof Float32Array && gradsB[i] instanceof Float32Array) {
                    const b = context.biases[i]; const gB = gradsB[i];
                     const mB = context.m_db[i]; const vB = context.v_db[i]; const sB = context.s_db[i];

                     for (let k = 0; k < b.length; k++) {
                         const mVal = mB ? mB[k] : null;
                         const vVal = vB ? vB[k] : null;
                         const sVal = sB ? sB[k] : null;

                         const result = applyUpdate(b[k], gB[k], mVal, vVal, sVal, k, i, 'B');

                         if (isNaN(result.param)) {
                              console.warn(`Optimizer L${i}-B[${k}] resulted in NaN. Keeping original value ${b[k]}.`);
                         } else {
                              b[k] = result.param;
                              if (mB) mB[k] = result.m;
                              if (vB) vB[k] = result.v;
                              if (sB) sB[k] = result.s;
                         }
                     }
                     if (context.debug && i === 0 && b.length > 0) console.log(`Phase 3 Opt L0 B: First few updated = ${b.slice(0, 3).map(v=>v.toFixed(4))}`);
                 }


              // --- LayerNorm Parameter Updates (Gamma & Beta) ---
                 if (isLN && context.gammas[i] instanceof Float32Array && context.betas[i] instanceof Float32Array && gradsGamma[i] instanceof Float32Array && gradsBeta[i] instanceof Float32Array) {
                     const gamma = context.gammas[i]; const gGamma = gradsGamma[i];
                     const beta = context.betas[i]; const gBeta = gradsBeta[i];
                     const mGamma = context.m_dgamma[i]; const vGamma = context.v_dgamma[i]; const sGamma = context.s_dgamma[i];
                     const mBeta = context.m_dbeta[i]; const vBeta = context.v_dbeta[i]; const sBeta = context.s_dbeta[i];

                     for (let k = 0; k < gamma.length; k++) {
                  // Update Gamma
                         const mGammaVal = mGamma ? mGamma[k] : null;
                         const vGammaVal = vGamma ? vGamma[k] : null;
                         const sGammaVal = sGamma ? sGamma[k] : null;
                         const resultGamma = applyUpdate(gamma[k], gGamma[k], mGammaVal, vGammaVal, sGammaVal, k, i, 'Gamma');

                         if (isNaN(resultGamma.param)) {
                             console.warn(`Optimizer L${i}-Gamma[${k}] resulted in NaN. Keeping original value ${gamma[k]}.`);
                         } else {
                             gamma[k] = resultGamma.param;
                             if (mGamma) mGamma[k] = resultGamma.m;
                             if (vGamma) vGamma[k] = resultGamma.v;
                             if (sGamma) sGamma[k] = resultGamma.s;
                  }

                  // Update Beta
                         const mBetaVal = mBeta ? mBeta[k] : null;
                         const vBetaVal = vBeta ? vBeta[k] : null;
                         const sBetaVal = sBeta ? sBeta[k] : null;
                          const resultBeta = applyUpdate(beta[k], gBeta[k], mBetaVal, vBetaVal, sBetaVal, k, i, 'Beta');

                         if (isNaN(resultBeta.param)) {
                              console.warn(`Optimizer L${i}-Beta[${k}] resulted in NaN. Keeping original value ${beta[k]}.`);
                         } else {
                              beta[k] = resultBeta.param;
                              if (mBeta) mBeta[k] = resultBeta.m;
                              if (vBeta) vBeta[k] = resultBeta.v;
                              if (sBeta) sBeta[k] = resultBeta.s;
                         }
                     }
                       if (context.debug && i < 2 && gamma.length > 0) console.log(`Phase 3 Opt L${i} LN: First Gamma=${gamma[0]?.toFixed(4)}, Beta=${beta[0]?.toFixed(4)}`);

                 }
            } // End layer loop
        },
        // {{ End Phase 3 Update }}
    };
    // --- End Optimizer Namespace ---

    // --- Namespace for Utility Functions ---
    const oblixUtils = {
        // {{ Phase 2: Update positionalEncoding to return Float32Array }}
        positionalEncoding: function(input, maxLen = -1) {
            // Input validation - assume input might be standard array or Float32Array
            const dModel = input.length;
            if (dModel === 0) return new Float32Array(0);

            let inputArray;
            if (input instanceof Float32Array) {
                inputArray = input;
            } else if (Array.isArray(input)) {
                console.warn("Phase 2 PosEnc: Input was standard Array, converting to Float32Array.");
                inputArray = new Float32Array(input);
            } else {
                 console.error("Phase 2 PosEnc: Invalid input type.", input);
                 return new Float32Array(dModel); // Return zero array on error
            }


            if (maxLen < 0) maxLen = dModel; // Use input dimension if maxLen not specified
            const pe = new Float32Array(dModel).fill(0); // Initialize PE array

            for (let i = 0; i < dModel; i++) {
                // Original formula used 'i' for position, let's assume input vector index IS position
                const position = i; // Or adapt if a separate position sequence is needed
                const divTermBase = Math.pow(10000, Math.floor(i / 2) * 2 / dModel);
                if (divTermBase === 0) continue; // Avoid division by zero

                const angle = position / divTermBase;
                pe[i] = (i % 2 === 0) ? Math.sin(angle) : Math.cos(angle);
            }

            // Add positional encoding to input
            const output = new Float32Array(dModel);
            for(let i=0; i<dModel; i++) {
                output[i] = inputArray[i] + pe[i];
            }

             if (input.constructor && output.constructor && typeof console !== 'undefined' && console.log) { // Check for console.log existence
                  if (console.debug) { // Prefer console.debug if available
                      console.debug(`Phase 2 PosEnc: Input type=${input.constructor.name}, Output type=${output.constructor.name}, len=${output.length}`);
                  } else {
                     // Fallback to console.log
                     // console.log(`Phase 2 PosEnc: Input type=${input.constructor.name}, Output type=${output.constructor.name}, len=${output.length}`);
                  }
             }


            return output; // Return Float32Array
        },
        // {{ End Phase 2 Update }}

        /**
         * Calculates accuracy for classification tasks.
         * Assumes predictions are probability distributions (e.g., from Softmax)
         * and targets are either class indices or one-hot encoded vectors.
         * @param {number[][]} predictions - Array of prediction vectors.
         * @param {(number[]|number[][])} targets - Array of target class indices or one-hot vectors.
         * @returns {number} Accuracy value (0.0 to 1.0).
         */
        calculateAccuracy: function(predictions, targets) {
            if (!predictions || !targets || predictions.length === 0 || predictions.length !== targets.length) {
                console.warn("Accuracy calculation: Invalid input arrays.");
                return 0.0;
            }

            let correct = 0;
            for (let i = 0; i < predictions.length; i++) {
                const predVec = predictions[i];
                const targetInfo = targets[i];

                if (!predVec || !targetInfo) continue; // Skip invalid entries

                // Find the index of the highest probability in the prediction
                const predictedIndex = predVec.indexOf(Math.max(...predVec));

                let targetIndex = -1;
                // Check if target is a class index (single number)
                if (typeof targetInfo === 'number' && Number.isInteger(targetInfo)) {
                    targetIndex = targetInfo;
                }
                // Check if target is a one-hot vector
                else if (Array.isArray(targetInfo) && targetInfo.length === predVec.length) {
                    targetIndex = targetInfo.indexOf(1);
                    // Handle cases where it's not strictly one-hot (e.g., [0.9, 0.1]) - find max
                    if (targetIndex === -1) {
                       targetIndex = targetInfo.indexOf(Math.max(...targetInfo));
                    }
                } else {
                     console.warn(`Accuracy calc: Invalid target format at index ${i}`, targetInfo);
                     continue;
                }


                if (predictedIndex === targetIndex && targetIndex !== -1) {
                    correct++;
                }
            }
            return predictions.length > 0 ? correct / predictions.length : 0.0;
        },

        /**
         * Calculates the R-squared (Coefficient of Determination) for regression tasks.
         * @param {number[]} predictions - Array of predicted values.
         * @param {number[]} targets - Array of true target values.
         * @returns {number} R-squared value. Can be negative for poor models.
         */
        calculateRSquared: function(predictions, targets) {
            if (!predictions || !targets || predictions.length === 0 || predictions.length !== targets.length) {
                console.warn("R-squared calculation: Invalid input arrays.");
                return NaN; // Indicate invalid input
            }
             if (predictions.length < 2) {
                 console.warn("R-squared calculation: Need at least 2 data points.");
                 return NaN; // R-squared is ill-defined for n < 2
             }

             // {{ Logging: Check received arrays and calculated mean }}
             if (typeof console !== 'undefined' && console.log) {
                console.log(`R² Func Check: Received preds[0]=${predictions[0]} (typeof: ${typeof predictions[0]}), targets[0]=${targets[0]} (typeof: ${typeof targets[0]})`);
                const targetMean = targets.reduce((sum, val) => sum + val, 0) / targets.length;
                console.log(`R² Func Check: Calculated targetMean = ${targetMean}`);
             }
             // {{ End Logging }}


            let ssTot = 0; // Total sum of squares
            let ssRes = 0; // Residual sum of squares

            for (let i = 0; i < targets.length; i++) {
                const targetVal = targets[i];
                const predVal = predictions[i];

                // {{ Logging: Check values right before the problematic check - KEEP for now }}
                 if (typeof console !== 'undefined' && console.log) {
                     // Previous detailed check breakdown logging can be removed if the fix works,
                     // but let's keep it for one more run just in case.
                     const isTargetNum = typeof targetVal === 'number';
                     const isPredNum = typeof predVal === 'number';
                     const isTargetFinite = isFinite(targetVal);
                     const isPredFinite = isFinite(predVal);
                     if (i === 0) { // Only log details for the first element
                         console.log(`R² Func Loop i=${i}: Checks -> targetNum=${isTargetNum}, predNum=${isPredNum}, targetFinite=${isTargetFinite}, predFinite=${isPredFinite}`);
                     }
                 }
                 // {{ End Logging }}

                // {{ Edit: Simplify the check to rely only on isFinite }}
                if (!isFinite(targetVal) || !isFinite(predVal)) {
                     console.warn(`R-squared calc: Non-finite number at index ${i} (Pred: ${predVal}, Target: ${targetVal})`);
                     return NaN; // Invalid data point
                }
                // {{ End Edit }}

                // These lines should now only execute if both values are finite numbers
                const targetMean = targets.reduce((sum, val) => sum + val, 0) / targets.length; // Recalculate mean here just to ensure it's defined in this scope if needed later (though it's calculated above) - consider refactoring mean calculation location
                ssTot += (targetVal - targetMean) ** 2;
                ssRes += (targetVal - predVal) ** 2;
            }

            if (ssTot === 0) {
                return (ssRes === 0) ? 1.0 : NaN;
            }
            return 1 - (ssRes / ssTot);
        },

        // Add other utils here later if needed

        // {{ Add gaussianRandom helper using Box-Muller }}
        _gaussian_spare: null, // Cache for Box-Muller
        gaussianRandom: function() {
            if (this._gaussian_spare !== null) {
                const spare = this._gaussian_spare;
                this._gaussian_spare = null;
                return spare;
            }
            let u, v, s;
            do {
                u = Math.random() * 2 - 1; // (-1, 1)
                v = Math.random() * 2 - 1; // (-1, 1)
                s = u * u + v * v;
            } while (s >= 1 || s === 0);
            const mul = Math.sqrt(-2.0 * Math.log(s) / s);
            this._gaussian_spare = v * mul; // Store the second sample
            return u * mul; // Return the first sample
        }
        // {{ End gaussianRandom helper }}
    };
    // --- End Utility Namespace ---


    class oblix {
      constructor(debug = true) {
        this.layers = [];
        // {{ Edit 1: Initialize with empty arrays for Float32Array storage }}
        this.weights = []; // Will store Float32Array[] | null[]
        this.biases = []; // Will store Float32Array[] | null[]
        this.gammas = []; // Will store Float32Array[] | null[]
        this.betas = []; // Will store Float32Array[] | null[]
        this.masks = []; // Stores dropout masks (can remain standard arrays or null)
        // {{ End Edit 1 }}

        this.details = {}; this.debug = debug; this.usePositionalEncoding = false; this.isTraining = false;
        this.beta1 = 0.9; this.beta2 = 0.999; this.epsilon = 1e-8; this.t = 0;

        // {{ Edit 2: Initialize optimizer states as empty arrays }}
        // These will hold Float32Array[] | null[]
        this.m_dw = []; this.v_dw = []; this.m_db = []; this.v_db = [];
        this.m_dgamma = []; this.v_dgamma = []; this.m_dbeta = []; this.v_dbeta = [];
        this.s_dw = []; this.s_db = []; this.s_dgamma = []; this.s_dbeta = [];
        // {{ End Edit 2 }}

        this.decayRate = 0.9;
        this.lastActivations = null; this.forwardCache = null;

        if (this.debug) console.log("oblix instance created. Phase 1 Refactor: Initializing for Float32Array storage.");
      }

      reset() {
        if (this.debug) console.log("Resetting oblix instance...");
        this.layers = [];
        // {{ Edit 3: Reset all parameter/state arrays }}
        this.weights = []; this.biases = []; this.gammas = []; this.betas = []; this.masks = [];
        this.details = {};
        this.isTraining = false;
        this.t = 0;
        this.m_dw = []; this.v_dw = []; this.m_db = []; this.v_db = [];
        this.m_dgamma = []; this.v_dgamma = []; this.m_dbeta = []; this.v_dbeta = [];
        this.s_dw = []; this.s_db = []; this.s_dgamma = []; this.s_dbeta = [];
        // {{ End Edit 3 }}
        this.lastActivations = null; this.forwardCache = null;
        if (this.debug) console.log("Phase 1 Refactor: Oblix reset complete.");
      }

      layer(config) {
        // {{ Update signature to include weightInit }}
        const { type = 'dense', inputSize, outputSize, activation = 'tanh', numHeads = 2, useBias = true, rate = 0.5, weightInit = 'glorot' } = config;
        if (typeof inputSize !== 'number' || inputSize <= 0) throw new Error(`Layer ${this.layers.length}: Invalid inputSize: ${inputSize}.`);
        if (this.layers.length > 0) { const prevLayer = this.layers[this.layers.length - 1]; if (inputSize !== prevLayer.outputSize) throw new Error(`Layer ${this.layers.length} (${type}): Input size ${inputSize} doesn't match previous layer's output size ${prevLayer.outputSize}.`); }
        let actualOutputSize = outputSize;
        switch (type) { case 'dense': if (typeof outputSize !== 'number' || outputSize <= 0) throw new Error(`Dense Layer ${this.layers.length}: Invalid outputSize: ${outputSize}.`); break; case 'layernorm': case 'attention': case 'dropout': case 'softmax': actualOutputSize = inputSize; if (outputSize !== undefined && outputSize !== inputSize) console.warn(`${type} layer ${this.layers.length}: Output size ignored.`); break; default: throw new Error(`Unknown layer type: ${type}`); }
        if (type === 'attention' && inputSize % numHeads !== 0) throw new Error(`Attention layer ${this.layers.length}: Input size ${inputSize} not divisible by numHeads ${numHeads}.`);
        if (type === 'dropout' && (rate < 0 || rate >= 1)) throw new Error(`Dropout layer ${this.layers.length}: Rate ${rate} must be >= 0 and < 1.`);
        // {{ Pass weightInit to layerConfig }}
        const layerConfig = { type, inputSize, outputSize: actualOutputSize, activation, numHeads, useBias, rate, weightInit };
        const layerIndex = this.layers.length; // Get index before pushing config
        this.layers.push(layerConfig);

        // {{ Edit 4: Push null placeholders, then initialize Float32Arrays }}
        this.weights.push(null); this.biases.push(null); this.gammas.push(null); this.betas.push(null); this.masks.push(null);
        this.m_dw.push(null); this.v_dw.push(null); this.m_db.push(null); this.v_db.push(null); this.m_dgamma.push(null); this.v_dgamma.push(null); this.m_dbeta.push(null); this.v_dbeta.push(null); this.s_dw.push(null); this.s_db.push(null); this.s_dgamma.push(null); this.s_dbeta.push(null);

        if (type === 'dense') {
            // {{ Implement Conditional Initialization }}
            const weightCount = actualOutputSize * inputSize;
            const weightsArray = new Float32Array(weightCount);
            let initFunc;
            if (weightInit === 'he') {
                const stdDev = Math.sqrt(2 / inputSize); // fan_in is inputSize
                initFunc = () => oblixUtils.gaussianRandom() * stdDev;
                if (this.debug) console.log(`Phase 1 Refactor: L${layerIndex} Dense Weights init: He (stdDev=${stdDev.toFixed(4)})`);

            } else { // Default to Glorot (Xavier)
                const limit = Math.sqrt(6 / (inputSize + actualOutputSize));
                initFunc = () => (Math.random() * 2 - 1) * limit;
                 if (this.debug) console.log(`Phase 1 Refactor: L${layerIndex} Dense Weights init: Glorot (limit=${limit.toFixed(4)})`);
            }

            for (let i = 0; i < weightCount; i++) {
                weightsArray[i] = initFunc();
            }
            this.weights[layerIndex] = weightsArray; // Use layerIndex
            // {{ Remove old Glorot calculation logging - now handled above }}
            // if (this.debug) console.log(`Phase 1 Refactor: L${layerIndex} Dense Weights init: ${this.weights[layerIndex] instanceof Float32Array}, Length: ${this.weights[layerIndex]?.length}`);
             // {{ End Conditional Initialization }}

            if (useBias) {
                 this.biases[layerIndex] = new Float32Array(actualOutputSize).fill(0.01); // Use layerIndex
                 if (this.debug) console.log(`Phase 1 Refactor: L${layerIndex} Dense Biases init: ${this.biases[layerIndex] instanceof Float32Array}, Length: ${this.biases[layerIndex]?.length}`);
            }
        } else if (type === 'layernorm') {
            this.gammas[layerIndex] = new Float32Array(actualOutputSize).fill(1.0); // Use layerIndex
            this.betas[layerIndex] = new Float32Array(actualOutputSize).fill(0.0); // Use layerIndex
            if (this.debug) console.log(`Phase 1 Refactor: L${layerIndex} LayerNorm Gamma init: ${this.gammas[layerIndex] instanceof Float32Array}, Length: ${this.gammas[layerIndex]?.length}`);
            if (this.debug) console.log(`Phase 1 Refactor: L${layerIndex} LayerNorm Beta init: ${this.betas[layerIndex] instanceof Float32Array}, Length: ${this.betas[layerIndex]?.length}`);
        }
        // {{ End Edit 4 }}
      }

      // --- positionalEncoding REMOVED (Moved to namespace) ---

      initializeOptimizerState(optimizer) {
          // {{ Edit 5: Initialize optimizer states with Float32Arrays }}
          const numLayers = this.layers.length;
          if (this.debug) console.log(`Phase 1 Refactor: Init optimizer state (${optimizer}) for ${numLayers} layers. Creating Float32Arrays.`);
          this.t = 0;
          // Ensure arrays have the correct length first, filled with null
          const ensureLen = (arrName) => {
              if (!this[arrName] || this[arrName].length !== numLayers) {
                  this[arrName] = Array(numLayers).fill(null);
              }
          };
          ['m_dw', 'v_dw', 'm_db', 'v_db', 'm_dgamma', 'v_dgamma', 'm_dbeta', 'v_dbeta', 's_dw', 's_db', 's_dgamma', 's_dbeta'].forEach(ensureLen);

          for (let i = 0; i < numLayers; i++) {
              const cfg = this.layers[i]; if (!cfg) continue;
              const w = this.weights[i]; // Should be Float32Array or null
              const b = this.biases[i]; // Should be Float32Array or null
              const g = this.gammas[i]; // Should be Float32Array or null
              const beta = this.betas[i]; // Should be Float32Array or null

              const needsWState = cfg.type === 'dense' && w instanceof Float32Array;
              const needsBState = cfg.type === 'dense' && cfg.useBias && b instanceof Float32Array;
              const needsLNState = cfg.type === 'layernorm' && g instanceof Float32Array && beta instanceof Float32Array;

              if (optimizer === 'adam' || optimizer === 'rmsprop' || optimizer === 'adamw') {
                  try {
                      if (needsWState) {
                          const size = w.length;
                          if (optimizer === 'adam' || optimizer === 'adamw') {
                              if (!this.m_dw[i]) this.m_dw[i] = new Float32Array(size).fill(0);
                              if (!this.v_dw[i]) this.v_dw[i] = new Float32Array(size).fill(0);
                          }
                          if (optimizer === 'rmsprop') {
                              if (!this.s_dw[i]) this.s_dw[i] = new Float32Array(size).fill(0);
                          }
                          // Log creation confirmation
                           if (this.debug) console.log(`Phase 1 Refactor: L${i} Dense W Opt State (${optimizer}) init: ${this.m_dw[i]?.length || this.s_dw[i]?.length} elements`);
                      }
                      if (needsBState) {
                          const size = b.length;
                           if (optimizer === 'adam' || optimizer === 'adamw') {
                               if (!this.m_db[i]) this.m_db[i] = new Float32Array(size).fill(0);
                               if (!this.v_db[i]) this.v_db[i] = new Float32Array(size).fill(0);
                           }
                           if (optimizer === 'rmsprop') {
                               if (!this.s_db[i]) this.s_db[i] = new Float32Array(size).fill(0);
                           }
                           if (this.debug) console.log(`Phase 1 Refactor: L${i} Dense B Opt State (${optimizer}) init: ${this.m_db[i]?.length || this.s_db[i]?.length} elements`);
                      }
                      if (needsLNState) {
                          const size = g.length;
                           if (optimizer === 'adam' || optimizer === 'adamw') {
                               if (!this.m_dgamma[i]) this.m_dgamma[i] = new Float32Array(size).fill(0);
                               if (!this.v_dgamma[i]) this.v_dgamma[i] = new Float32Array(size).fill(0);
                               if (!this.m_dbeta[i]) this.m_dbeta[i] = new Float32Array(size).fill(0);
                               if (!this.v_dbeta[i]) this.v_dbeta[i] = new Float32Array(size).fill(0);
                           }
                           if (optimizer === 'rmsprop') {
                               if (!this.s_dgamma[i]) this.s_dgamma[i] = new Float32Array(size).fill(0);
                               if (!this.s_dbeta[i]) this.s_dbeta[i] = new Float32Array(size).fill(0);
                           }
                            if (this.debug) console.log(`Phase 1 Refactor: L${i} LN Opt State (${optimizer}) init: ${this.m_dgamma[i]?.length || this.s_dgamma[i]?.length} elements`);
                      }
                  } catch (e) {
                      console.error(`Phase 1 Refactor: InitOpt State Err L${i} (${optimizer}): ${e.message}`);
                      // Nullify potentially partially created states on error
                      this.m_dw[i]=null; this.v_dw[i]=null; this.s_dw[i]=null; this.m_db[i]=null; this.v_db[i]=null; this.s_db[i]=null; this.m_dgamma[i]=null; this.v_dgamma[i]=null; this.s_dgamma[i]=null; this.m_dbeta[i]=null; this.v_dbeta[i]=null; this.s_dbeta[i]=null;
                  }
              }
          }
          if (this.debug) console.log(`Phase 1 Refactor: Optimizer state init finished.`);
          // {{ End Edit 5 }}
      }

      getTotalParameters() {
          let total = 0;
          if (!this.layers || this.layers.length === 0) return 0;
          this.layers.forEach((l, i) => {
              if (l.type === 'dense') {
                  // {{ Edit 1: Use .length directly on Float32Array (or null) }}
                  total += this.weights[i] ? this.weights[i].length : 0; // Float32Array is already flat
                  total += (l.useBias && this.biases[i]) ? this.biases[i].length : 0; // Also Float32Array
                  // {{ End Edit 1 }}
              } else if (l.type === 'layernorm') {
                  // {{ Edit 2: Use .length directly on Float32Array (or null) }}
                  total += this.gammas[i] ? this.gammas[i].length : 0; // Float32Array
                  total += this.betas[i] ? this.betas[i].length : 0;   // Float32Array
                  // {{ End Edit 2 }}
              }
              // Other layer types (attention, dropout, softmax) don't add parameters in this structure
          });
          // Log the calculated total during debug
          if (this.debug) console.log(`Phase 1 getTotalParameters: Calculated total: ${total}`);
          return total;
      }

      getCurrentLearningRate(epoch, initialLR, options) {
          const { lrSchedule, lrStepDecayFactor, lrStepDecaySize, lrExpDecayRate } = options;
          let currentLR = initialLR;

          if (lrSchedule === 'step') {
              // Ensure step size is at least 1
              const stepSize = Math.max(1, Math.floor(lrStepDecaySize));
              // Calculate how many decay steps have occurred
              const decaySteps = Math.floor(epoch / stepSize);
              currentLR = initialLR * Math.pow(lrStepDecayFactor, decaySteps);
          } else if (lrSchedule === 'exponential') {
              // Decay happens every epoch
              currentLR = initialLR * Math.pow(lrExpDecayRate, epoch);
          }
          // 'none' schedule uses initialLR

          return currentLR;
      }

      async train(trainSet, options = {}) {
        this.isTraining = true; const start = Date.now();
        let {
            epochs = 100, learningRate = 0.01, batchSize = 16, printEveryEpochs = 10,
            earlyStopThreshold = 1e-7, testSet = null, callback = null, optimizer = 'adam',
            lossFunction = 'mse', l2Lambda = 0, decayRate = 0.9,
            usePositionalEncoding = this.usePositionalEncoding, gradientClipValue = 0,
            lrSchedule = 'none', lrStepDecayFactor = 0.1, lrStepDecaySize = 10, lrExpDecayRate = 0.95
        } = options;
        // Store the initial learning rate separately
        const initialLearningRate = learningRate;

        if (!trainSet || trainSet.length === 0) throw new Error("Training set empty."); if (this.layers.length === 0) throw new Error("No layers."); const effectiveBatchSize = Math.max(1, Math.min(batchSize, trainSet.length)); this.usePositionalEncoding = usePositionalEncoding; this.decayRate = decayRate;
        let needsOptimizerInit = this.m_dw?.length !== this.layers.length || this.v_dw?.length !== this.layers.length || this.s_dw?.length !== this.layers.length || this.m_db?.length !== this.layers.length || this.v_db?.length !== this.layers.length || this.s_db?.length !== this.layers.length; for (let i=0; i < this.layers.length && !needsOptimizerInit; i++) { if (this.layers[i].type === 'dense') { if (this.weights[i] && this.m_dw[i] === null) needsOptimizerInit = true; if (this.biases[i] && this.m_db[i] === null) needsOptimizerInit = true; } else if (this.layers[i].type === 'layernorm') { if (this.gammas[i] && this.m_dgamma[i] === null) needsOptimizerInit = true; if (this.betas[i] && this.m_dbeta[i] === null) needsOptimizerInit = true; } } if (needsOptimizerInit) { if (this.debug) console.log("Optimizer state needs init."); this.initializeOptimizerState(optimizer); }
        let lastTrainLoss = Infinity; let lastTestLoss = null;
        // {{ Add: Variable for the validation metric }}
        let lastValidationMetric = null;
        let validationMetricName = ''; // e.g., "Acc" or "R²"

        for (let epoch = 0; epoch < epochs; epoch++) {
          // ++ Calculate Learning Rate for the Current Epoch ++
          const currentEpochLearningRate = this.getCurrentLearningRate(epoch, initialLearningRate, options);
          // -- END LR Calculation --

          let totalEpochTrainError = 0; for (let i = trainSet.length - 1; i > 0; i--) { const j = Math.floor(Math.random() * (i + 1)); [trainSet[i], trainSet[j]] = [trainSet[j], trainSet[i]]; }
          for (let b = 0; b < trainSet.length; b += effectiveBatchSize) {
            const batch = trainSet.slice(b, b + effectiveBatchSize); if (batch.length === 0) continue;
            // {{ Edit: Initialize gradient arrays as Float32Arrays matching parameter array shapes }}
            const gradsW = this.weights.map(L => L instanceof Float32Array ? new Float32Array(L.length).fill(0) : null);
            const gradsB = this.biases.map(L => L instanceof Float32Array ? new Float32Array(L.length).fill(0) : null);
            const gradsGamma = this.gammas.map(L => L instanceof Float32Array ? new Float32Array(L.length).fill(0) : null);
            const gradsBeta = this.betas.map(L => L instanceof Float32Array ? new Float32Array(L.length).fill(0) : null);
            // {{ End Edit }}
            let batchLossSum = 0;
            for (const data of batch) {
                let currentInput = data.input; if (!Array.isArray(currentInput) || !Array.isArray(data.output)) { console.warn("Skip invalid data"); continue; }
                // {{ Edit: Use namespaced utility }}
                if (this.usePositionalEncoding) {
                    currentInput = oblixUtils.positionalEncoding(currentInput);
                }
                // Initialize forward cache (assuming currentInput is Float32Array or gets converted)
                const initialAct = currentInput instanceof Float32Array ? currentInput : new Float32Array(currentInput);
                this.forwardCache = { activations: [initialAct], rawValues: [], layerNormIntermediates: [], attentionIntermediates: [], softmaxOutputs: [] };
                let layerInput = this.forwardCache.activations[0]; // Start with the Float32Array

                for (let i = 0; i < this.layers.length; i++) {
                    const cfg=this.layers[i];
                    let out; // Will store the output Float32Array
                    // Initialize cache entries for this layer
                    this.forwardCache.rawValues[i]=null; this.forwardCache.layerNormIntermediates[i]=null; this.forwardCache.attentionIntermediates[i]=null; this.forwardCache.softmaxOutputs[i]=null;

                    try {
                         // Input validation now uses layerInput which should be Float32Array
                         if (!(layerInput instanceof Float32Array)) throw new Error(`L${i}(${cfg.type}): Internal error - input is not Float32Array.`);
                         if(layerInput.length!==cfg.inputSize) throw new Error(`L${i}(${cfg.type}): Sz mismatch ${layerInput.length}!=${cfg.inputSize}`);

                        switch(cfg.type){
                            case 'dense':
                                const w=this.weights[i]; // Float32Array
                                const b=this.biases[i];  // Float32Array or null
                                if (!(w instanceof Float32Array)) throw new Error(`L${i} Dense: Weights not Float32Array.`);
                                if (b && !(b instanceof Float32Array)) throw new Error(`L${i} Dense: Bias not Float32Array.`);

                                // {{ Edit 1: Correct dense forward pass with 1D indexing }}
                                const rawSums = new Float32Array(cfg.outputSize); // Use Float32Array for raw sums too
                                for(let j=0; j < cfg.outputSize; ++j){ // Output neuron index
                                    let sum = b ? b[j] : 0;
                                    const weightRowOffset = j * cfg.inputSize; // Calculate base index for this output neuron's weights
                                    for(let k=0; k < cfg.inputSize; ++k){ // Input neuron index
                                        // Use 1D indexing: weightRowOffset + k
                                        sum += layerInput[k] * w[weightRowOffset + k];
                                    }
                                    rawSums[j] = sum;
                                }
                                this.forwardCache.rawValues[i] = rawSums; // Cache the Float32Array sums
                                // Apply activation function element-wise
                                out = new Float32Array(cfg.outputSize);
                                for(let j=0; j<cfg.outputSize; ++j) {
                                    out[j] = oblixActivations.apply(rawSums[j], cfg.activation);
                                }
                                // {{ End Edit 1 }}
                                break;
                            case 'layernorm':
                                // Pass Float32Arrays directly
                                out=oblixLayerOps.layerNormForward(this, layerInput,this.gammas[i],this.betas[i]).output;
                                break;
                            case 'attention':
                                // Pass Float32Array directly
                                out=oblixLayerOps.attentionForward(this, layerInput,cfg.numHeads);
                                break;
                            case 'dropout':
                                 // Pass Float32Array directly
                                out=oblixLayerOps.dropoutForward(this, layerInput,cfg.rate);
                                break;
                            case 'softmax':
                                 // Pass Float32Array directly
                                out=oblixLayerOps.softmaxForward(this, layerInput);
                                break;
                            default: throw new Error(`Fwd Pass: Unknown type ${cfg.type}`);
                        }
                         // Ensure output is Float32Array before caching and passing on
                         if (!(out instanceof Float32Array)) throw new Error(`L${i}(${cfg.type}): Internal error - output is not Float32Array.`);
                        this.forwardCache.activations.push(out); // Cache the output activation Float32Array
                        layerInput=out; // Update layerInput for the next iteration
                    } catch(e){ console.error(`Fwd L${i}(${cfg.type}) Err:`,e); this.isTraining=false; throw e; }
                } // End forward layer loop

                const finalOut=layerInput; // Should be Float32Array
                const targetOut=data.output; // Still standard array
                if(finalOut.length!==targetOut.length) throw new Error(`Output/Target len mismatch`);
                let dLastErr; // Will become Float32Array
                const eps_ce=1e-9;

                if(lossFunction==='crossentropy'){
                    let loss=0;
                    const lastLyr=this.layers[this.layers.length-1];
                    const wasSoftmax=lastLyr.type==='softmax'||(lastLyr.type==='dense'&&lastLyr.activation==='softmax');
                    const wasSigmoid=lastLyr.type==='dense'&&lastLyr.activation==='sigmoid';

                    if(wasSoftmax){
                        const oneHotTarget = new Float32Array(finalOut.length).fill(0); // Use Float32Array
                        if(targetOut.length===1 && Number.isInteger(targetOut[0]) && targetOut[0]>=0 && targetOut[0]<finalOut.length){
                             oneHotTarget[targetOut[0]]=1; // Target is index
                        } else if (targetOut.length === finalOut.length) {
                            for(let i=0; i<targetOut.length; ++i) oneHotTarget[i] = targetOut[i]; // Target is vector
                        } else {
                            throw new Error("CE target unclear");
                        }
                        // Calculate loss (using potentially Float32Arrays)
                        for(let i=0; i<finalOut.length; ++i) loss -= oneHotTarget[i] * Math.log(finalOut[i] + eps_ce);
                        // Calculate initial gradient (output - target) as Float32Array
                        dLastErr = new Float32Array(finalOut.length);
                        for(let i=0; i<finalOut.length; ++i) dLastErr[i] = finalOut[i] - oneHotTarget[i];

                    } else if(wasSigmoid){
                        if(finalOut.length!==1||targetOut.length!==1)throw new Error("BCE needs single out/target");
                        const p=finalOut[0], t=targetOut[0];
                        loss=-(t*Math.log(p+eps_ce)+(1-t)*Math.log(1-p+eps_ce));
                        dLastErr=new Float32Array([p-t]); // Gradient as Float32Array

                    } else {
                        console.warn("CE w/o final softmax/sigmoid, using simple diff");
                        // Calculate initial gradient (output - target) as Float32Array
                        dLastErr = new Float32Array(finalOut.length);
                        for(let i=0; i<finalOut.length; ++i) dLastErr[i] = finalOut[i] - targetOut[i];
                        // Loss calculation remains tricky without softmax/sigmoid assumption
                        loss = 0.5 * dLastErr.reduce((s,e)=>s+e*e,0); // Using MSE as fallback loss metric
                    }
                    if(!isNaN(loss))batchLossSum+=loss;

                } else { // MSE Loss
                    // Calculate initial gradient (output - target) as Float32Array
                    dLastErr=new Float32Array(finalOut.length);
                    let loss = 0;
                    for(let i=0; i<finalOut.length; ++i) {
                        const diff = finalOut[i] - targetOut[i];
                        dLastErr[i] = diff;
                        loss += diff * diff;
                    }
                    loss *= 0.5;
                    if(!isNaN(loss))batchLossSum+=loss;
                }

                // --- Start Backpropagation ---
                let dAct = dLastErr; // This is now a Float32Array

                for(let i=this.layers.length-1; i>=0; i--){
                    const cfg=this.layers[i];
                    const act_prev=this.forwardCache.activations[i]; // Previous layer's output (Float32Array)
                    let dIn; // Gradient w.r.t input of this layer (will be Float32Array)

                     // Validate dAct is Float32Array and correct size
                     if(!(dAct instanceof Float32Array) || dAct.length!==cfg.outputSize){
                         console.warn(`Bkwd L${i}(${cfg.type}): Invalid dAct. Type: ${dAct?.constructor?.name}, Len: ${dAct?.length}. Expected Len: ${cfg.outputSize}. Using zeros.`);
                         // Initialize dIn correctly as Float32Array of zeros
                         dIn = new Float32Array(cfg.inputSize).fill(0);
                         dAct = new Float32Array(cfg.outputSize).fill(0); // Prevent error propagation
                         continue; // Skip to next layer if gradient is corrupted
                     }

                    try{
                        switch(cfg.type){
                            case 'dense':
                                const w=this.weights[i]; // Float32Array
                                const b=this.biases[i];  // Float32Array or null
                                const raw=this.forwardCache.rawValues[i]; // Float32Array or null
                                const act=cfg.activation;
                                const inSz=cfg.inputSize;
                                const outSz=cfg.outputSize;

                                if (!(raw instanceof Float32Array)) throw new Error(`L${i} Dense Bkwd: Missing or invalid raw values cache.`);
                                if (!(w instanceof Float32Array)) throw new Error(`L${i} Dense Bkwd: Weights not Float32Array.`);
                                if (!(act_prev instanceof Float32Array)) throw new Error(`L${i} Dense Bkwd: Previous activation not Float32Array.`);

                                // {{ Edit 2: Correct dense backprop with 1D indexing and Float32Arrays }}
                                const delta = new Float32Array(outSz); // Gradient * derivative
                                for(let j=0; j<outSz; ++j) {
                                    const deriv = oblixActivations.derivative(raw[j], act);
                                    if(typeof deriv!=='number'||!isFinite(deriv)){
                                        console.warn(`L${i} Dense, j=${j}: Deriv NaN/Inf. Activation: ${act}, Raw Input: ${raw[j]}, Derivative: ${deriv}`);
                                        delta[j] = 0; // Set to zero if derivative is invalid
                                    } else {
                                        delta[j] = dAct[j] * deriv; // dError/dRaw = dError/dAct * dAct/dRaw
                                    }
                                }

                                // Calculate gradient w.r.t previous layer's activation (dIn)
                                dIn = new Float32Array(inSz).fill(0);
                                for(let k=0; k<inSz; k++){ // Iterate input neurons of current layer (output of prev layer)
                                    for(let j=0; j<outSz; j++){ // Iterate output neurons of current layer
                                        const weightIndex = j * inSz + k; // Correct 1D index
                                        dIn[k] += delta[j] * w[weightIndex]; // Sum contributions weighted by transpose
                                    }
                                }

                                // Calculate gradients w.r.t. weights (gradsW) and biases (gradsB)
                                const gW = gradsW[i]; // The accumulator Float32Array
                                const gB = gradsB[i]; // The accumulator Float32Array or null
                                if (gW) {
                                    for(let j=0; j<outSz; j++){ // Iterate output neurons
                                        const weightRowOffset = j * inSz;
                                        for(let k=0; k<inSz; k++){ // Iterate input neurons
                                            gW[weightRowOffset + k] += delta[j] * act_prev[k]; // dLoss/dW = delta * activation_prev
                                        }
                                    }
                                }
                                if (gB) {
                                    for(let j=0; j<outSz; j++){
                                        gB[j] += delta[j]; // dLoss/dBias = delta
                                    }
                                }
                                // {{ End Edit 2 }}
                                break;
                            case 'layernorm':
                                const lnCache = this.forwardCache.layerNormIntermediates[i];
                                if(!lnCache) throw new Error(`L${i} LN Bkwd: Missing cache`);
                                const { dInput: ln_dIn, dGamma, dBeta } = oblixLayerOps.layerNormBackward(this, dAct, lnCache);
                                dIn = ln_dIn; // Float32Array
                                const gGamma = gradsGamma[i]; // Accumulator
                                const gBeta = gradsBeta[i]; // Accumulator
                                if(gGamma && gBeta) {
                                     for(let j=0; j<dGamma.length; j++){
                                         gGamma[j] += dGamma[j] || 0;
                                         gBeta[j] += dBeta[j] || 0;
                                     }
                                }
                                break;
                            case 'attention':
                                const attnCache = this.forwardCache.attentionIntermediates[i];
                                if(!attnCache) throw new Error(`L${i} Attn Bkwd: Missing cache`);
                                const { dInput: attn_dIn } = oblixLayerOps.attentionBackward(this, dAct, attnCache);
                                dIn = attn_dIn; // Float32Array
                                // No trainable parameters in this attention implementation
                                break;
                            case 'dropout':
                                dIn = oblixLayerOps.dropoutBackward(this, dAct, i); // Float32Array
                                break;
                            case 'softmax':
                                dIn = oblixLayerOps.softmaxBackward(this, dAct, i); // Float32Array
                                break;
                            default: throw new Error(`Bkwd Pass: Unknown type ${cfg.type}`);
                        }
                         // Ensure dIn is Float32Array before assigning to dAct for next iteration
                         if (!(dIn instanceof Float32Array)) throw new Error(`Bkwd L${i}(${cfg.type}): Internal error - dIn is not Float32Array.`);
                        dAct=dIn; // Pass gradient back to the previous layer

                    } catch(e){console.error(`Bkwd L${i}(${cfg.type}) Err:`,e); this.isTraining=false; throw e;}
                } // End bkwd layer loop
            } // End batch sample loop

            // --- Parameter Update ---
            // {{ Edit: Delegate parameter updates to namespace }}
            const updateOptions = {
                learningRate: currentEpochLearningRate,
                initialLearningRate: initialLearningRate,
                optimizer: optimizer,
                batchSize: batch.length, // Use actual batch length
                l2Lambda: l2Lambda,
                gradientClipValue: gradientClipValue,
                decayRate: this.decayRate // Pass context's decayRate
            };
            oblixOptimizers.updateParameters(this, gradsW, gradsB, gradsGamma, gradsBeta, updateOptions);

            totalEpochTrainError += batchLossSum;
          } // End batch loop

          lastTrainLoss = totalEpochTrainError / trainSet.length;

          // --- Validation Step ---
          if (testSet && testSet.length > 0) {
              let testError = 0;
              const allValPredictions = [];
              const allValTargets = [];

              for (const data of testSet) {
                  const prediction = this.predict(data.input);
                  if (prediction && data.output && prediction.length === data.output.length) {
                      const target = data.output;
                      allValPredictions.push(prediction);
                      allValTargets.push(target);

                      // --- Calculate Loss (Explicitly include CE loss) ---
                      let sampleLoss = 0;
                      const eps_ce = 1e-9;
                      if (lossFunction === 'crossentropy') {
                          // {{ Add: Explicit Cross-Entropy sample loss calculation }}
                          const lastLayer = this.layers[this.layers.length-1];
                          const wasSoftmax = lastLayer.type === 'softmax' || (lastLayer.type === 'dense' && lastLayer.activation === 'softmax');
                          const wasSigmoid = lastLayer.type === 'dense' && lastLayer.activation === 'sigmoid';

                          if (wasSoftmax) {
                              // Handle one-hot or index targets
                              if (target.length === 1 && Number.isInteger(target[0]) && target[0] >= 0 && target[0] < prediction.length) {
                                  // Target is a class index
                                  sampleLoss = -Math.log(prediction[target[0]] + eps_ce);
                              } else if (target.length === prediction.length) {
                                   // Target is likely a probability distribution/one-hot vector
                                  sampleLoss = -target.reduce((sum, t, i) => sum + t * Math.log(prediction[i] + eps_ce), 0);
                              } else {
                                  console.warn("CE Val Loss: Target/Prediction shape mismatch for Softmax.");
                                  sampleLoss = NaN; // Cannot calculate
                              }
                          } else if (wasSigmoid && prediction.length === 1 && target.length === 1) {
                              // Binary Cross-Entropy
                              const p = prediction[0]; const t = target[0];
                              sampleLoss = - (t * Math.log(p + eps_ce) + (1 - t) * Math.log(1 - p + eps_ce));
                          } else {
                              // Fallback if CE is selected but final layer isn't appropriate
                              console.warn("CE Val Loss: Final layer activation not Softmax/Sigmoid. Using MSE for loss metric.");
                              sampleLoss = 0.5 * prediction.reduce((sum, p, i) => sum + (p - target[i])**2, 0);
                          }
                      } else { // Default to MSE
                          sampleLoss = 0.5 * prediction.reduce((sum, p, i) => sum + (p - target[i])**2, 0);
                      }
                      // {{ End Add }}

                      if (!isNaN(sampleLoss) && isFinite(sampleLoss)) {
                          testError += sampleLoss;
                      }
                  }
              }
              lastTestLoss = testError / testSet.length; // Average loss over validation set

              // --- Calculate Accuracy or R-squared ---
              if (lossFunction === 'crossentropy') {
                  validationMetricName = 'Acc';
                  // Targets might be [[0], [1], [0]] or [0, 1, 0]
                  // Accuracy function handles both index and one-hot vector targets
                  lastValidationMetric = oblixUtils.calculateAccuracy(allValPredictions, allValTargets);
              } else { // Default to R-squared for MSE / other regression tasks
                  validationMetricName = 'R²';
                  // R-squared expects flat arrays of numbers
                  // Flatten predictions/targets if they are multi-dimensional (e.g., [[0.1], [0.9]])
                  const flatPreds = allValPredictions.flat();
                  const flatTargets = allValTargets.flat();

                  // {{ Add Logging: Verify types AFTER flattening }}
                  if (this.debug) {
                      console.log(`R-Squared Type Check: flatPreds type = ${flatPreds?.constructor?.name}, flatTargets type = ${flatTargets?.constructor?.name}`);
                      console.log(`R-Squared Input Check: flatPreds[0]=${flatPreds[0]}, flatTargets[0]=${flatTargets[0]}`);
                      if (flatPreds.length > 1) console.log(`  flatPreds[1]=${flatPreds[1]}, flatTargets[1]=${flatTargets[1]}`);
                  }
                  // {{ End Add Logging }}

                  lastValidationMetric = oblixUtils.calculateRSquared(flatPreds, flatTargets);
              }

          } else {
              lastTestLoss = null;
              lastValidationMetric = null; // No validation set, no metric
              validationMetricName = '';
          }
          // --- End Validation Step ---

          if ((epoch + 1) % printEveryEpochs === 0 && this.debug) {
              const lrStr = lrSchedule !== 'none' ? `, LR: ${currentEpochLearningRate.toExponential(2)}` : '';
              let logMsg = `Epoch ${epoch + 1}/${epochs}, Train Loss: ${lastTrainLoss.toFixed(6)}`;
              if (lastTestLoss !== null) {
                   logMsg += `, Val Loss: ${lastTestLoss.toFixed(6)}`;
              }
              // {{ Add: Log metric if calculated }}
              if (lastValidationMetric !== null && !isNaN(lastValidationMetric)) {
                  logMsg += `, Val ${validationMetricName}: ${lastValidationMetric.toFixed(4)}`;
              }
               logMsg += lrStr;
               console.log(logMsg);
          }

          // {{ Edit: Pass forwardCache to callback }}
          if (callback) {
              // Pass the forwardCache from the *last* sample processed in the batch/epoch
              // Note: This visualizes the state after processing one sample, which might jump around.
              // Averaging activations over a batch would be smoother but more complex.
              await callback(epoch + 1, lastTrainLoss, lastTestLoss, validationMetricName, lastValidationMetric, this.forwardCache); // Pass the cache
          }

          await new Promise(resolve => setTimeout(resolve, 0)); // Yield to browser
          if (lastTrainLoss < earlyStopThreshold) {
              if (this.debug) console.log(`Early stopping @ Epoch ${epoch + 1}.`);
              epochs = epoch + 1; // Adjust total epochs for summary
              break;
          }
        } // End epoch loop

        const end = Date.now(); this.isTraining = false;
        const totalParams = this.getTotalParameters();

        // {{ Edit: Add validation metric to summary }}
        const trainingSummary = {
            trainLoss: lastTrainLoss,
            testLoss: lastTestLoss,
            validationMetric: { name: validationMetricName, value: lastValidationMetric },
            parameters: totalParams,
            training: {
                time: end - start, epochs: epochs,
                learningRate: initialLearningRate, // Report initial LR
                batchSize: effectiveBatchSize, optimizer: optimizer, lossFunction: lossFunction,
                l2Lambda: l2Lambda, decayRate: this.decayRate,
                usePositionalEncoding: this.usePositionalEncoding, gradientClipValue: gradientClipValue,
                lrSchedule: lrSchedule,
                lrStepDecayFactor: lrSchedule === 'step' ? lrStepDecayFactor : undefined,
                lrStepDecaySize: lrSchedule === 'step' ? lrStepDecaySize : undefined,
                lrExpDecayRate: lrSchedule === 'exponential' ? lrExpDecayRate : undefined
            },
            layers: this.layers.map(l => ({ type: l.type, inputSize: l.inputSize, outputSize: l.outputSize, activation: l.activation, numHeads: l.numHeads, useBias: l.useBias, rate: l.rate }))
        };
        this.details = trainingSummary;
        if (this.debug) console.log("Training finished.", trainingSummary);
        return trainingSummary;
      }

      predict(input) {
          // {{ Phase 2: Refactor Predict for Native Float32Array Operation }}
          if (this.debug) console.log("Phase 2 Predict: Starting prediction with native Float32Array logic.");

          const wasTraining = this.isTraining; this.isTraining = false;
          if (!this.layers || this.layers.length === 0) { console.error("Predict Error: Model not initialized."); return null; }
          if (!input || typeof input.length !== 'number') { console.error("Predict Error: Invalid input provided.", input); return null; }

          // Ensure input is Float32Array
          let currentInput;
           if (input instanceof Float32Array) {
                currentInput = input;
           } else if (Array.isArray(input)) {
                 if (this.debug) console.log("Phase 2 Predict: Input is standard array, converting to Float32Array.");
                 currentInput = new Float32Array(input);
           } else {
               console.error("Predict Error: Input is not an array or Float32Array.", input);
               return null;
           }

           if (this.debug) console.log(`Phase 2 Predict: Initial Input type=${currentInput.constructor.name}, len=${currentInput.length}`);


          // Apply positional encoding if enabled
        if (this.usePositionalEncoding) {
               if (this.debug) console.log("Phase 2 Predict: Applying positional encoding.");
              currentInput = oblixUtils.positionalEncoding(currentInput); // Now returns Float32Array
              if (this.debug) console.log(`Phase 2 Predict: After PosEnc type=${currentInput.constructor.name}, len=${currentInput.length}`);
          }

          // Store initial (potentially encoded) input
           this.lastActivations = [currentInput]; // Store reference to Float32Array

          try {
              for (let i = 0; i < this.layers.length; i++) {
                  const cfg = this.layers[i];
                  const layerInput = this.lastActivations[this.lastActivations.length - 1]; // This is Float32Array

                   if (this.debug) console.log(`Phase 2 Predict: Processing L${i} (${cfg.type}). Input type=${layerInput.constructor.name}, len=${layerInput.length}`);

                  if (!(layerInput instanceof Float32Array)) { // Safety check
                       throw new Error(`L${i}(${cfg.type}): Internal error - input is not Float32Array.`);
                  }
                  if (layerInput.length !== cfg.inputSize) {
                       throw new Error(`L${i}(${cfg.type}): Size mismatch. Expected ${cfg.inputSize}, got ${layerInput.length}.`);
                  }

                  let output; // Will hold the Float32Array output of the layer

                  switch (cfg.type) {
                      case 'dense':
                          const w = this.weights[i]; // Float32Array
                          const b = this.biases[i];  // Float32Array or null
                          if (!w) throw new Error(`L${i} Dense: Weights not initialized.`);
                          if (!(w instanceof Float32Array)) throw new Error(`L${i} Dense: Weights internal error - not Float32Array.`);
                           if (b && !(b instanceof Float32Array)) throw new Error(`L${i} Dense: Biases internal error - not Float32Array.`);

                          // {{ Edit 3: Correct dense predict pass with 1D indexing }}
                          output = new Float32Array(cfg.outputSize); // Create output array
                          if (this.debug) console.log(`Phase 2 Predict L${i} Dense: InputLen=${layerInput.length}, WeightLen=${w.length}, BiasLen=${b?.length}, OutputLen=${output.length}`);

                          for (let j = 0; j < cfg.outputSize; ++j) { // Iterate output neurons
                              let sum = b ? b[j] : 0; // Get bias
                              const weightRowOffset = j * cfg.inputSize; // Calculate base index for this output neuron's weights

                              // --- Optimized dot product using flat indexing ---
                              for (let k = 0; k < cfg.inputSize; ++k) { // Iterate input neurons
                                  // Use 1D indexing: weightRowOffset + k
                                  sum += layerInput[k] * w[weightRowOffset + k];
                              }
                              // --- End dot product ---

                              output[j] = oblixActivations.apply(sum, cfg.activation); // Apply activation

                               if (this.debug && j === 0 && i < 2) { // Log first neuron sum/output for first few layers
                                   console.log(`Phase 2 Predict L${i} Dense, Neuron 0: Sum=${sum.toFixed(4)}, Activated=${output[0].toFixed(4)}`);
                               }
                          }
                           // {{ End Edit 3 }}
                          break;

                      case 'layernorm':
                          const gamma = this.gammas[i];
                          const beta = this.betas[i];
                          if (!gamma || !beta) throw new Error(`L${i} LayerNorm: Gamma/Beta not initialized.`);
                           if (!(gamma instanceof Float32Array) || !(beta instanceof Float32Array)) throw new Error(`L${i} LN: Gamma/Beta internal error - not Float32Array.`);

                          // layerNormForward now accepts and returns Float32Arrays internally
                          const { output: lnOut } = oblixLayerOps.layerNormForward(this, layerInput, gamma, beta);
                          output = lnOut; // lnOut is already Float32Array
                          break;

                      case 'attention':
                           // attentionForward now accepts and returns Float32Array
                          output = oblixLayerOps.attentionForward(this, layerInput, cfg.numHeads);
                          break;

                      case 'dropout':
                           // dropoutForward now accepts and returns Float32Array
                           // It handles the isTraining check internally
                          output = oblixLayerOps.dropoutForward(this, layerInput, cfg.rate);
                          break;

                      case 'softmax':
                           // softmaxForward now accepts and returns Float32Array
                          output = oblixLayerOps.softmaxForward(this, layerInput);
                          break;

                      default:
                          throw new Error(`Predict: Unknown layer type ${cfg.type}`);
                  }

                   if (!(output instanceof Float32Array)) { // Final check
                       throw new Error(`L${i}(${cfg.type}): Internal error - output is not Float32Array.`);
                  }
                   if (this.debug) console.log(`Phase 2 Predict: Output L${i} (${cfg.type}) type=${output.constructor.name}, len=${output.length}, first val=${output[0]?.toFixed(4)}`);
                  this.lastActivations.push(output); // Store reference to output Float32Array
              }

              this.isTraining = wasTraining; // Restore training state
               const finalOutput = this.lastActivations[this.lastActivations.length - 1];
               if (this.debug) console.log(`Phase 2 Predict: Finished. Final output type=${finalOutput?.constructor?.name}, len=${finalOutput?.length}`);
              return finalOutput; // Return the final Float32Array output

          } catch (error) {
              console.error("Prediction Error:", error);
              this.lastActivations = null; // Clear activations on error
              this.isTraining = wasTraining; // Restore training state
              return null;
          }
      }
      // {{ End Phase 2 Refactor }}

      save(name = 'model') {
          if (!this.layers || this.layers.length === 0) { console.warn("Save: Empty model."); }
          const numLayers = this.layers.length;
          // Helper to ensure state arrays match layer count before saving
          const ensureLen = (arrName, dv = null) => {
              let currentArr = this[arrName];
              if (!Array.isArray(currentArr) || currentArr.length !== numLayers) {
                   console.warn(`Phase 1 Refactor Save: Adjusting length of ${arrName} to ${numLayers}`);
                   const newArr = Array(numLayers).fill(dv);
                   if (Array.isArray(currentArr)) {
                       for (let i=0; i<Math.min(numLayers, currentArr.length); ++i) newArr[i] = currentArr[i];
                   }
                   return newArr;
              }
              return currentArr;
          };

          // Prepare optimizer state, ensuring correct lengths
          const optimizerState = {
              t: this.t,
              m_dw: ensureLen('m_dw'), v_dw: ensureLen('v_dw'),
              m_db: ensureLen('m_db'), v_db: ensureLen('v_db'),
              m_dgamma: ensureLen('m_dgamma'), v_dgamma: ensureLen('v_dgamma'),
              m_dbeta: ensureLen('m_dbeta'), v_dbeta: ensureLen('v_dbeta'),
              s_dw: ensureLen('s_dw'), s_db: ensureLen('s_db'),
              s_dgamma: ensureLen('s_dgamma'), s_dbeta: ensureLen('s_dbeta')
          };

          // JSON.stringify handles Float32Array by converting to { "0": val0, "1": val1, ... } objects
          const data = {
              weights: ensureLen('weights'), // Already Float32Array or null
              biases: ensureLen('biases'), // Already Float32Array or null
              gammas: ensureLen('gammas'), // Already Float32Array or null
              betas: ensureLen('betas'),   // Already Float32Array or null
              layers: this.layers,
              details: this.details,
              usePositionalEncoding: this.usePositionalEncoding,
              optimizerState: optimizerState // Contains Float32Arrays or null
          };

          try {
              if (this.debug) console.log("Phase 1 Refactor Save: Preparing data object:", data);
              const jsonStr = JSON.stringify(data);
              if (this.debug) console.log("Phase 1 Refactor Save: Stringified JSON (length):", jsonStr.length);
              // Check if Float32Arrays were stringified as objects (they should be)
              if (this.debug && data.weights[0] instanceof Float32Array) {
                  console.log("Phase 1 Refactor Save: Sample stringified weight (should be object):", jsonStr.substring(0, 500).includes('"weights":[{"0":'));
              }

              const blob = new Blob([jsonStr], { type: 'application/json' });
              const url = URL.createObjectURL(blob);
              const a = document.createElement('a');
              a.href = url; a.download = `${name}.json`; document.body.appendChild(a);
              a.click(); document.body.removeChild(a); URL.revokeObjectURL(url);
              if (this.debug) console.log(`Model saved: ${name}.json`);
          } catch (e) {
              console.error("Save failed.", e);
              if (this.debug) console.error("Phase 1 Refactor Save: Error during stringify or download.");
          }
      }

      load(callback) {
          const input = document.createElement('input'); input.type = 'file'; input.accept = '.json'; input.style.display = 'none';
          const handleListener = (event) => {
              const file = event.target.files[0]; if (!file) { cleanup(); return; }
              const reader = new FileReader();
              reader.onload = (e) => {
                  const text = e.target.result;
                  try {
                      if (this.debug) console.log("Phase 1 Refactor Load: Reading file text...");
                      const data = JSON.parse(text);
                      if (!data.layers || !Array.isArray(data.layers)) throw new Error("Invalid model: 'layers' missing.");
                      if (this.debug) console.log("Phase 1 Refactor Load: Parsed data, layers found:", data.layers.length);

                      this.reset(); // Reset state before loading
                      this.layers = data.layers;
                      this.details = data.details || {};
                      this.usePositionalEncoding = data.usePositionalEncoding || false;
                      const numLayers = this.layers.length;

                      // --- Helper to load arrays and reconstruct Float32Arrays ---
                      const loadAndReconstruct = (arrName, sourceObj, expectedLen) => {
                          let loadedArr = sourceObj?.[arrName] || [];
                           if (!Array.isArray(loadedArr)) { // Ensure it's an array
                               console.warn(`Phase 1 Refactor Load: ${arrName} in loaded data is not an array, creating default.`);
                               loadedArr = [];
                           }
                           // Ensure array has the correct length, filling with null if needed
                           if (loadedArr.length !== expectedLen) {
                                console.warn(`Phase 1 Refactor Load: ${arrName} length mismatch (expected ${expectedLen}, got ${loadedArr.length}). Adjusting...`);
                                const adjustedArr = Array(expectedLen).fill(null);
                                for(let i=0; i<Math.min(expectedLen, loadedArr.length); ++i) adjustedArr[i] = loadedArr[i];
                                loadedArr = adjustedArr;
                           }

                           // Reconstruct Float32Array from { "0": ..., "1": ... } objects
                           return loadedArr.map((item, index) => {
                               if (item !== null && typeof item === 'object' && item.hasOwnProperty('0')) {
                                   // Potential Float32Array object representation
                                   const values = Object.values(item);
                                   // {{ Edit 1: Modify the check and add detailed logging on failure }}
                                   const allNumbers = values.every(v => typeof v === 'number' && isFinite(v)); // Also check for finite numbers

                                   if (allNumbers) {
                                   // {{ End Edit 1 }}
                                       const reconstructed = new Float32Array(values);
                                       if (this.debug) console.log(`Phase 1 Refactor Load: Reconstructed Float32Array for ${arrName}[${index}], Length: ${reconstructed.length}`);
                                       return reconstructed;
                                   } else {
                                       // {{ Edit 2: Log the specific problematic values }}
                                       console.warn(`Phase 1 Refactor Load: Object for ${arrName}[${index}] looks like Float32Array but check failed. Logging values:`);
                                       // Log only the first few non-numeric/non-finite values found
                                       let loggedCount = 0;
                                       for (let k=0; k<values.length && loggedCount < 5; k++) {
                                           const v = values[k];
                                           if (typeof v !== 'number' || !isFinite(v)) {
                                                console.warn(`  - ${arrName}[${index}], Value[${k}]: Type=${typeof v}, Value=${v}`);
                                                loggedCount++;
                                           }
                                       }
                                       if (loggedCount === 0 && values.length > 0) { // Should not happen if allNumbers is false, but safety check
                                            console.warn(`  - ${arrName}[${index}]: Check failed but couldn't find non-numeric/non-finite value? First value:`, values[0]);
                                       }
                                       // {{ End Edit 2 }}
                                       return null; // Data corruption or unexpected format?
                                   }
                               } else if (item instanceof Float32Array) {
                                   // Should not happen from JSON.parse, but safety check
                                    if (this.debug) console.log(`Phase 1 Refactor Load: Item ${arrName}[${index}] is already Float32Array? Length: ${item.length}`);
                                    return item;
                               } else if (item === null) { // Check for null *before* instanceof check
                                    return null; // Keep nulls explicitly
                               } else { // Handle other unexpected types like empty objects, numbers, strings etc.
                                   console.warn(`Phase 1 Refactor Load: Unexpected item type for ${arrName}[${index}] (Type: ${typeof item}). Setting to null. Value:`, item);
                                   return null;
                               }
                           });
                      };
                      // --- End Helper ---

                      this.weights = loadAndReconstruct('weights', data, numLayers);
                      this.biases = loadAndReconstruct('biases', data, numLayers);
                      this.gammas = loadAndReconstruct('gammas', data, numLayers);
                      this.betas = loadAndReconstruct('betas', data, numLayers);
                      this.masks = Array(numLayers).fill(null); // Masks are transient

                      const optState = data.optimizerState || {};
                      this.t = optState.t || 0;
                      if (this.debug) console.log("Phase 1 Refactor Load: Loading optimizer state...");
                      this.m_dw = loadAndReconstruct('m_dw', optState, numLayers);
                      this.v_dw = loadAndReconstruct('v_dw', optState, numLayers);
                      this.m_db = loadAndReconstruct('m_db', optState, numLayers);
                      this.v_db = loadAndReconstruct('v_db', optState, numLayers);
                      this.m_dgamma = loadAndReconstruct('m_dgamma', optState, numLayers);
                      this.v_dgamma = loadAndReconstruct('v_dgamma', optState, numLayers);
                      this.m_dbeta = loadAndReconstruct('m_dbeta', optState, numLayers);
                      this.v_dbeta = loadAndReconstruct('v_dbeta', optState, numLayers);
                      this.s_dw = loadAndReconstruct('s_dw', optState, numLayers);
                      this.s_db = loadAndReconstruct('s_db', optState, numLayers);
                      this.s_dgamma = loadAndReconstruct('s_dgamma', optState, numLayers);
                      this.s_dbeta = loadAndReconstruct('s_dbeta', optState, numLayers);

                      this.lastActivations = null; this.forwardCache = null; this.isTraining = false;
                      if (callback) callback();
                      if (this.debug) console.log('Phase 1 Refactor: Model loaded successfully. Stored parameters/states should be Float32Arrays or null.');
                      if (this.debug && this.weights.length > 0) console.log('Phase 1 Refactor Load: Sample loaded weight type:', this.weights[0] instanceof Float32Array ? 'Float32Array' : typeof this.weights[0]);

                  } catch (err) {
                      console.error('Load failed:', err);
                      alert(`Error loading model: ${err.message}`);
                       if (this.debug) console.error("Phase 1 Refactor Load: Error during parsing or reconstruction.");
                      if (callback) callback(err);
                  } finally {
                      cleanup();
                  }
              };
              reader.onerror = (err) => { console.error('File read error:', err); alert('Error reading file.'); cleanup(); if (callback) callback(err); };
              reader.readAsText(file);
          };
          const cleanup = () => { input.removeEventListener('change', handleListener); document.body.removeChild(input); };
          input.addEventListener('change', handleListener); document.body.appendChild(input); input.click();
      }

    } // End oblix class


    // --- UI Interaction Logic ---
    document.addEventListener('DOMContentLoaded', () => {
        const nn = new oblix(true);
        let lossHistory = [];
        const lossCanvas = document.getElementById('lossGraph'); const networkCanvas = document.getElementById('networkGraph'); const lossCtx = lossCanvas?.getContext('2d'); const networkCtx = networkCanvas?.getContext('2d');
        const statsEl = document.getElementById('stats'); const trainButton = document.getElementById('trainButton'); const predictButton = document.getElementById('predictButton'); const saveButton = document.getElementById('saveButton'); const loadButton = document.getElementById('loadButton'); const unloadButton = document.getElementById('unloadButton'); const epochBar = document.getElementById('epochBar'); const predictionResultEl = document.getElementById('predictionResult'); const numHiddenLayersInput = document.getElementById('numHiddenLayers'); const hiddenLayersConfigContainer = document.getElementById('hiddenLayersConfig'); const optimizerSelect = document.getElementById('optimizer'); const usePositionalEncodingCheckbox = document.getElementById('usePositionalEncoding'); const lossFunctionSelect = document.getElementById('lossFunction'); const l2LambdaInput = document.getElementById('l2Lambda'); const decayRateGroup = document.getElementById('decayRateGroup'); const decayRateInput = document.getElementById('decayRate'); const gradientClipValueInput = document.getElementById('gradientClipValue'); const trainingDataTextarea = document.getElementById('trainingData'); const testDataTextarea = document.getElementById('testData'); const epochsInput = document.getElementById('epochs'); const learningRateInput = document.getElementById('learningRate'); const batchSizeInput = document.getElementById('batchSize');
        const lrSchedulerSelect = document.getElementById('lrScheduler');
        const lrStepParamsDiv = document.getElementById('lrStepParams');
        const lrExpParamsDiv = document.getElementById('lrExpParams');
        // {{ Add reference to template select }}
        const architectureTemplateSelect = document.getElementById('architectureTemplateSelect');

        // {{ Define Architecture Templates }}
        const architectureTemplates = {
            'mlp': {
                numHidden: 2,
                layers: [
                    { type: 'dense', size: 16, activation: 'relu', useBias: true },
                    { type: 'dense', size: 8, activation: 'relu', useBias: true }
                ]
            },
            'autoencoder': {
                numHidden: 3,
                layers: [
                    { type: 'dense', size: 16, activation: 'relu', useBias: true }, // Encoder
                    { type: 'dense', size: 4, activation: 'relu', useBias: true },  // Bottleneck
                    { type: 'dense', size: 16, activation: 'relu', useBias: true }  // Decoder
                ],
                // Special hint for final layer config during training
                finalActivationHint: 'sigmoid' // Suggest sigmoid if inputs are 0-1, or 'none' otherwise
            },
            'transformerEncoder': {
                numHidden: 8,
                layers: [
                    // First: Multi-head attention with layer norm
                    { type: 'layernorm', size: null },  // Pre-norm architecture
                    { type: 'attention', numHeads: 3 },
                    { type: 'dense', size: 32, activation: 'relu', useBias: true }, // FFN part 1
                    { type: 'dropout', rate: 0.1 },  // Regularization
                    // Second attention block
                    { type: 'layernorm', size: null },
                    { type: 'attention', numHeads: 4 },
                    { type: 'dense', size: 32, activation: 'relu', useBias: true },
                    { type: 'dropout', rate: 0.1 }
                ],
                // Hint for final layer based on task
                finalActivationHint: 'none'  // Default to linear output
            },
            'residualAttention': {
                numHidden: 5,
                layers: [
                    { type: 'layernorm' },
                    { type: 'attention', numHeads: 3 },
                    { type: 'dense', size: 32, activation: 'relu', useBias: true },
                    { type: 'dropout', rate: 0.1 },
                    { type: 'layernorm' }
                ],
                finalActivationHint: 'none'
            }
            // Add more templates here
        };
        // {{ End Template Definitions }}

        // {{ Add watermark logic }}
        // --- Start Watermark ---
        try {
          function _wm(c){if(!c)return;let d=Math.max(0,Math.floor((new Date()-new Date(2023,0,1))/864e5)),r=0x4F,g=(d>>8)&255,b=d&255;c.fillStyle=`rgb(${r},${g},${b})`;c.fillRect(0,0,1,1)}
          _wm(document.getElementById('watermarkCanvas')?.getContext('2d'));
          // if (nn.debug) console.log('Watermark attempt finished.');
        } catch (e) { console.error("Watermark err:", e); }
        // --- End Watermark ---

        function generateRandomData(numSamples, numInputs, numOutputs = 1, noiseLevel = 0.05) { if (numInputs <= 0 || numOutputs <= 0) return ""; const data = []; for (let i = 0; i < numSamples; i++) { const input = []; for (let j = 0; j < numInputs; j++) input.push(Math.random()); const output = []; for (let j = 0; j < numOutputs; j++) { const base = Math.sin(input[0] * Math.PI * 2) * 0.4 + 0.5; const noise = (Math.random() - 0.5) * 2 * noiseLevel; let final = Math.max(0.01, Math.min(0.99, base + noise)); output.push(final); } data.push([...input, ...output].map(v => v.toFixed(3)).join(', ')); } return data.join('\n'); }
        document.getElementById("generateDataBtn").addEventListener('click', () => { 
            // Read values from inputs, with fallbacks to original defaults
            const numTrain = parseInt(document.getElementById("numTrainSamples").value) || 100;
            const numTest = parseInt(document.getElementById("numTestSamples").value) || 25;
            const numIn = parseInt(document.getElementById("numInputDims").value) || 3;
            const numOut = parseInt(document.getElementById("numOutputDims").value) || 1;
            const noise = parseFloat(document.getElementById("noiseLevel").value) || 0.05;
            
            // Validate within reasonable ranges
            const safeNumTrain = Math.max(1, Math.min(1000, numTrain));
            const safeNumTest = Math.max(1, Math.min(500, numTest));
            const safeNumIn = Math.max(1, Math.min(50, numIn));
            const safeNumOut = Math.max(1, Math.min(20, numOut));
            const safeNoise = Math.max(0, Math.min(1, noise));
            
            // Generate data with the configured parameters
            trainingDataTextarea.value = generateRandomData(safeNumTrain, safeNumIn, safeNumOut, safeNoise);
            testDataTextarea.value = generateRandomData(safeNumTest, safeNumIn, safeNumOut, safeNoise);
            
            // Update UI feedback
            statsEl.innerHTML = `Generated ${safeNumTrain}/${safeNumTest} random samples with ${safeNumIn} inputs, ${safeNumOut} outputs, and ${safeNoise.toFixed(2)} noise.`;
            
            try {
                const sample = parseCSV(trainingDataTextarea.value)[0];
                if(sample && nn.layers && nn.layers.length > 0) nn.predict(sample.input);
            } catch (e) {}
            
            drawNetwork();
        });
        function parseCSV(csvString) { if (!csvString || typeof csvString !== 'string') return []; return csvString.trim().split('\n').map(r=>r.trim()).filter(r=>r.length>0).map((r,idx)=>{ const v=r.split(',').map(v=>parseFloat(v.trim())); if(v.some(isNaN)){console.warn(`R ${idx+1} NaN`);return null;} if(v.length<2){console.warn(`R ${idx+1} <2 vals`);return null;} const i=v.slice(0,-1), o=v.slice(-1); if(i.length===0){console.warn(`R ${idx+1} no input`);return null;} return {input:i,output:o}; }).filter(i=>i!==null); }
        function drawLossGraph() { if (!lossCtx || !lossCanvas) return; lossCtx.clearRect(0, 0, lossCanvas.width, lossCanvas.height); if (lossHistory.length < 2) return; const trainL=lossHistory.map(h=>h.train).filter(l=>l!==null&&isFinite(l)); const testL=lossHistory.map(h=>h.test).filter(l=>l!==null&&isFinite(l)); let maxL=0.1; if(trainL.length>0)maxL=Math.max(maxL,...trainL); if(testL.length>0)maxL=Math.max(maxL,...testL); maxL=Math.max(maxL,0.1); const W=lossCanvas.width,H=lossCanvas.height,nPts=lossHistory.length,pH=H*0.9,yOff=H*0.05; const plot=(ctx,pts,c)=>{ ctx.strokeStyle=c; ctx.lineWidth=1.5; ctx.beginPath(); let first=true; pts.forEach((p,i)=>{if(p!==null&&isFinite(p)){const x=(i/Math.max(1,nPts-1))*W; const y=H-(p/maxL)*pH-yOff; if(first){ctx.moveTo(x,y);first=false;}else{ctx.lineTo(x,y);}}else{first=true;}}); ctx.stroke(); }; const trainC=getComputedStyle(document.body).getPropertyValue('--text')?.trim()||'#fff'; plot(lossCtx,lossHistory.map(h=>h.train),trainC); plot(lossCtx,lossHistory.map(h=>h.test),'#87CEEB'); }

         function createLayerConfigUI(numLayers) {
            hiddenLayersConfigContainer.innerHTML = '';
            const activationTypes = ['tanh', 'sigmoid', 'relu', 'leakyrelu', 'gelu', 'selu', 'swish', 'mish', 'softmax', 'none'];
            const layerTypes = ['dense', 'layernorm', 'attention', 'dropout', 'softmax'];
            if (numLayers === 0) { hiddenLayersConfigContainer.innerHTML = '<p class="layer-note">No hidden layers. Direct input-to-output connection (final layer added automatically).</p>'; return; }
            for (let i = 0; i < numLayers; i++) { const layerGroup = document.createElement('div'); layerGroup.className = 'input-group settings-grid'; const typeDiv = document.createElement('div'); typeDiv.className = 'input-group'; const typeLabel = document.createElement('label'); typeLabel.textContent = `Layer ${i+1} Type:`; typeLabel.htmlFor = `layerType_${i}`; typeLabel.title = "Choose the operation for this layer..."; const typeSelect = document.createElement('select'); typeSelect.id = `layerType_${i}`; typeSelect.dataset.layerIndex = i; typeSelect.dataset.configType = 'type'; layerTypes.forEach(t => { const o = document.createElement('option'); o.value = t; o.textContent = t; if (t === 'dense') o.selected = true; typeSelect.appendChild(o); }); typeDiv.appendChild(typeLabel); typeDiv.appendChild(typeSelect); layerGroup.appendChild(typeDiv); const optionsDiv = document.createElement('div'); optionsDiv.className = 'layer-options-container'; optionsDiv.dataset.layerIndex = i; layerGroup.appendChild(optionsDiv);
            // {{ Add listener to layer type select for manual changes }}
            typeSelect.addEventListener('change', () => architectureTemplateSelect.value = 'custom');

            const updateOptionsUI = (idx, selType) => { const optsDiv = hiddenLayersConfigContainer.querySelector(`.layer-options-container[data-layer-index='${idx}']`); if (!optsDiv) return; optsDiv.innerHTML = ''; const createIn = (l,id,t,v,mn,st,cfg,nt=null,tt=null)=>{const dv=document.createElement('div'); dv.className='input-group'; const lb=document.createElement('label'); lb.textContent=l; lb.htmlFor=id; if(tt)lb.title=tt; const ip=document.createElement('input'); ip.type=t; ip.id=id; ip.value=v; if(mn!==null)ip.min=mn; if(st!==null)ip.step=st; ip.dataset.layerIndex=idx; ip.dataset.configType=cfg; dv.appendChild(lb); dv.appendChild(ip); if(nt){const p=document.createElement('p');p.className='layer-note';p.textContent=nt;dv.appendChild(p);} return dv;}; const createSel = (l,id,opts,selV,cfg,tt=null)=>{const dv=document.createElement('div'); dv.className='input-group'; const lb=document.createElement('label'); lb.textContent=l; lb.htmlFor=id; if(tt)lb.title=tt; const sel=document.createElement('select'); sel.id=id; sel.dataset.layerIndex=idx; sel.dataset.configType=cfg; opts.forEach(o=>{const op=document.createElement('option'); op.value=o; op.textContent=o; if(o===selV)op.selected=true; sel.appendChild(op);}); dv.appendChild(lb); dv.appendChild(sel); return dv;}; const createChk=(l,id,chkd,cfg,tt=null)=>{const dv=document.createElement('div'); dv.className='input-group'; const lb=document.createElement('label'); const ip=document.createElement('input'); ip.type='checkbox'; ip.id=id; ip.checked=chkd; ip.dataset.layerIndex=idx; ip.dataset.configType=cfg; const sp=document.createElement('span'); sp.textContent=l; if(tt)sp.title=tt; lb.appendChild(ip); lb.appendChild(sp); dv.appendChild(lb); return dv;}; const createNt=(txt)=>{const p=document.createElement('p'); p.className='layer-note'; p.textContent=txt; const dv=document.createElement('div'); dv.style.gridColumn='1/-1'; dv.appendChild(p); return dv;};
            if(selType==='dense'){optsDiv.appendChild(createIn('Nodes:',`layerNodes_${idx}`, 'number', 10, 1, 1, 'size', null,"Num neurons.")); optsDiv.appendChild(createSel('Activation:',`layerAct_${idx}`, activationTypes, 'tanh', 'activation', "Neuron output function.")); optsDiv.appendChild(createChk('Use Bias:',`layerBias_${idx}`, true, 'useBias', "Add learnable bias term?"));} else if(selType==='attention'){optsDiv.appendChild(createIn('Num Heads:',`layerHeads_${idx}`, 'number', 2, 1, 1, 'numHeads', null,"Parallel attention mechanisms. Input size must be divisible by heads.")); optsDiv.appendChild(createNt('Input size must be divisible by Num Heads. Output size matches input.'));} else if(selType==='layernorm'){optsDiv.appendChild(createNt('Normalizes features across the feature dimension.'));} else if(selType==='dropout'){optsDiv.appendChild(createIn('Dropout Rate:',`layerRate_${idx}`, 'number', 0.5, 0, 0.01, 'rate', 'Fraction of neurons to zero out during training (0 to <1). Helps prevent overfitting.',"Higher value means more dropout."));} else if(selType==='softmax'){optsDiv.appendChild(createNt('Outputs probabilities summing to 1. For multi-class classification.'));}};
            typeSelect.addEventListener('change', (event) => updateOptionsUI(i, event.target.value)); hiddenLayersConfigContainer.appendChild(layerGroup); updateOptionsUI(i, typeSelect.value); }
        }
        numHiddenLayersInput.addEventListener('change', (event) => { const numLayers = Math.max(0, parseInt(event.target.value) || 0); event.target.value = numLayers; createLayerConfigUI(numLayers);
        // {{ Reset template dropdown on manual layer count change }}
        architectureTemplateSelect.value = 'custom';
        }); createLayerConfigUI(parseInt(numHiddenLayersInput.value));
        optimizerSelect.addEventListener('change', () => { decayRateGroup.style.display = optimizerSelect.value === 'rmsprop' ? 'block' : 'none'; }); decayRateGroup.style.display = optimizerSelect.value === 'rmsprop' ? 'block' : 'none';

        lrSchedulerSelect.addEventListener('change', () => {
            const selectedSchedule = lrSchedulerSelect.value;
            lrStepParamsDiv.style.display = selectedSchedule === 'step' ? 'grid' : 'none'; // Use grid for settings-grid
            lrExpParamsDiv.style.display = selectedSchedule === 'exponential' ? 'grid' : 'none'; // Use grid for settings-grid
        });
        lrSchedulerSelect.dispatchEvent(new Event('change'));

        // {{ Add Template Selector Logic }}
        architectureTemplateSelect.addEventListener('change', (event) => {
            const templateKey = event.target.value;
            if (templateKey === 'custom') {
                // Allow manual configuration, do nothing here
                return;
            }

            const template = architectureTemplates[templateKey];
            if (!template) {
                console.warn("Selected template not found:", templateKey);
                return;
            }

            statsEl.innerHTML = `Applying '${templateKey}' template...`;

            // 1. Set number of hidden layers
            numHiddenLayersInput.value = template.numHidden;

            // 2. Trigger change to regenerate UI (IMPORTANT)
            numHiddenLayersInput.dispatchEvent(new Event('change'));

            // 3. Use setTimeout to allow DOM update before populating
            setTimeout(() => {
                try {
                    template.layers.forEach((layerConfig, i) => {
                        const setV=(s,v)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&v!==undefined)e.value=v;};
                        const setC=(s,c)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&c!==undefined)e.checked=c;};
                        const triggerChange=(s)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e) e.dispatchEvent(new Event('change',{bubbles:true}));}; // Helper to trigger change

                        // Set layer type first and trigger change to update options UI
                        setV(`select[data-layer-index="${i}"][data-config-type="type"]`, layerConfig.type || 'dense');
                        triggerChange(`select[data-layer-index="${i}"][data-config-type="type"]`); // Crucial!

                        // Set other config values based on type
                        switch(layerConfig.type){
                            case 'dense':
                                setV(`input[data-layer-index="${i}"][data-config-type="size"]`, layerConfig.size);
                                setV(`select[data-layer-index="${i}"][data-config-type="activation"]`, layerConfig.activation);
                                setC(`input[data-layer-index="${i}"][data-config-type="useBias"]`, layerConfig.useBias);
                                break;
                            case 'attention':
                                setV(`input[data-layer-index="${i}"][data-config-type="numHeads"]`, layerConfig.numHeads);
                                break;
                            case 'dropout':
                                setV(`input[data-layer-index="${i}"][data-config-type="rate"]`, layerConfig.rate);
                                break;
                            // LayerNorm and Softmax have no specific config values in this simple setup
                        }
                    });
                    // {{ Edit 1: Apply blue color and bold to only the template name }}
                    statsEl.innerHTML = `Applied <span style="color: #87CEEB; font-weight: bold;">${templateKey}</span> template. Ready.`;
                 } catch (error) {
                     console.error("Error applying template UI:", error);
                     statsEl.innerHTML = `<span class="error">Error applying template: ${error.message}</span>`;
                     architectureTemplateSelect.value = 'custom'; // Revert on error
                 }
            }, 0); // Timeout needed to ensure createLayerConfigUI finishes DOM manipulation
        });
        // {{ End Template Selector Logic }}


        trainButton.addEventListener('click', async () => {
            statsEl.innerHTML = 'Starting training...'; trainButton.disabled = true; trainButton.textContent = 'Training...'; predictButton.disabled = true; saveButton.disabled = true; loadButton.disabled = true; unloadButton.disabled = true; epochBar.style.width = '0%'; lossHistory = []; drawLossGraph();
            try {
                // {{ Read Weight Init Method }}
                const weightInitMethod = document.getElementById('weightInit').value;

                const trainingData = parseCSV(trainingDataTextarea.value); const testData = parseCSV(testDataTextarea.value); if (trainingData.length === 0) throw new Error("Training data empty/invalid."); if (!trainingData[0]?.input || !trainingData[0]?.output) throw new Error("Cannot get input/output size from data.");
                nn.reset(); const numHidden = parseInt(numHiddenLayersInput.value); const layerCfgs = []; const numIns = trainingData[0].input.length; let curInSize = numIns;
                for (let i = 0; i < numHidden; i++) { 
                    const getV=(s)=>hiddenLayersConfigContainer.querySelector(s)?.value; 
                    const getC=(s)=>hiddenLayersConfigContainer.querySelector(s)?.checked; 
                    const lType=getV(`select[data-layer-index="${i}"][data-config-type="type"]`) || 'dense'; 
                    // {{ Pass weightInitMethod to layer config }}
                    let cfg={type:lType, inputSize:curInSize, weightInit: weightInitMethod }; 
                    switch(lType){ 
                        case 'dense': 
                            cfg.outputSize=parseInt(getV(`input[data-layer-index="${i}"][data-config-type="size"]`)||1); 
                            if(cfg.outputSize<=0) throw new Error(`L${i+1} Dense: Invalid nodes.`); 
                            cfg.activation=getV(`select[data-layer-index="${i}"][data-config-type="activation"]`) || 'tanh'; 
                            cfg.useBias=getC(`input[data-layer-index="${i}"][data-config-type="useBias"]`)??true; 
                            break; 
                        case 'attention': 
                            cfg.numHeads=parseInt(getV(`input[data-layer-index="${i}"][data-config-type="numHeads"]`)||1); 
                            if(cfg.numHeads<=0) throw new Error(`L${i+1} Attn: Invalid heads.`); 
                            if(curInSize%cfg.numHeads!==0) throw new Error(`L${i+1} Attn: Input ${curInSize} not divisible by ${cfg.numHeads} heads.`); 
                            cfg.outputSize=curInSize; 
                            break; 
                        case 'dropout': 
                            cfg.rate=parseFloat(getV(`input[data-layer-index="${i}"][data-config-type="rate"]`)||0); 
                            if(cfg.rate<0||cfg.rate>=1) throw new Error(`L${i+1} Dropout: Invalid rate.`); 
                            cfg.outputSize=curInSize; 
                            break; 
                        case 'layernorm': 
                            case 'softmax': 
                            cfg.outputSize=curInSize; 
                            break; 
                        default: 
                            throw new Error(`L${i+1}: Unknown type "${lType}".`); 
                    } 
                    if(!cfg.outputSize) throw new Error(`L${i+1}: No output size.`); 
                    layerCfgs.push(cfg); 
                    curInSize=cfg.outputSize; 
                }
                // {{ Adjust final layer based on template and data }}
                const isAutoencoder = architectureTemplateSelect.value === 'autoencoder';
                const numOuts = isAutoencoder ? numIns : trainingData[0].output.length;
                if(numOuts <= 0) throw new Error("Zero output cols defined.");

                const selectedLoss = lossFunctionSelect.value; // Get selected loss function

                // {{ Edit: Refined final activation logic }}
                let finalAct = 'none'; // Default to linear activation ('none') for regression (MSE)

                if (selectedLoss === 'crossentropy') {
                    if (isAutoencoder) {
                         // Autoencoders typically reconstruct input, often using sigmoid/tanh depending on input range
                    finalAct = architectureTemplates['autoencoder']?.finalActivationHint || 'sigmoid';
                         console.log(`Autoencoder with Cross-Entropy? Using template hint or sigmoid: ${finalAct}`);
                         // Note: Using CE loss with autoencoders is less common than MSE unless inputs are treated as probabilities.
                    } else if (numOuts > 1) {
                         // Multi-class classification
                         finalAct = 'softmax';
                    } else if (numOuts === 1) {
                         // Binary classification
                         finalAct = 'sigmoid';
                    }
                } else if (selectedLoss === 'mse') {
                     if (isAutoencoder) {
                         // Autoencoders with MSE often use sigmoid/tanh if input is bounded, or none otherwise
                         finalAct = architectureTemplates['autoencoder']?.finalActivationHint || 'sigmoid'; // Keep sigmoid/tanh hint possibility
                         console.log(`Autoencoder with MSE, using template hint or sigmoid: ${finalAct}`);
                     } else {
                         // Standard regression - default is already 'none'
                         // If target values are known to be bounded (e.g., 0-1 or -1 to 1),
                         // tanh or sigmoid *might* be appropriate, but 'none' is safer general default.
                         finalAct = 'none';
                         console.log('MSE loss selected, using linear final activation: none');
                     }
                }
                // If a different loss function were added later, it would default to 'none' here.

                console.log(`Adding final dense: ${curInSize}->${numOuts} (Act:${finalAct})`);
                layerCfgs.push({type:'dense', inputSize:curInSize, outputSize:numOuts, activation:finalAct, useBias:true});
                // {{ End final layer adjustment }}

                layerCfgs.forEach((cfg,i)=>{try{nn.layer(cfg);}catch(e){throw new Error(`Cfg L${i+1}(${cfg.type}): ${e.message}`);}}); if(nn.layers.length===0)throw new Error("Zero layers configured."); if(nn.debug)console.log("Net structure:",nn.layers);
                const opts={
                    epochs:parseInt(epochsInput.value)||50,
                    learningRate:parseFloat(learningRateInput.value)||0.01, // Initial LR
                    batchSize:parseInt(batchSizeInput.value)||8,
                    testSet:testData.length>0?testData:null,
                    optimizer:optimizerSelect.value,
                    lossFunction:lossFunctionSelect.value,
                    l2Lambda:parseFloat(l2LambdaInput.value)||0,
                    decayRate:parseFloat(decayRateInput.value)||0.9,
                    gradientClipValue:parseFloat(gradientClipValueInput.value)||0,
                    usePositionalEncoding:usePositionalEncodingCheckbox.checked,
                    lrSchedule: lrSchedulerSelect.value,
                    lrStepDecayFactor: parseFloat(document.getElementById('lrStepDecayFactor').value) || 0.1,
                    lrStepDecaySize: parseInt(document.getElementById('lrStepDecaySize').value) || 10,
                    lrExpDecayRate: parseFloat(document.getElementById('lrExpDecayRate').value) || 0.95,
                    callback:async(ep,trL,tstL,metricName,metricVal, lastForwardCache)=>{ // Add lastForwardCache parameter
                        // {{ Log 1: Confirm callback execution }}
                        //console.log(`UI Callback - Epoch ${ep} Received`);

                        lossHistory.push({train:trL,test:tstL});
                        drawLossGraph();
                        epochBar.style.width=`${(ep/opts.epochs)*100}%`;

                        const currentLR = nn.getCurrentLearningRate(ep -1, opts.learningRate, opts); // ep is 1-based, need 0-based for calc
                        const lrString = opts.lrSchedule !== 'none' ? ` | LR: ${currentLR.toExponential(2)}` : '';

                        let statusText = `Ep:${ep}/${opts.epochs} | Loss:${trL.toFixed(6)}`;
                        if (tstL !== null) {
                             statusText += ` | Val Loss:${tstL.toFixed(6)}`;
                        }
                        // {{ Add: Display metric in status if available }}
                        if (metricName && metricVal !== null && !isNaN(metricVal)) {
                             statusText += ` | Val ${metricName}:${metricVal.toFixed(4)}`;
                        }
                         statusText += lrString;
                         statsEl.innerHTML = statusText;

                         // Update visualization based on the last sample's activations
                         if (lastForwardCache && lastForwardCache.activations && lastForwardCache.activations.length > 0) {
                             nn.lastActivations = lastForwardCache.activations; // Update the data source for drawNetwork

                             // Throttle drawing - requestAnimationFrame ensures smooth rendering
                             if (!window._drawNetworkScheduled) { // Use a simple flag to prevent queuing multiple frames
                                 window._drawNetworkScheduled = true;
                                 requestAnimationFrame(() => {
                                     try { // Add try-catch around drawing in case of errors during intensive training
                                         drawNetwork();
                                     } catch (drawErr) {
                                         console.warn("Error during dynamic drawNetwork:", drawErr);
                                         // Optionally disable further dynamic drawing on error to prevent flooding logs
                                         // window._disableDynamicDraw = true;
                                     } finally {
                                         window._drawNetworkScheduled = false; // Allow scheduling the next frame
                                     }
                                 });
                             }
                         }
                    }
                };
                statsEl.innerHTML=`Training (${opts.optimizer}, ${opts.lossFunction}` + (opts.lrSchedule !== 'none' ? `, ${opts.lrSchedule} LR` : '') + `)...`;
                // Reset the scheduling flag before training starts
                window._drawNetworkScheduled = false;
                // window._disableDynamicDraw = false; // Reset disable flag if implemented
                const summary=await nn.train(trainingData,opts);
                const totalParams=nn.getTotalParameters();
                statsEl.innerHTML=`<strong>Done!</strong> Loss:${summary.trainLoss.toFixed(6)}`+(summary.testLoss!==null?`, Val:${summary.testLoss.toFixed(6)}`:'')+` | Params:${totalParams.toLocaleString()}`;
                console.log("Final Summary:",summary);
                if(trainingData.length>0){try{nn.predict(trainingData[0].input);drawNetwork();}catch(e){}}
            } catch (error) { console.error('Train err:', error); statsEl.innerHTML = `<span class="error">Error: ${error.message}</span>`; }
            finally { trainButton.disabled=false; trainButton.textContent='Train Model'; predictButton.disabled=false; saveButton.disabled=false; loadButton.disabled=false; unloadButton.disabled=false; }
        });

        function drawNetwork() {
            if (!networkCtx || !networkCanvas) return;
            const networkContainer = networkCanvas.parentElement; // Should be #network-viz-container
            if (!networkContainer) return;
            const containerWidth = networkContainer.clientWidth; const containerHeight = networkContainer.clientHeight;
            networkCtx.clearRect(0, 0, networkCanvas.width, networkCanvas.height); // Clear previous drawing
            const hasModel = nn.lastActivations && nn.lastActivations.length > 0 && nn.layers && nn.layers.length > 0;

            if (!hasModel) {
                networkCtx.fillStyle = "#555"; networkCtx.font = "10px monospace"; networkCtx.textAlign = "center"; networkCtx.textBaseline = "middle";
                // Set canvas size to container size if no model to draw
                if (networkCanvas.width !== containerWidth) networkCanvas.width = containerWidth;
                if (networkCanvas.height !== containerHeight) networkCanvas.height = containerHeight;
                networkCtx.fillText("Train/Predict to visualize", containerWidth / 2, containerHeight / 2);
                return;
            }

            // Configs
            const pad=35, maxNds=20, nRBase=2, nRScale=3, cBOp=0.02, cMOp=0.85, cWScale=2, ellOff=10, lblOff=20, lblFnt="10px monospace", lblClr="#aaa";
            const nVizLyrs=nn.lastActivations.length;

            // Adjust spacing based on number of layers
            // More spacing for fewer layers, minimum spacing for many layers
            const baseSpacing = 150; // Base spacing between layers (increased from 70)
            const minLayerSpacing = Math.max(120, Math.min(baseSpacing, containerWidth / (nVizLyrs > 1 ? nVizLyrs : 1)));

            // Calculate required width with improved spacing
            const requiredWidth = (nVizLyrs <= 1)
                ? containerWidth
                : pad * 2 + (nVizLyrs - 1) * minLayerSpacing;

            // Ensure we have at least 20% more space than the minimal calculated width
            // This creates more visual breathing room
            const canvasDrawWidth = Math.max(containerWidth, requiredWidth * 1.2);

            // Set Canvas Attributes
            networkCanvas.width = canvasDrawWidth; networkCanvas.height = containerHeight;

            // Drawing dimensions
            const drawAreaWidth = canvasDrawWidth - pad * 2; const drawAreaHeight = containerHeight - pad * 2;
            const layerXs = Array.from({ length: nVizLyrs }, (_, i) => pad + (nVizLyrs === 1 ? drawAreaWidth / 2 : (drawAreaWidth * i) / (nVizLyrs - 1)));

            // 1. Calculate Node Positions
            const layerPos = [];
            nn.lastActivations.forEach((act, lIdx) => {
                // {{ Edit: Modify check to handle Float32Array correctly }}
                // if (!Array.isArray(act)) { // Old check
                if (!act || typeof act.length !== 'number') { // New check: Ensure 'act' exists and has a length property
                // {{ End Edit }}
                    if (nn.debug) console.warn(`drawNetwork L${lIdx}: Activation data is not array-like.`, act); // Add warning
                    layerPos.push([]); // Push empty if data is invalid
                    return;
                }
                // --- The rest of the node position calculation logic remains the same ---
                const lNodes = [];
                const nNodes = act.length;
                const dNodes = Math.min(nNodes, maxNds);
                const lX = layerXs[lIdx];
                for (let j = 0; j < dNodes; j++) {
                    const origIdx = nNodes <= maxNds ? j : Math.floor(j * nNodes / dNodes);
                    const nodeVal = act[origIdx];
                    const nY = pad + (dNodes === 1 ? drawAreaHeight / 2 : (drawAreaHeight * j) / (dNodes - 1));
                    // Ensure value is finite number before storing
                    lNodes.push({ x: lX, y: nY, value: (typeof nodeVal === 'number' && isFinite(nodeVal) ? nodeVal : 0) });
                }
                if (nNodes > maxNds) {
                    lNodes.push({ x: lX, y: pad + drawAreaHeight + ellOff, value: 0, isEllipsis: true, originalCount: nNodes });
                }
                layerPos.push(lNodes);
                // --- End of unchanged node position logic ---
            });

            // 2. Draw Connections
            networkCtx.lineWidth = 1;
            for (let i = 0; i < nVizLyrs - 1; i++) {
                const curNodes = layerPos[i].filter(n => !n.isEllipsis);
                const nextNodes = layerPos[i + 1].filter(n => !n.isEllipsis);
                const cfg = nn.layers[i];
                if (!cfg) continue;

                // {{ Edit 1: Correct check for Float32Array weights }}
                // const isDenseW = cfg.type === 'dense' && Array.isArray(nn.weights?.[i]); // Old check
                const isDenseW = cfg.type === 'dense' && nn.weights?.[i] instanceof Float32Array; // New check
                const w = isDenseW ? nn.weights[i] : null; // w is Float32Array or null
                // {{ End Edit 1 }}

                // Get input size for dense layers (needed for 1D indexing)
                const inputSizeForLayer = cfg.inputSize;

                for (let j = 0; j < curNodes.length; j++) { // Index for current layer nodes (corresponds to input index k in train/predict)
                    for (let k = 0; k < nextNodes.length; k++) { // Index for next layer nodes (corresponds to output index j in train/predict)
                        let op = 0.1, col = '100,100,100', lw = 0.5;
                        let lineDash = []; // Default: solid line
                        let weight = null; // Variable to hold the specific weight value

                        // --- Retrieve weight ONLY for dense layers ---
                        if (isDenseW && w && typeof inputSizeForLayer === 'number' && inputSizeForLayer > 0) {
                             // {{ Edit 2: Use correct 1D indexing to get weight }}
                             // Calculate the flat index: (next_node_index * input_size) + current_node_index
                             // Note: 'k' here is the next layer index, 'j' is the current layer index
                             const weightIndex = k * inputSizeForLayer + j;

                             // Check if the index is valid before accessing
                             if (weightIndex >= 0 && weightIndex < w.length) {
                                 weight = w[weightIndex]; // Get the weight value using 1D index
                             } else {
                                 if (nn.debug) console.warn(`drawNetwork L${i} Dense: Invalid weight index ${weightIndex} (nextIdx=${k}, currIdx=${j}, inSize=${inputSizeForLayer}, wLen=${w.length})`);
                                 weight = null; // Set weight to null if index is invalid
                             }
                             // {{ End Edit 2 }}
                        }

                        // --- Styling based on layer type and weight (if available) ---
                        // This logic uses the retrieved 'weight' variable
                        if (isDenseW && weight !== null && typeof weight === 'number' && typeof curNodes[j].value === 'number') {
                            // Dense layer styling using 'weight'
                            const wMag = Math.tanh(Math.abs(weight));
                            const aMag = Math.tanh(Math.abs(curNodes[j].value));
                            const combSig = (wMag * 0.8 + aMag * 0.2);
                            op = Math.min(Math.max(cBOp + combSig * (cMOp - cBOp), cBOp), cMOp);
                            col = weight >= 0 ? '255,255,255' : '180,180,255'; // White/Blue
                            lw = Math.min(Math.max(0.5, op * cWScale), 2);
                            lineDash = []; // Solid
                        } else if (cfg.type === 'attention') {
                            // Attention styling (no weight needed)
                            op = 0.5; lw = 1.2; col = '0,200,200'; lineDash = [];
                        } else if (cfg.type === 'layernorm') {
                            // LayerNorm styling (no weight needed)
                            op = 0.4; lw = 0.8; col = '255,255,0'; lineDash = [2, 2];
                        } else if (cfg.type === 'dropout') {
                            // Dropout styling (no weight needed)
                            op = 0.2; lw = 0.6; col = '255,165,0'; lineDash = [3, 3];
                        } else if (cfg.type === 'softmax') {
                             // Softmax styling (no weight needed)
                             op = 0.4; lw = 1.0; col = '200,0,200'; lineDash = [];
                        } else if (!isDenseW) { // Fallback for non-dense layers not explicitly handled
                             op = 0.1; lw = 0.5; col = '100,100,100'; lineDash = [];
                        } else { // Fallback for dense layers if weight retrieval failed
                            op = 0.05; lw = 0.3; col = '80,80,80'; lineDash = [1, 2]; // Even fainter, dashed
                        }
                        // --- End Styling ---

                        // --- Draw the line ---
                        networkCtx.strokeStyle = `rgba(${col},${op})`;
                        networkCtx.lineWidth = lw;
                        networkCtx.setLineDash(lineDash || []);
                        networkCtx.beginPath();
                        networkCtx.moveTo(curNodes[j].x, curNodes[j].y);
                        networkCtx.lineTo(nextNodes[k].x, nextNodes[k].y);
                        networkCtx.stroke();
                        networkCtx.setLineDash([]); // Reset line dash
                    }
                }
            }

            // 3. Draw Nodes and Labels
            networkCtx.textAlign = "center";

            // Draw the "Layers" label centered over all hidden layers
            if (layerXs.length > 2) {
                // Calculate center position between first hidden layer and last hidden layer
                const firstHiddenLayerX = layerXs[1]; // Index 1 is first hidden layer
                const lastHiddenLayerX = layerXs[layerXs.length - 2]; // Second-to-last is last hidden layer
                const centerX = (firstHiddenLayerX + lastHiddenLayerX) / 2;

                // Draw "Layers" label at the calculated center position
                networkCtx.fillStyle = lblClr;
                networkCtx.font = lblFnt;
                networkCtx.textBaseline = "bottom";
                networkCtx.fillText("Layers", centerX, pad - lblOff / 2);
            }

            layerPos.forEach((lNodes, lIdx) => {
                // Draw Label
                networkCtx.fillStyle = lblClr; networkCtx.font = lblFnt; networkCtx.textBaseline = "bottom";
                if (lIdx === 0) {
                    networkCtx.fillText("Input", layerXs[lIdx], pad - lblOff / 2);
                } else if (lIdx === layerPos.length - 1) {
                    networkCtx.fillText("Output", layerXs[lIdx], pad - lblOff / 2);
                }
                // Draw Nodes
                lNodes.forEach(n => {
                    if (n.isEllipsis) { networkCtx.fillStyle = "#777"; networkCtx.font = "10px monospace"; networkCtx.textBaseline = "top"; networkCtx.fillText(`(${n.originalCount} nodes)`, n.x, n.y); }
                    else { const actStr = Math.tanh(Math.abs(n.value)); const r = nRBase + actStr * nRScale; const op = 0.3 + actStr * 0.7; const col = n.value >= 0 ? '255,255,255' : '200,200,255'; networkCtx.fillStyle = `rgba(${col},${op})`; networkCtx.strokeStyle = 'rgba(255,255,255,0.6)'; networkCtx.lineWidth = 1; networkCtx.beginPath(); networkCtx.arc(n.x, n.y, r, 0, Math.PI * 2); networkCtx.fill(); networkCtx.stroke(); }
                });
            });
        }
        function resizeCanvases() {
             const lossContainer = lossCanvas?.parentElement;
             // Target the specific container for the network graph
             const networkContainer = document.getElementById('network-viz-container');

             // Resize and redraw loss graph as before
             if (lossContainer?.clientWidth > 0 && lossCanvas) {
                 lossCanvas.width = lossContainer.clientWidth;
                 lossCanvas.height = lossContainer.clientHeight;
                 drawLossGraph();
             }
             // For network graph, just call drawNetwork. It will read the container size
             // and set its own canvas width/height appropriately before drawing.
             if (networkContainer?.clientWidth > 0 && networkCanvas) {
                 drawNetwork();
             }
        }
        window.addEventListener('resize', resizeCanvases); setTimeout(resizeCanvases, 150);

        saveButton.addEventListener('click', () => { if (!nn.layers || nn.layers.length === 0) { statsEl.innerHTML = '<span class="error">No model to save.</span>'; return; } nn.save('oblix_model'); statsEl.innerHTML = 'Model saved.'; });
        loadButton.addEventListener('click', () => { statsEl.innerHTML = 'Loading...'; nn.load((error) => { if (error) { statsEl.innerHTML = `<span class="error">Load failed: ${error.message}</span>`; return; } const params = nn.getTotalParameters(); statsEl.innerHTML = `<strong>Model loaded!</strong> Params: ${params.toLocaleString()}`; try { const d = nn.details?.training; const l = nn.layers || []; usePositionalEncodingCheckbox.checked = nn.usePositionalEncoding || false; if (d) { epochsInput.value=d.epochs||50; learningRateInput.value=d.learningRate||0.01; batchSizeInput.value=d.batchSize||8; optimizerSelect.value=d.optimizer||'adam'; lossFunctionSelect.value=d.lossFunction||'mse'; l2LambdaInput.value=d.l2Lambda||0; decayRateInput.value=d.decayRate||0.9; gradientClipValueInput.value=d.gradientClipValue||0; optimizerSelect.dispatchEvent(new Event('change')); } const numHid = Math.max(0, l.length - 1); numHiddenLayersInput.value = numHid; createLayerConfigUI(numHid); l.slice(0, numHid).forEach((layer, i) => { const setV=(s,v)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&v!==undefined)e.value=v;}; const setC=(s,c)=>{const e=hiddenLayersConfigContainer.querySelector(s); if(e&&c!==undefined)e.checked=c;}; setV(`select[data-layer-index="${i}"][data-config-type="type"]`, layer.type||'dense'); const ts=hiddenLayersConfigContainer.querySelector(`select[data-layer-index="${i}"][data-config-type="type"]`); if(ts)ts.dispatchEvent(new Event('change',{bubbles:true})); switch(layer.type){ case 'dense': setV(`input[data-layer-index="${i}"][data-config-type="size"]`, layer.outputSize); setV(`select[data-layer-index="${i}"][data-config-type="activation"]`, layer.activation); setC(`input[data-layer-index="${i}"][data-config-type="useBias"]`, layer.useBias); break; case 'attention': setV(`input[data-layer-index="${i}"][data-config-type="numHeads"]`, layer.numHeads); break; case 'dropout': setV(`input[data-layer-index="${i}"][data-config-type="rate"]`, layer.rate); break; } });
                // {{ Set template dropdown to Custom after load }}
                architectureTemplateSelect.value = 'custom';
                lossHistory=[]; drawLossGraph(); predictionResultEl.innerHTML='Result: -'; try { const sample = parseCSV(trainingDataTextarea.value)[0]; if(sample) nn.predict(sample.input); } catch(e) {} drawNetwork(); } catch (uiError) { console.error("UI update err after load:", uiError); statsEl.innerHTML += ` <span class="error">(UI update failed)</span>`; } }); });
        predictButton.addEventListener('click', () => { predictionResultEl.innerHTML=`Predicting...`; try { const inputStr = document.getElementById('predictionInput').value; if(!inputStr)throw new Error("Input empty."); const input=inputStr.split(',').map(s=>parseFloat(s.trim())); if(input.some(isNaN))throw new Error("Invalid input."); if(!nn.layers||nn.layers.length===0)throw new Error("Network not init."); const expectSz=nn.layers[0]?.inputSize; if(expectSz===undefined)throw new Error("Cannot get input size."); if(input.length!==expectSz)throw new Error(`Input size mismatch: Exp ${expectSz}, got ${input.length}.`); const pred=nn.predict(input); if(pred===null)throw new Error("Prediction failed."); const predStr=pred.map(p=>p.toFixed(5)).join(', '); predictionResultEl.innerHTML=`Result: [${predStr}]`; drawNetwork(); } catch (error) { console.error("Predict error:", error); predictionResultEl.innerHTML = `<span class="error">Error: ${error.message}</span>`; } });
        unloadButton.addEventListener('click', () => { console.log("Unload clicked."); try { nn.reset(); lossHistory = []; drawLossGraph(); drawNetwork(); epochBar.style.width = '0%'; statsEl.innerHTML = 'Status: Model unloaded.'; predictionResultEl.innerHTML = 'Result: -'; const defaultLayers = 2; numHiddenLayersInput.value = defaultLayers; createLayerConfigUI(defaultLayers);
            // {{ Set template dropdown to Custom after unload/reset }}
            architectureTemplateSelect.value = 'custom';
             console.log("Model & UI reset."); } catch (error) { console.error("Unload error:", error); statsEl.innerHTML = `<span class="error">Unload error: ${error.message}</span>`; } });

    }); // End DOMContentLoaded
  </script>
  <canvas id="watermarkCanvas" width="1" height="1" style="display: none;"></canvas>
</body>
</html>
